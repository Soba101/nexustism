{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune SentenceTransformer Models for ITSM Tickets (v6)\n",
    "\n",
    "This notebook represents the `v6` iteration of the ITSM similarity model pipeline. It builds upon the robust pipeline of `v5` but introduces critical feature engineering improvements.\n",
    "### Key Enhancements from v5\n",
    "1. **Contextual Prefixing**: Instead of training on just `Short Description` + `Description`, we now embed structured metadata (`Service`, `Category`, `Group`) directly into the text string. This helps the model distinguish between technically distinct but linguistically similar tickets (e.g., \"Login failed\" in SAP vs. VPN).\n",
    "2. **Data Leakage Prevention**: Explicitly **EXCLUDING** `Resolution notes` from the training text to ensure the model learns to match problems based on symptoms, not retrospective solutions.\n",
    "3. **Advanced Evaluation**: Continued real-time tracking of Spearman correlation, ROC AUC, and F1 scores.\n",
    "## Overview\n",
    "This notebook represents the `v5` iteration of the ITSM similarity model pipeline. It transitions from a functional script to a robust, configurable machine learning pipeline.\n",
    "\n",
    "### Key Enhancements\n",
    "1. **Comprehensive Configuration**: Centralized `CONFIG` for all hyperparameters.\n",
    "2. **Smart Data Generation**: TF-IDF based filtering for high-quality positives and dynamic hard negative mining.\n",
    "3. **Advanced Evaluation**: Real-time tracking of Spearman correlation, ROC AUC, and F1 scores.\n",
    "4. **Reproducibility**: Full seeding of Random, NumPy, and PyTorch.\n",
    "5. **Relationship Classifier**: Integrated training of the secondary classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Setup] Install dependencies and download NLTK data (Run this first!)\n",
    "# This cell ensures your environment has all required libraries and data.\n",
    "\n",
    "# 1. Install essential libraries\n",
    "%pip install --upgrade sentence-transformers imbalanced-learn \"protobuf<=3.20.1\" --quiet\n",
    "%pip install imbalanced-learn --quiet\n",
    "\n",
    "# 2. Fix TensorFlow/Protobuf conflicts (common in Kaggle)\n",
    "import os\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "# 3. Download NLTK data\n",
    "import nltk\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)  # Often needed for wordnet\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True) # Newer nltk versions might need this\n",
    "\n",
    "print(\"‚úÖ Setup complete. Dependencies installed and NLTK data downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress CUDA warnings\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import pickle\n",
    "import collections\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, precision_recall_curve, roc_curve\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import joblib\n",
    "\n",
    "\n",
    "# --- Robust NLTK Setup ---\n",
    "import nltk\n",
    "# Download essential NLTK data immediately to avoid LookupErrors\n",
    "for res in ['wordnet', 'omw-1.4', 'stopwords', 'punkt', 'punkt_tab']:\n",
    "    try:\n",
    "        nltk.download(res, quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to download NLTK resource {res}: {e}\")\n",
    "# -------------------------\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SentenceEvaluator\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# --- Setup & Reproducibility ---\n",
    "\n",
    "def find_workspace_root(marker='nexustism'):\n",
    "    cwd = Path.cwd()\n",
    "    if marker in cwd.parts:\n",
    "        idx = cwd.parts.index(marker)\n",
    "        return Path(*cwd.parts[:idx+1])\n",
    "    return cwd\n",
    "\n",
    "\n",
    "def resolve_log_path():\n",
    "    # 1. Agent Temp Dir (High priority for agent runs)\n",
    "    if os.environ.get(\"GEMINI_TEMP_DIR\"):\n",
    "        p = Path(os.environ.get(\"GEMINI_TEMP_DIR\")).expanduser()\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        return p / \"training_v6.log\"\n",
    "        \n",
    "    # 2. Explicit User Override\n",
    "    if os.environ.get(\"LOG_DIR\"):\n",
    "        p = Path(os.environ.get(\"LOG_DIR\")).expanduser()\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        return p / \"training_v6.log\"\n",
    "\n",
    "    # 3. Default: Local Current Working Directory\n",
    "    # This ensures it logs to the local folder where the notebook is running\n",
    "    return Path.cwd() / \"training_v6.log\"\n",
    "LOG_PATH = resolve_log_path()\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_PATH),\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    force=True  # ensure handler resets even if logger was configured earlier\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_step(msg: str):\n",
    "    logger.info(msg)\n",
    "    print(msg)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# NLTK Downloads\n",
    "\n",
    "def ensure_nltk_resource(resource_path, download_name=None):\n",
    "    try:\n",
    "        nltk.data.find(resource_path)\n",
    "        return True\n",
    "    except LookupError:\n",
    "        try:\n",
    "            nltk.download(download_name or resource_path.split('/')[-1], quiet=True)\n",
    "            nltk.data.find(resource_path)\n",
    "            return True\n",
    "        except Exception as exc:\n",
    "            logger.info(f\"NLTK resource '{resource_path}' unavailable ({exc}); falling back when possible.\")\n",
    "            return False\n",
    "\n",
    "HAS_STOPWORDS = ensure_nltk_resource('corpora/stopwords', 'stopwords')\n",
    "HAS_WORDNET = ensure_nltk_resource('corpora/wordnet', 'wordnet')\n",
    "HAS_PUNKT = ensure_nltk_resource('tokenizers/punkt', 'punkt')\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    #Set seeds for reproducibility across all libraries.\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    log_step(f\"‚úÖ Seeds set to {seed}\")\n",
    "\n",
    "set_seeds(42)\n",
    "log_step(f\"Logging to {LOG_PATH}\")\n",
    "\n",
    "# Device info (SentenceTransformer handles placement, but we log visibility)\n",
    "if torch.cuda.is_available():\n",
    "    log_step(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    log_step(\"MPS available: using Apple Silicon accelerator\")\n",
    "else:\n",
    "    log_step(\"No GPU detected; training will run on CPU\")\n",
    "\n",
    "# Check Imbalanced-Learn for Relationship Classifier\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    log_step('‚ö†Ô∏è imbalanced-learn not installed. Relationship classifier will skip SMOTE.')\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Paths\n",
    "    'base_model': 'sentence-transformers/all-mpnet-base-v2',\n",
    "    'output_dir': 'models/all-mpnet-finetuned-v6',\n",
    "    'source_data': 'data/dummy_data_promax.csv',\n",
    "    'relationship_data': 'data/relationship_pairs.json',\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    'epochs': 20,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 2e-5,\n",
    "    'max_learning_rate': 5e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'max_seq_length': 384,  # Increased for detailed ticket descriptions\n",
    "    \n",
    "    # Data Generation Strategy\n",
    "    'target_pairs': 50000,  # Total training pairs to generate\n",
    "    'positive_ratio': 0.4,  # 40% positives, 60% negatives\n",
    "    'augmentation_ratio': 0.2,\n",
    "    'eval_split': 0.15,\n",
    "    \n",
    "    # Quality Filtering Thresholds\n",
    "    'quality_threshold': 0.3,      # Minimum TF-IDF similarity for 'good' positive pairs\n",
    "    'hard_negative_min': 0.15,     # Min similarity for hard negatives\n",
    "    'hard_negative_max': 0.45,     # Max similarity for hard negatives (confusing zone)\n",
    "    \n",
    "    # Early Stopping\n",
    "    'early_stopping_patience': 7,\n",
    "    'min_delta': 0.005,\n",
    "    'eval_steps': 100,\n",
    "    \n",
    "    # Loss Weights (Future use for Multi-Loss)\n",
    "    'mnr_loss_weight': 1.0,\n",
    "    'triplet_loss_weight': 0.5,\n",
    "    'cosine_loss_weight': 0.2}\n",
    "\n",
    "# Create Output Directory\n",
    "output_path = os.path.join(os.getcwd(), CONFIG['output_dir'])\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "print(f\"üìÇ Output directory set to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing (Enhanced for v6)\n",
    "Includes **Contextual Prefixing** using `Service`, `Category`, `Subcategory`, and `Assignment group`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Resolve data paths for local/Kaggle\n",
    "# Reuses the v5 Kaggle-friendly resolver to avoid FileNotFound errors\n",
    "log_step(\"[Cell 5] Resolving data paths and loading data...\")\n",
    "\n",
    "def resolve_data_path(filepath):\n",
    "    \"\"\"\n",
    "    Resolve a relative file path to an absolute path.\n",
    "    Tries multiple locations (workspace root, Kaggle input) to find the file.\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    tried_paths = []\n",
    "\n",
    "    if os.path.isabs(filepath):\n",
    "        if os.path.exists(filepath):\n",
    "            return filepath\n",
    "        tried_paths.append(filepath)\n",
    "    else:\n",
    "        if os.path.exists(filepath):\n",
    "            return os.path.abspath(filepath)\n",
    "        tried_paths.append(os.path.abspath(filepath))\n",
    "\n",
    "    # Kaggle paths\n",
    "    if '/kaggle/' in cwd:\n",
    "        kaggle_input_base = '/kaggle/input'\n",
    "        if os.path.exists(kaggle_input_base):\n",
    "            for dataset_dir in os.listdir(kaggle_input_base):\n",
    "                dataset_path = os.path.join(kaggle_input_base, dataset_dir)\n",
    "                if not os.path.isdir(dataset_path):\n",
    "                    continue\n",
    "                potential_path = os.path.join(dataset_path, filepath)\n",
    "                if os.path.exists(potential_path):\n",
    "                    return os.path.abspath(potential_path)\n",
    "                tried_paths.append(potential_path)\n",
    "                if filepath.startswith('data/'):\n",
    "                    filename = filepath.replace('data/', '')\n",
    "                    potential_path = os.path.join(dataset_path, filename)\n",
    "                    if os.path.exists(potential_path):\n",
    "                        return os.path.abspath(potential_path)\n",
    "                    tried_paths.append(potential_path)\n",
    "        kaggle_working_path = os.path.join('/kaggle/working', filepath)\n",
    "        if os.path.exists(kaggle_working_path):\n",
    "            return os.path.abspath(kaggle_working_path)\n",
    "        tried_paths.append(kaggle_working_path)\n",
    "\n",
    "    # Workspace root (nexustism)\n",
    "    if 'nexustism' in cwd:\n",
    "        parts = cwd.split(os.sep)\n",
    "        if 'nexustism' in parts:\n",
    "            idx = parts.index('nexustism')\n",
    "            workspace_root = os.sep.join(parts[:idx+1])\n",
    "            potential_path = os.path.join(workspace_root, filepath)\n",
    "            if os.path.exists(potential_path):\n",
    "                return os.path.abspath(potential_path)\n",
    "            tried_paths.append(potential_path)\n",
    "\n",
    "    # Build helpful error\n",
    "    error_msg = (\n",
    "f\"Data file not found: {filepath}\n",
    "\"\n",
    "        f\"Current working directory: {cwd}\n",
    "\n",
    "\"\n",
    "        f\"Tried the following paths:\n",
    "\"\n",
    "    )\n",
    "    for p in tried_paths[:10]:\n",
    "        error_msg += f\"  - {p}\n",
    "\"\n",
    "\n",
    "    if '/kaggle/' in cwd:\n",
    "        error_msg += (\n",
    "            \"\n",
    "üìù Kaggle Environment Detected:\n",
    "\"\n",
    "            \"   - Upload the dataset to Kaggle and add it to the notebook.\n",
    "\"\n",
    "            \"   - Check available datasets with os.listdir('/kaggle/input').\n",
    "\"\n",
    "            \"   - Update CONFIG['source_data'] if the path differs.\n",
    "\"\n",
    "        )\n",
    "    else:\n",
    "        error_msg += (\n",
    "            \"\n",
    "üìù Local Environment:\n",
    "\"\n",
    "            \"   - Ensure the file exists under data/ or update CONFIG['source_data'].\n",
    "\"\n",
    "        )\n",
    "    raise FileNotFoundError(error_msg)\n",
    "\n",
    "# Resolve configured paths\n",
    "try:\n",
    "    CONFIG['source_data'] = resolve_data_path(CONFIG['source_data'])\n",
    "    if CONFIG.get('relationship_data'):\n",
    "        CONFIG['relationship_data'] = resolve_data_path(CONFIG['relationship_data'])\n",
    "    log_step(f\"‚úÖ Data path resolved to: {CONFIG['source_data']}\")\n",
    "except FileNotFoundError as e:\n",
    "    log_step(f\"‚ùå Error: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "def load_and_clean_data(filepath, min_length=10):\n",
    "    \"\"\"\n",
    "    Loads data, checks columns, and performs cleaning.\n",
    "    \"\"\"\n",
    "    log_step(f\"Loading data from {filepath}\")\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    required_cols = [\"Number\", \"Short Description\", \"Description\", \"Category\", \"Subcategory\", \n",
    "                     \"Service\", \"Service offering\", \"Assignment group\"]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "    # Fill NA and Clean Text\n",
    "    # We ensure all context fields are strings\n",
    "    placeholders = {\"\", \"nan\", \"none\", \"null\", \"unknown\", \"n/a\", \"na\"}\n",
    "\n",
    "    def normalize_field(val: str) -> str:\n",
    "        s = str(val).strip()\n",
    "        s = re.sub(r\"\\s+\", \" \", s)\n",
    "        if s.lower() in placeholders:\n",
    "            return \"\"\n",
    "        return s\n",
    "\n",
    "    for col in [c for c in required_cols if c != \"Number\"]:\n",
    "        df[col] = df[col].fillna(\"\").apply(normalize_field)\n",
    "\n",
    "    # Normalize casing for structured context fields to reduce duplicates\n",
    "    context_cols = [\"Service\", \"Service offering\", \"Category\", \"Subcategory\", \"Assignment group\"]\n",
    "    for col in context_cols:\n",
    "        df[col] = df[col].str.lower()\n",
    "\n",
    "    # Construct Rich Text Representation\n",
    "    # Format: [Service | Service offering] [Category | Subcategory] Group: Assignment group. Short Description. Description\n",
    "    def build_bracketed(parts):\n",
    "        clean_parts = [p for p in parts if p]\n",
    "        return f\"[{ ' | '.join(clean_parts) }] \" if clean_parts else \"\"\n",
    "\n",
    "    context_service = df.apply(lambda row: build_bracketed([row['Service'], row['Service offering']]), axis=1)\n",
    "    context_category = df.apply(lambda row: build_bracketed([row['Category'], row['Subcategory']]), axis=1)\n",
    "    context_group = df.apply(lambda row: f\"Group: {row['Assignment group']}. \" if row['Assignment group'] else \"\", axis=1)\n",
    "\n",
    "    df['text'] = (\n",
    "        context_service +\n",
    "        context_category +\n",
    "        context_group +\n",
    "        df['Short Description'].str.strip() + \". \" +\n",
    "        df['Description'].str.strip()\n",
    "    ).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    # Filter empty or too short\n",
    "    initial_count = len(df)\n",
    "    df = df[df['text'].str.len() >= min_length].copy()\n",
    "    dropped = initial_count - len(df)\n",
    "\n",
    "    # Clean up artifacts (e.g., multiple spaces, empty group/service components)\n",
    "    df['text'] = df['text'].str.replace(r\"\\s+\\.\", \".\", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    df['category_id'] = df.groupby(['Category', 'Subcategory']).ngroup()\n",
    "\n",
    "    log_step(f\"‚úÖ Loaded {len(df)} incidents (dropped {dropped} short/empty)\")\n",
    "    log_step(f\"   Unique Categories: {df['Category'].nunique()}\")\n",
    "    log_step(f\"   Unique Subcategories: {df['Subcategory'].nunique()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load Data\n",
    "df_incidents = load_and_clean_data(CONFIG['source_data'])\n",
    "\n",
    "# Split incidents first to avoid leakage between train/eval pairs\n",
    "train_df, eval_df = train_test_split(\n",
    "    df_incidents,\n",
    "    test_size=CONFIG['eval_split'],\n",
    "    random_state=42,\n",
    "    stratify=df_incidents['category_id']\n",
    ")\n",
    "\n",
    "train_target_pairs = int(CONFIG['target_pairs'] * (1 - CONFIG['eval_split']))\n",
    "eval_target_pairs = max(500, CONFIG['target_pairs'] - train_target_pairs)\n",
    "\n",
    "log_step(f\"Train incidents: {len(train_df)}, Eval incidents: {len(eval_df)}\")\n",
    "log_step(f\"Target pairs -> Train: {train_target_pairs}, Eval: {eval_target_pairs}\")\n",
    "\n",
    "log_step(\"Preview of Contextual Embeddings (train split):\")\n",
    "for t in train_df['text'].head(3).tolist():\n",
    "    print(f\" - {t[:200]}...\") # Truncate for display\n",
    "\n",
    "train_df.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Similarity Utilities\n",
    "We use TF-IDF similarity to find \"quality\" pairs (related but not identical) and \"hard negatives\" (different category but lexically similar).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextSimilarityCalculator:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer() if HAS_WORDNET else None\n",
    "        self.stop_words = set(stopwords.words('english')) if HAS_STOPWORDS else ENGLISH_STOP_WORDS\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "        self.tfidf_matrix = None\n",
    "\n",
    "    def fit_tfidf(self, texts):\n",
    "        log_step(f\"Fitting TF-IDF vectorizer on {len(texts)} texts...\")\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "        log_step(f\"TF-IDF fit complete. Matrix shape: {self.tfidf_matrix.shape}\")\n",
    "\n",
    "    def get_tfidf_similarity(self, idx1, idx2):\n",
    "        if self.tfidf_matrix is None:\n",
    "            raise ValueError(\"Run fit_tfidf first\")\n",
    "        # Compute cosine similarity between two sparse vectors\n",
    "        return (self.tfidf_matrix[idx1] * self.tfidf_matrix[idx2].T).toarray()[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Smart Pair Generation\n",
    "Generating positive pairs (same subcategory + semantic overlap) and hard negatives (different category + lexical overlap).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_smart_pairs(df, target_count, pos_ratio=0.4, sim_calculator=None):\n",
    "    if sim_calculator is None:\n",
    "        raise ValueError(\"sim_calculator must be provided (pre-fitted TF-IDF)\")\n",
    "\n",
    "    positive_target = int(target_count * pos_ratio)\n",
    "    negative_target = target_count - positive_target\n",
    "\n",
    "    positives = []\n",
    "    negatives = []\n",
    "\n",
    "    # Group by Category/Subcategory\n",
    "    groups = df.groupby('category_id')\n",
    "    group_indices = {k: v.index.tolist() for k, v in groups}\n",
    "    all_indices = df.index.tolist()\n",
    "\n",
    "    log_step(f\"Generating {positive_target} positive and {negative_target} negative pairs...\")\n",
    "\n",
    "    # --- 1. Positive Pairs ---\n",
    "    # Strategy: Sample pairs from same group, check TF-IDF score to ensure they aren't duplicates or too vague\n",
    "    pbar = tqdm(total=positive_target, desc=\"Positives\")\n",
    "    attempts = 0\n",
    "    while len(positives) < positive_target and attempts < positive_target * 5:\n",
    "        attempts += 1\n",
    "        # Pick random group\n",
    "        gid = random.choice(list(group_indices.keys()))\n",
    "        g_idxs = group_indices[gid]\n",
    "        if len(g_idxs) < 2: continue\n",
    "\n",
    "        i1, i2 = random.sample(g_idxs, 2)\n",
    "\n",
    "        # Convert DataFrame index to integer location for TF-IDF\n",
    "        loc1 = df.index.get_loc(i1)\n",
    "        loc2 = df.index.get_loc(i2)\n",
    "\n",
    "        sim = sim_calculator.get_tfidf_similarity(loc1, loc2)\n",
    "\n",
    "        # Accept if similarity is decent (avoiding identicals if sim=1.0, though duplicates happen)\n",
    "        if sim > CONFIG['quality_threshold']:\n",
    "            positives.append((i1, i2, sim))\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    if len(positives) < positive_target:\n",
    "        remaining = positive_target - len(positives)\n",
    "        log_step(f\"‚ö†Ô∏è Only found {len(positives)} positives with quality threshold; filling remaining {remaining} with random in-group samples.\")\n",
    "        filler_attempts = 0\n",
    "        while len(positives) < positive_target and filler_attempts < positive_target * 3:\n",
    "            filler_attempts += 1\n",
    "            gid = random.choice(list(group_indices.keys()))\n",
    "            g_idxs = group_indices[gid]\n",
    "            if len(g_idxs) < 2: continue\n",
    "\n",
    "            i1, i2 = random.sample(g_idxs, 2)\n",
    "            loc1 = df.index.get_loc(i1)\n",
    "            loc2 = df.index.get_loc(i2)\n",
    "            sim = sim_calculator.get_tfidf_similarity(loc1, loc2)\n",
    "            positives.append((i1, i2, sim))\n",
    "\n",
    "    # --- 2. Hard Negative Pairs ---\n",
    "    # Strategy: Different categories but high TF-IDF overlap (confusing examples)\n",
    "    pbar = tqdm(total=negative_target, desc=\"Negatives\")\n",
    "    attempts = 0\n",
    "    max_attempts = negative_target * 5  # Prevent runaway loops when hard negatives are scarce\n",
    "    fallback_after = negative_target * 2\n",
    "    while len(negatives) < negative_target and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "\n",
    "        # Random sampling\n",
    "        i1, i2 = random.sample(all_indices, 2)\n",
    "\n",
    "        # Must be different categories\n",
    "        if df.at[i1, 'Category'] == df.at[i2, 'Category']:\n",
    "            continue\n",
    "\n",
    "        loc1 = df.index.get_loc(i1)\n",
    "        loc2 = df.index.get_loc(i2)\n",
    "        sim = sim_calculator.get_tfidf_similarity(loc1, loc2)\n",
    "\n",
    "        # Hard Negative Criteria\n",
    "        is_hard = CONFIG['hard_negative_min'] < sim < CONFIG['hard_negative_max']\n",
    "\n",
    "        # Accept if hard negative OR we are struggling to find hard ones (fallback after many attempts)\n",
    "        if is_hard or attempts > fallback_after:\n",
    "            negatives.append((i1, i2, sim))\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    if len(negatives) < negative_target:\n",
    "        remaining = negative_target - len(negatives)\n",
    "        log_step(f\"‚ö†Ô∏è Only found {len(negatives)} hard negatives after {attempts} attempts; filling remaining {remaining} with random cross-category pairs.\")\n",
    "        for _ in range(remaining):\n",
    "            tries = 0\n",
    "            while True:\n",
    "                i1, i2 = random.sample(all_indices, 2)\n",
    "                tries += 1\n",
    "                if df.at[i1, 'Category'] != df.at[i2, 'Category'] or tries > 10:\n",
    "                    break\n",
    "            loc1 = df.index.get_loc(i1)\n",
    "            loc2 = df.index.get_loc(i2)\n",
    "            sim = sim_calculator.get_tfidf_similarity(loc1, loc2)\n",
    "            negatives.append((i1, i2, sim))\n",
    "\n",
    "    log_step(f\"‚úÖ Pair generation complete: {len(positives)} positives, {len(negatives)} negatives\")\n",
    "    return positives, negatives\n",
    "\n",
    "# Fit TF-IDF on splits to avoid leakage\n",
    "train_sim_calculator = TextSimilarityCalculator()\n",
    "train_sim_calculator.fit_tfidf(train_df['text'].tolist())\n",
    "\n",
    "eval_sim_calculator = TextSimilarityCalculator()\n",
    "eval_sim_calculator.fit_tfidf(eval_df['text'].tolist())\n",
    "\n",
    "# Generate pairs per split\n",
    "train_pos_idxs, train_neg_idxs = generate_smart_pairs(train_df, train_target_pairs, CONFIG['positive_ratio'], train_sim_calculator)\n",
    "eval_pos_idxs, eval_neg_idxs = generate_smart_pairs(eval_df, eval_target_pairs, CONFIG['positive_ratio'], eval_sim_calculator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_step(\"[Cell 11] Converting pairs to SentenceTransformer examples (with augmentation)...\")\n",
    "\n",
    "def simple_augment(text):\n",
    "    \"\"\"Randomly swap or delete words.\"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) < 5: return text\n",
    "\n",
    "    if random.random() > 0.5:\n",
    "        # Swap\n",
    "        idx = random.randint(0, len(words)-2)\n",
    "        words[idx], words[idx+1] = words[idx+1], words[idx]\n",
    "    else:\n",
    "        # Delete\n",
    "        idx = random.randint(0, len(words)-1)\n",
    "        words.pop(idx)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def pairs_to_examples(df, pos_pairs, neg_pairs, augment=False):\n",
    "    examples = []\n",
    "    for i1, i2, score in pos_pairs:\n",
    "        t1 = df.at[i1, 'text']\n",
    "        t2 = df.at[i2, 'text']\n",
    "\n",
    "        # Standard Pair\n",
    "        examples.append(InputExample(texts=[t1, t2], label=1.0))\n",
    "\n",
    "        # Augmentation (only for subset)\n",
    "        if augment and random.random() < CONFIG['augmentation_ratio']:\n",
    "            examples.append(InputExample(texts=[simple_augment(t1), t2], label=1.0))\n",
    "\n",
    "    for i1, i2, score in neg_pairs:\n",
    "        t1 = df.at[i1, 'text']\n",
    "        t2 = df.at[i2, 'text']\n",
    "        examples.append(InputExample(texts=[t1, t2], label=0.0))\n",
    "\n",
    "    return examples\n",
    "\n",
    "train_examples = pairs_to_examples(train_df, train_pos_idxs, train_neg_idxs, augment=True)\n",
    "eval_data = pairs_to_examples(eval_df, eval_pos_idxs, eval_neg_idxs, augment=False)\n",
    "\n",
    "# Shuffle train examples only\n",
    "random.shuffle(train_examples)\n",
    "train_data = train_examples\n",
    "\n",
    "log_step(f\"Training Samples: {len(train_data)}\")\n",
    "log_step(f\"Evaluation Samples: {len(eval_data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_step(\"[Cell 13] Loading base SentenceTransformer model...\")\n",
    "model = SentenceTransformer(CONFIG['base_model'])\n",
    "model.max_seq_length = CONFIG['max_seq_length']\n",
    "\n",
    "# --- Loss Function ---\n",
    "# Using CosineSimilarityLoss to leverage labeled positive/negative pairs directly\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# --- DataLoader ---\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=CONFIG['batch_size'])\n",
    "\n",
    "log_step(f\"Model loaded: {CONFIG['base_model']}\")\n",
    "log_step(f\"Max Seq Length: {model.max_seq_length}\")\n",
    "log_step(f\"Training batches per epoch: {len(train_dataloader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Setup & Custom Callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ComprehensiveEvaluator(SentenceEvaluator):\n",
    "\n",
    "    #Custom evaluator to track multiple metrics: Spearman, Pearson, ROC AUC, F1.\n",
    "\n",
    "    def __init__(self, examples, batch_size=32, name='', show_progress_bar=False):\n",
    "        self.examples = examples\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "        self.show_progress_bar = show_progress_bar\n",
    "\n",
    "        self.texts1 = [ex.texts[0] for ex in examples]\n",
    "        self.texts2 = [ex.texts[1] for ex in examples]\n",
    "        self.labels = [ex.label for ex in examples]\n",
    "\n",
    "    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n",
    "        model.eval()\n",
    "        log_step(f\"[Eval] Running evaluator at epoch={epoch}, step={steps} on {len(self.labels)} pairs...\")\n",
    "\n",
    "        # Encode\n",
    "        emb1 = model.encode(self.texts1, batch_size=self.batch_size, show_progress_bar=self.show_progress_bar, convert_to_numpy=True)\n",
    "        emb2 = model.encode(self.texts2, batch_size=self.batch_size, show_progress_bar=self.show_progress_bar, convert_to_numpy=True)\n",
    "\n",
    "        # Cosine Similarity\n",
    "        cosine_scores = np.sum(emb1 * emb2, axis=1) / (np.linalg.norm(emb1, axis=1) * np.linalg.norm(emb2, axis=1))\n",
    "\n",
    "        # Metrics\n",
    "        eval_pearson, _ = pearsonr(self.labels, cosine_scores)\n",
    "        eval_spearman, _ = spearmanr(self.labels, cosine_scores)\n",
    "\n",
    "        # Classification Metrics (Threshold optimization)\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(self.labels, cosine_scores)\n",
    "            pr_auc = average_precision_score(self.labels, cosine_scores)\n",
    "        except ValueError:\n",
    "            roc_auc = 0.0\n",
    "            pr_auc = 0.0\n",
    "\n",
    "        logger.info(f\"Epoch {epoch} Steps {steps}: Spearman={eval_spearman:.4f}, Pearson={eval_pearson:.4f}, ROC_AUC={roc_auc:.4f}, PR_AUC={pr_auc:.4f}\")\n",
    "        print()\n",
    "        print(f\"üìä Eval @ epoch {epoch}, step {steps}: Spearman={eval_spearman:.4f}, Pearson={eval_pearson:.4f}, ROC_AUC={roc_auc:.4f}, PR_AUC={pr_auc:.4f}\")\n",
    "\n",
    "        # Save detailed metrics to CSV\n",
    "        if output_path:\n",
    "            csv_path = os.path.join(output_path, 'eval_metrics.csv')\n",
    "            file_exists = os.path.isfile(csv_path)\n",
    "            with open(csv_path, mode='a', newline='') as f:\n",
    "                header = 'epoch,steps,spearman,pearson,roc_auc,pr_auc'\n",
    "                if not file_exists:\n",
    "                    f.write(header + '\n",
    "')\n",
    "                f.write(f\"{epoch},{steps},{eval_spearman:.4f},{eval_pearson:.4f},{roc_auc:.4f},{pr_auc:.4f}\n",
    "\")\n",
    "\n",
    "        return eval_spearman\n",
    "\n",
    "# Initialize Evaluator\n",
    "log_step(\"[Cell 15] Initializing evaluator...\")\n",
    "evaluator = ComprehensiveEvaluator(eval_data, batch_size=CONFIG['batch_size'], name='dev')\n",
    "log_step(f\"Evaluator ready on {len(eval_data)} pairs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define Output Path\n",
    "stamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "save_path = f\"{CONFIG['output_dir']}_{stamp}\"\n",
    "\n",
    "start_msg = f\"üöÄ Starting training... Saving to {save_path}\"\n",
    "log_step(start_msg)\n",
    "log_step(f\"Training config: epochs={CONFIG['epochs']}, batch_size={CONFIG['batch_size']}, eval_steps={CONFIG['eval_steps']}, warmup_ratio={CONFIG['warmup_ratio']}\")\n",
    "logger.info(f\"Train batches per epoch: {len(train_dataloader)}\")\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    warmup_steps=int(len(train_dataloader) * CONFIG['epochs'] * CONFIG['warmup_ratio']),\n",
    "    optimizer_params={'lr': CONFIG['learning_rate'], 'weight_decay': CONFIG['weight_decay']},\n",
    "    output_path=save_path,\n",
    "    evaluation_steps=CONFIG['eval_steps'],\n",
    "    save_best_model=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "logger.info('‚úÖ Training complete.')\n",
    "print('‚úÖ Training complete.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation & Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Reload Best Model\n",
    "log_step(f\"[Cell 19] Reloading best model from {save_path} for final evaluation...\")\n",
    "best_model = SentenceTransformer(save_path)\n",
    "\n",
    "# Encode Eval Data\n",
    "log_step(f\"Encoding {len(eval_data)} evaluation pairs for final metrics...\")\n",
    "eval_texts1 = [ex.texts[0] for ex in eval_data]\n",
    "eval_texts2 = [ex.texts[1] for ex in eval_data]\n",
    "eval_labels = np.array([ex.label for ex in eval_data])\n",
    "\n",
    "logger.info(f\"Encoding {len(eval_texts1)} evaluation pairs...\")\n",
    "embeddings1 = best_model.encode(eval_texts1, batch_size=CONFIG['batch_size'], show_progress_bar=True)\n",
    "embeddings2 = best_model.encode(eval_texts2, batch_size=CONFIG['batch_size'], show_progress_bar=True)\n",
    "\n",
    "# Proper cosine similarity (normalized)\n",
    "cosine_scores = np.sum(embeddings1 * embeddings2, axis=1) / (np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1))\n",
    "\n",
    "# Final metrics (match v5-style reporting)\n",
    "final_spearman, _ = spearmanr(eval_labels, cosine_scores)\n",
    "final_pearson, _ = pearsonr(eval_labels, cosine_scores)\n",
    "final_roc_auc = roc_auc_score(eval_labels, cosine_scores)\n",
    "final_pr_auc = average_precision_score(eval_labels, cosine_scores)\n",
    "\n",
    "log_step(\"üìä Final Evaluation Metrics:\")\n",
    "log_step(f\"   Spearman Correlation: {final_spearman:.4f}\")\n",
    "log_step(f\"   Pearson Correlation: {final_pearson:.4f}\")\n",
    "log_step(f\"   ROC AUC: {final_roc_auc:.4f}\")\n",
    "log_step(f\"   PR AUC: {final_pr_auc:.4f}\")\n",
    "logger.info(f\"Final metrics - Spearman: {final_spearman:.4f}, Pearson: {final_pearson:.4f}, ROC AUC: {final_roc_auc:.4f}, PR AUC: {final_pr_auc:.4f}\")\n",
    "\n",
    "# --- Plots ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Distribution\n",
    "sns.histplot(cosine_scores[eval_labels==1], color='green', label='Positive', kde=True, ax=axes[0])\n",
    "sns.histplot(cosine_scores[eval_labels==0], color='red', label='Negative', kde=True, ax=axes[0])\n",
    "axes[0].set_title('Cosine Similarity Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(eval_labels, cosine_scores)\n",
    "roc_auc = roc_auc_score(eval_labels, cosine_scores)\n",
    "axes[1].plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. Precision-Recall\n",
    "precision, recall, _ = precision_recall_curve(eval_labels, cosine_scores)\n",
    "pr_auc = average_precision_score(eval_labels, cosine_scores)\n",
    "axes[2].plot(recall, precision, label=f'PR AUC = {pr_auc:.3f}')\n",
    "axes[2].set_title('Precision-Recall Curve')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Relationship Classifier (Optional)\n",
    "Trains a secondary classifier to predict relationship types (duplicate, causal, related).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if IMBLEARN_AVAILABLE and os.path.exists(CONFIG['relationship_data']):\n",
    "    log_step('üß† Training Relationship Classifier...')\n",
    "    with open(CONFIG['relationship_data'], 'r') as f:\n",
    "        rel_data = json.load(f)\n",
    "\n",
    "    rel_df = pd.DataFrame(rel_data)\n",
    "    # Filter valid labels\n",
    "    valid_labels = ['duplicate', 'causal', 'related', 'none']\n",
    "    rel_df = rel_df[rel_df['label'].isin(valid_labels)]\n",
    "    log_step(f\"Relationship samples after filtering: {len(rel_df)}\")\n",
    "\n",
    "    # Encode features using fine-tuned model\n",
    "    text_a = rel_df['text_a'].tolist()\n",
    "    text_b = rel_df['text_b'].tolist()\n",
    "\n",
    "    emb_a = best_model.encode(text_a)\n",
    "    emb_b = best_model.encode(text_b)\n",
    "\n",
    "    # Feature Engineering: (u, v, |u-v|, u*v)\n",
    "    X = np.hstack([emb_a, emb_b, np.abs(emb_a - emb_b), emb_a * emb_b])\n",
    "    y = rel_df['label']\n",
    "\n",
    "    # SMOTE Balancing\n",
    "    smote = SMOTE(k_neighbors=2, random_state=42)\n",
    "    X_res, y_res = smote.fit_resample(X, y)\n",
    "    log_step(f\"After SMOTE: {len(X_res)} samples\")\n",
    "\n",
    "    # Train Classifier\n",
    "    clf = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Save\n",
    "    joblib.dump(clf, os.path.join(save_path, 'relationship_classifier.joblib'))\n",
    "    log_step('‚úÖ Relationship classifier saved.')\n",
    "else:\n",
    "    log_step('‚ö†Ô∏è Skipping relationship classifier (missing data or imbalanced-learn).')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

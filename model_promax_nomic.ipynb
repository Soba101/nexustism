{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184dfd2c",
   "metadata": {},
   "source": [
    "# üöÄ ITSM Ticket Similarity - Real ServiceNow Data Training (V2)\n",
    "\n",
    "**Model:** `nomic-ai/nomic-embed-text-v1.5` (768-dim embeddings, 8192 token context)  \n",
    "**Data:** Real ServiceNow incidents (Mar 2024 ‚Üí Sep 2025, ~30K+ tickets)  \n",
    "**Use Case:** Find similar tickets, detect duplicates, assist routing\n",
    "\n",
    "## V2 Improvements over V1\n",
    "1. **Harder negative mining** ‚Äî Increased hard_neg_ratio (35%‚Üí45%), stricter TF-IDF threshold (0.20‚Üí0.12)\n",
    "2. **Curriculum learning** ‚Äî Progressive difficulty: easy pairs first, hard pairs later\n",
    "3. **Borderline test set** ‚Äî New \"adversarial v2\" with TF-IDF 0.25-0.35 (ambiguous cases)\n",
    "4. **Error analysis** ‚Äî Systematic failure pattern detection\n",
    "5. **Cross-validated threshold** ‚Äî 5-fold CV for robust production threshold\n",
    "\n",
    "## Key Differences from Dummy Data Pipeline\n",
    "1. **No Short Description** ‚Äî Real data only has `Description` field\n",
    "2. **Rich vocabulary** ‚Äî 10K+ unique terms vs 111 in dummy data\n",
    "3. **Realistic metrics expected** ‚Äî ROC-AUC 0.85-0.90, Spearman 0.65-0.75\n",
    "4. **No Resolution notes in training text** ‚Äî Avoids data leakage for new tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9dc0f3",
   "metadata": {},
   "source": [
    "# 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118e0c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing: einops\n"
     ]
    }
   ],
   "source": [
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment variables to suppress warnings\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_SILENT'] = 'true'\n",
    "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Install required packages\n",
    "def ensure_packages():\n",
    "    try:\n",
    "        import importlib.metadata as importlib_metadata\n",
    "    except ImportError:\n",
    "        import importlib_metadata\n",
    "    \n",
    "    required = {\n",
    "        'sentence-transformers': 'sentence-transformers>=2.2.2',\n",
    "        'torch': 'torch',\n",
    "        'scikit-learn': 'scikit-learn>=1.3.0',\n",
    "        'pandas': 'pandas',\n",
    "        'numpy': 'numpy>=1.24.0',\n",
    "        'tqdm': 'tqdm',\n",
    "        'matplotlib': 'matplotlib',\n",
    "        'seaborn': 'seaborn',\n",
    "        'einops': 'einops',\n",
    "    }\n",
    "    \n",
    "    missing = []\n",
    "    for name, spec in required.items():\n",
    "        try:\n",
    "            importlib_metadata.version(name)\n",
    "        except importlib_metadata.PackageNotFoundError:\n",
    "            missing.append(spec)\n",
    "    \n",
    "    if missing:\n",
    "        print(f'üì¶ Installing: {\", \".join(missing)}')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', *missing])\n",
    "    else:\n",
    "        print('‚úÖ All packages installed')\n",
    "\n",
    "ensure_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1ad25",
   "metadata": {},
   "source": [
    "# 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae876b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ CUDA Detected: NVIDIA GeForce RTX 2080 SUPER\n",
      "üìä Device: cuda, Batch Size: 16\n",
      "\n",
      "üÜï V2 Improvements Active:\n",
      "   ‚Ä¢ Harder negatives: 45% hard neg ratio\n",
      "   ‚Ä¢ Stricter threshold: neg_tfidf < 0.12\n",
      "   ‚Ä¢ Curriculum learning: True\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Simple logging\n",
    "def log(msg, level=logging.INFO):\n",
    "    print(msg)\n",
    "\n",
    "# --- CONFIGURATION (V2 - Improved) ---\n",
    "CONFIG = {\n",
    "    # Model - Using Nomic Embed v1.5 (768-dim, 8192 token context)\n",
    "    'model_name': 'nomic-ai/nomic-embed-text-v1.5',\n",
    "    'output_dir': 'models/real_servicenow_finetuned_nomic',\n",
    "    \n",
    "    # Data\n",
    "    'source_data': 'data_new\\\\SNow_incident_ticket_data.csv',\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'epochs': 4,              # Real data needs fewer epochs (more signal)\n",
    "    'batch_size': 16,         # Will auto-reduce for MPS if needed\n",
    "    'lr': 2e-5,\n",
    "    'max_seq_length': 512,    # Nomic supports up to 8192, but 512 is good for ITSM tickets\n",
    "    'warmup_ratio': 0.1,\n",
    "    \n",
    "    # Pair generation (V2: HARDER NEGATIVES)\n",
    "    'num_pairs': 50000,       # 50K pairs for real data\n",
    "    'pos_ratio': 0.30,        # 30% positives (reduced from 35%)\n",
    "    'hard_neg_ratio': 0.45,   # 45% hard negatives (increased from 35%)\n",
    "    'easy_neg_ratio': 0.25,   # 25% easy negatives (reduced from 30%)\n",
    "    'pos_tfidf_threshold': 0.35,   # Lower threshold for real data (more variety)\n",
    "    'neg_tfidf_threshold': 0.12,   # STRICTER: Upper bound for negatives (was 0.20)\n",
    "    \n",
    "    # Curriculum learning (V2: NEW)\n",
    "    'use_curriculum': True,\n",
    "    'curriculum_phases': [\n",
    "        {'epochs': 2, 'hard_neg_ratio': 0.25, 'neg_threshold': 0.15},  # Phase 1: Easier\n",
    "        {'epochs': 2, 'hard_neg_ratio': 0.55, 'neg_threshold': 0.10},  # Phase 2: Harder\n",
    "    ],\n",
    "    \n",
    "    # Data splits\n",
    "    'eval_split': 0.15,\n",
    "    'holdout_split': 0.10,\n",
    "    \n",
    "    # Minimum text length (filter short descriptions)\n",
    "    'min_text_length': 25,\n",
    "    \n",
    "    # Cross-validation for threshold (V2: NEW)\n",
    "    'threshold_cv_folds': 5,\n",
    "    \n",
    "    # Seed\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "\n",
    "# Device detection: CUDA > MPS > CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    torch.cuda.manual_seed_all(CONFIG['seed'])\n",
    "    log(f\"üöÄ CUDA Detected: {torch.cuda.get_device_name(0)}\")\n",
    "    # Keep batch_size=16 for CUDA\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    torch.mps.manual_seed(CONFIG['seed'])\n",
    "    log(\"üçé MPS (Apple Silicon) Detected\")\n",
    "    CONFIG['batch_size'] = 8  # Reduce for MPS memory\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    log(\"‚ö†Ô∏è No GPU detected. Running on CPU.\")\n",
    "    CONFIG['batch_size'] = 8\n",
    "\n",
    "log(f\"üìä Device: {DEVICE}, Batch Size: {CONFIG['batch_size']}\")\n",
    "log(f\"\\nüÜï V2 Improvements Active:\")\n",
    "log(f\"   ‚Ä¢ Harder negatives: {CONFIG['hard_neg_ratio']*100:.0f}% hard neg ratio\")\n",
    "log(f\"   ‚Ä¢ Stricter threshold: neg_tfidf < {CONFIG['neg_tfidf_threshold']}\")\n",
    "log(f\"   ‚Ä¢ Curriculum learning: {CONFIG['use_curriculum']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b3f39",
   "metadata": {},
   "source": [
    "# 3. Data Loading & Preprocessing\n",
    "\n",
    "Load real ServiceNow incident data. Key differences from dummy data:\n",
    "- Only `Description` field (no Short Description)\n",
    "- Multi-line text with embedded newlines\n",
    "- Rich technical vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e629653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Looking for data file:\n",
      "   Configured path: data_new\\SNow_incident_ticket_data.csv\n",
      "   Absolute path: C:\\Users\\donov\\Downloads\\nexustism\\nexustism\\data_new\\SNow_incident_ticket_data.csv\n",
      "   Current working directory: c:\\Users\\donov\\Downloads\\nexustism\\nexustism\n",
      "   File exists: True\n",
      "üìÇ Loading real ServiceNow data from: data_new\\SNow_incident_ticket_data.csv\n",
      "üìä Loaded 10,633 raw records\n",
      "‚úÖ Available columns: ['Number', 'Description', 'Opened by', 'Company', 'ITSM Department', 'Created', 'Urgency', 'Impact', 'Priority', 'Assignment group', 'Assigned to', 'State', 'Service', 'Service offering', 'Closed', 'Closed by', 'Category', 'Subcategory', 'Resolution code', 'Resolution notes', 'User input', 'Comments and Work notes', 'Manday Effort (hrs)', 'Ticket Type', 'AMS Domain', 'AMS System Type', 'AMS Category Type', 'AMS Service Type', 'AMS Business Related', 'AMS IT Related']\n",
      "üìâ After filtering short descriptions: 10,486 records (dropped 147)\n",
      "\n",
      "üìä Data Summary:\n",
      "   Total records: 10,486\n",
      "   Unique categories: 30\n",
      "   Avg text length: 561 chars\n",
      "   Min text length: 66 chars\n",
      "   Max text length: 15545 chars\n",
      "\n",
      "üìù Sample preprocessed text:\n",
      "   'GRPT not working as expected. ZMMM_PO_REV is not generating correct dates as per maintained in GRPT table. E.g. P/O# 100024066 Vendor Ship mode is 03. As per GRPT route days are 12 days and GR days is...'\n",
      "\n",
      "‚úÖ Loaded 10,486 incidents ready for training\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def load_and_preprocess_real_data(config):\n",
    "    \"\"\"\n",
    "    Load and preprocess real ServiceNow incident data.\n",
    "    \n",
    "    Key differences from dummy data:\n",
    "    - Only 'Description' column (no 'Short Description')\n",
    "    - Real data has multi-line descriptions with embedded newlines\n",
    "    - Richer vocabulary and more varied text\n",
    "    \"\"\"\n",
    "    source_path = Path(config['source_data'])\n",
    "    \n",
    "    # Debug: Show absolute path\n",
    "    log(f\"üîç Looking for data file:\")\n",
    "    log(f\"   Configured path: {config['source_data']}\")\n",
    "    log(f\"   Absolute path: {source_path.resolve()}\")\n",
    "    log(f\"   Current working directory: {Path.cwd()}\")\n",
    "    log(f\"   File exists: {source_path.exists()}\")\n",
    "    \n",
    "    if not source_path.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {source_path.resolve()}\")\n",
    "    \n",
    "    log(f\"üìÇ Loading real ServiceNow data from: {source_path}\")\n",
    "    \n",
    "    # Load CSV - handle multi-line fields\n",
    "    df = pd.read_csv(source_path, encoding='utf-8', on_bad_lines='skip')\n",
    "    initial_count = len(df)\n",
    "    log(f\"üìä Loaded {initial_count:,} raw records\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['Number', 'Description', 'Category', 'Subcategory', \n",
    "                     'Service', 'Service offering', 'Assignment group']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        log(f\"‚ö†Ô∏è Missing columns: {missing_cols}\")\n",
    "        # Use available columns\n",
    "        required_cols = [col for col in required_cols if col in df.columns]\n",
    "    \n",
    "    log(f\"‚úÖ Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Clean text function\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text) or text is None:\n",
    "            return \"\"\n",
    "        text = str(text).strip()\n",
    "        # Normalize whitespace (collapse multiple spaces/newlines)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove common boilerplate\n",
    "        text = re.sub(r'Note\\s*:\\s*This is an automated.*?\\.', '', text, flags=re.IGNORECASE)\n",
    "        return text.strip()\n",
    "    \n",
    "    # Clean Description\n",
    "    df['Description'] = df['Description'].apply(clean_text)\n",
    "    \n",
    "    # Fill NA for context columns\n",
    "    context_cols = ['Category', 'Subcategory', 'Service', 'Service offering', 'Assignment group']\n",
    "    for col in context_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('').astype(str).str.strip().str.lower()\n",
    "    \n",
    "    # Build contextual text representation\n",
    "    # Format: \"Description (Context: [Service|Offering] [Category|Subcategory] Group: X)\"\n",
    "    def build_context(row):\n",
    "        parts = []\n",
    "        \n",
    "        # Service context\n",
    "        service_parts = []\n",
    "        if row.get('Service', ''):\n",
    "            service_parts.append(row['Service'])\n",
    "        if row.get('Service offering', ''):\n",
    "            service_parts.append(row['Service offering'])\n",
    "        if service_parts:\n",
    "            parts.append(f\"[{' | '.join(service_parts)}]\")\n",
    "        \n",
    "        # Category context\n",
    "        cat_parts = []\n",
    "        if row.get('Category', ''):\n",
    "            cat_parts.append(row['Category'])\n",
    "        if row.get('Subcategory', ''):\n",
    "            cat_parts.append(row['Subcategory'])\n",
    "        if cat_parts:\n",
    "            parts.append(f\"[{' | '.join(cat_parts)}]\")\n",
    "        \n",
    "        # Assignment group\n",
    "        if row.get('Assignment group', ''):\n",
    "            parts.append(f\"Group: {row['Assignment group']}\")\n",
    "        \n",
    "        return ' '.join(parts) if parts else ''\n",
    "    \n",
    "    # Build full text: Description + Context suffix\n",
    "    df['context'] = df.apply(build_context, axis=1)\n",
    "    df['text'] = df.apply(\n",
    "        lambda row: f\"{row['Description']} (Context: {row['context']})\" if row['context'] else row['Description'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Filter out short/empty descriptions\n",
    "    df = df[df['Description'].str.len() >= config['min_text_length']].copy()\n",
    "    log(f\"üìâ After filtering short descriptions: {len(df):,} records (dropped {initial_count - len(df):,})\")\n",
    "    \n",
    "    # Create category_id for stratified splitting\n",
    "    if 'Category' in df.columns and 'Subcategory' in df.columns:\n",
    "        df['category_id'] = df.groupby(['Category', 'Subcategory']).ngroup()\n",
    "    else:\n",
    "        df['category_id'] = 0\n",
    "    \n",
    "    # Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Summary stats\n",
    "    log(f\"\\nüìä Data Summary:\")\n",
    "    log(f\"   Total records: {len(df):,}\")\n",
    "    log(f\"   Unique categories: {df['category_id'].nunique()}\")\n",
    "    log(f\"   Avg text length: {df['text'].str.len().mean():.0f} chars\")\n",
    "    log(f\"   Min text length: {df['text'].str.len().min()} chars\")\n",
    "    log(f\"   Max text length: {df['text'].str.len().max()} chars\")\n",
    "    \n",
    "    # Sample text\n",
    "    log(f\"\\nüìù Sample preprocessed text:\")\n",
    "    log(f\"   '{df['text'].iloc[0][:200]}...'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "df_incidents = load_and_preprocess_real_data(CONFIG)\n",
    "print(f\"\\n‚úÖ Loaded {len(df_incidents):,} incidents ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35dc0c",
   "metadata": {},
   "source": [
    "# 4. Data Splitting\n",
    "\n",
    "Split into Train / Eval / Holdout sets with stratification by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d80a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using stratified split\n",
      "üìä Data Splits:\n",
      "   Train:   8,021 incidents (76.5%)\n",
      "   Eval:    1,416 incidents (13.5%)\n",
      "   Holdout: 1,049 incidents (10.0%)\n",
      "\n",
      "üîç Overlap Check:\n",
      "   Train ‚à© Eval: 0 incidents\n",
      "   Train ‚à© Holdout: 0 incidents\n",
      "   Eval ‚à© Holdout: 0 incidents\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df, config):\n",
    "    \"\"\"\n",
    "    Three-way split: Train / Eval / Holdout\n",
    "    - Holdout is completely unseen (for adversarial diagnostic)\n",
    "    - Stratified by category to ensure representation\n",
    "    \"\"\"\n",
    "    # Handle rare categories: group categories with <2 samples\n",
    "    category_counts = df['category_id'].value_counts()\n",
    "    rare_categories = category_counts[category_counts < 2].index\n",
    "    \n",
    "    # Create stratification column: use category_id for common categories, -1 for rare\n",
    "    df['stratify_col'] = df['category_id'].copy()\n",
    "    df.loc[df['category_id'].isin(rare_categories), 'stratify_col'] = -1\n",
    "    \n",
    "    # Check if we can stratify (need at least 2 samples per class)\n",
    "    stratify_counts = df['stratify_col'].value_counts()\n",
    "    can_stratify = all(stratify_counts >= 2)\n",
    "    \n",
    "    if can_stratify:\n",
    "        # First split: separate holdout set (stratified)\n",
    "        train_eval_df, holdout_df = train_test_split(\n",
    "            df,\n",
    "            test_size=config['holdout_split'],\n",
    "            stratify=df['stratify_col'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "        \n",
    "        # Second split: train/eval from remaining (stratified)\n",
    "        train_df, eval_df = train_test_split(\n",
    "            train_eval_df,\n",
    "            test_size=config['eval_split'],\n",
    "            stratify=train_eval_df['stratify_col'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "        log(\"‚úÖ Using stratified split\")\n",
    "    else:\n",
    "        # Fallback: random split without stratification\n",
    "        log(\"‚ö†Ô∏è Using random split (categories too imbalanced for stratification)\")\n",
    "        train_eval_df, holdout_df = train_test_split(\n",
    "            df,\n",
    "            test_size=config['holdout_split'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "        \n",
    "        train_df, eval_df = train_test_split(\n",
    "            train_eval_df,\n",
    "            test_size=config['eval_split'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "    \n",
    "    return train_df, eval_df, holdout_df\n",
    "\n",
    "# Split the data\n",
    "train_df, eval_df, holdout_df = split_data(df_incidents, CONFIG)\n",
    "\n",
    "log(f\"üìä Data Splits:\")\n",
    "log(f\"   Train:   {len(train_df):,} incidents ({len(train_df)/len(df_incidents)*100:.1f}%)\")\n",
    "log(f\"   Eval:    {len(eval_df):,} incidents ({len(eval_df)/len(df_incidents)*100:.1f}%)\")\n",
    "log(f\"   Holdout: {len(holdout_df):,} incidents ({len(holdout_df)/len(df_incidents)*100:.1f}%)\")\n",
    "\n",
    "# Check for data leakage\n",
    "def check_overlap(df1, df2, name1, name2):\n",
    "    overlap = len(set(df1['Number']) & set(df2['Number']))\n",
    "    log(f\"   {name1} ‚à© {name2}: {overlap} incidents\")\n",
    "\n",
    "log(f\"\\nüîç Overlap Check:\")\n",
    "check_overlap(train_df, eval_df, \"Train\", \"Eval\")\n",
    "check_overlap(train_df, holdout_df, \"Train\", \"Holdout\")\n",
    "check_overlap(eval_df, holdout_df, \"Eval\", \"Holdout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfadea",
   "metadata": {},
   "source": [
    "# 5. Pair Generation\n",
    "\n",
    "Generate training pairs using TF-IDF similarity mining:\n",
    "- **35% Positives**: High TF-IDF similarity (> 0.35)\n",
    "- **35% Hard Negatives**: Same category, low TF-IDF (< 0.20)\n",
    "- **30% Easy Negatives**: Different category, low TF-IDF\n",
    "\n",
    "This forces the model to learn semantic content, not just category matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff4c6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\donov\\anaconda3\\envs\\itsm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Running in WANDB offline mode\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import InputExample\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "class TFIDFSimilarityCalculator:\n",
    "    \"\"\"Efficient TF-IDF similarity calculator for large datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, max_features=15000):\n",
    "        log(\"‚è≥ Building TF-IDF matrix...\")\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),  # Unigrams + bigrams for better matching\n",
    "            min_df=2,           # Ignore very rare terms\n",
    "            max_df=0.95         # Ignore very common terms\n",
    "        )\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "        log(f\"‚úÖ TF-IDF matrix: {self.tfidf_matrix.shape} (vocab size: {len(self.vectorizer.vocabulary_)})\")\n",
    "    \n",
    "    def similarity(self, idx1, idx2):\n",
    "        \"\"\"Compute cosine similarity between two documents.\"\"\"\n",
    "        if idx1 >= self.tfidf_matrix.shape[0] or idx2 >= self.tfidf_matrix.shape[0]:\n",
    "            return 0.0\n",
    "        vec1 = self.tfidf_matrix[idx1]\n",
    "        vec2 = self.tfidf_matrix[idx2]\n",
    "        return (vec1 @ vec2.T).toarray()[0][0]\n",
    "\n",
    "\n",
    "def generate_training_pairs(df, target_count, config, desc=\"\", phase_config=None):\n",
    "    \"\"\"\n",
    "    Generate training pairs with configurable split (V2: supports curriculum phases).\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with ticket data\n",
    "        target_count: Total number of pairs to generate\n",
    "        config: Main configuration dict\n",
    "        desc: Description for logging\n",
    "        phase_config: Optional override for curriculum learning phase\n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Use phase config if provided (for curriculum learning)\n",
    "    pos_ratio = config['pos_ratio']\n",
    "    hard_neg_ratio = phase_config['hard_neg_ratio'] if phase_config else config['hard_neg_ratio']\n",
    "    neg_threshold = phase_config['neg_threshold'] if phase_config else config['neg_tfidf_threshold']\n",
    "    easy_neg_ratio = 1.0 - pos_ratio - hard_neg_ratio\n",
    "    \n",
    "    # Build TF-IDF for this split\n",
    "    tfidf_calc = TFIDFSimilarityCalculator(df['text'].tolist())\n",
    "    \n",
    "    # Calculate targets\n",
    "    pos_target = int(target_count * pos_ratio)\n",
    "    hard_neg_target = int(target_count * hard_neg_ratio)\n",
    "    easy_neg_target = target_count - pos_target - hard_neg_target\n",
    "    \n",
    "    pairs = []\n",
    "    all_indices = list(df.index)\n",
    "    \n",
    "    # Group by category for hard negatives\n",
    "    category_groups = df.groupby('category_id').indices\n",
    "    valid_groups = {k: list(v) for k, v in category_groups.items() if len(v) >= 2}\n",
    "    \n",
    "    log(f\"\\nüéØ Generating {target_count:,} pairs for {desc}:\")\n",
    "    log(f\"   Target: {pos_target:,} positives ({pos_ratio*100:.0f}%), {hard_neg_target:,} hard neg ({hard_neg_ratio*100:.0f}%), {easy_neg_target:,} easy neg\")\n",
    "    log(f\"   Neg TF-IDF threshold: {neg_threshold}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # 1. POSITIVES: High TF-IDF similarity (> threshold)\n",
    "    # ============================================\n",
    "    pbar = tqdm(total=pos_target, desc=\"Positives (high TF-IDF)\")\n",
    "    attempts, max_attempts = 0, pos_target * 50\n",
    "    \n",
    "    while len(pairs) < pos_target and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        i1, i2 = random.sample(all_indices, 2)\n",
    "        if i1 == i2:\n",
    "            continue\n",
    "        \n",
    "        sim = tfidf_calc.similarity(i1, i2)\n",
    "        if sim > config['pos_tfidf_threshold']:\n",
    "            pairs.append(InputExample(\n",
    "                texts=[df.at[i1, 'text'], df.at[i2, 'text']],\n",
    "                label=1.0\n",
    "            ))\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    actual_pos = len(pairs)\n",
    "    \n",
    "    # ============================================\n",
    "    # 2. HARD NEGATIVES: Same category, low TF-IDF (V2: stricter threshold)\n",
    "    # ============================================\n",
    "    current_len = len(pairs)\n",
    "    pbar = tqdm(total=hard_neg_target, desc=f\"Hard Negatives (same cat, TF-IDF<{neg_threshold})\")\n",
    "    attempts, max_attempts = 0, hard_neg_target * 50\n",
    "    \n",
    "    while (len(pairs) - current_len) < hard_neg_target and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        if not valid_groups:\n",
    "            break\n",
    "        \n",
    "        # Pick a random category with 2+ members\n",
    "        gid = random.choice(list(valid_groups.keys()))\n",
    "        g_indices = valid_groups[gid]\n",
    "        if len(g_indices) < 2:\n",
    "            continue\n",
    "        \n",
    "        i1, i2 = random.sample(g_indices, 2)\n",
    "        sim = tfidf_calc.similarity(i1, i2)\n",
    "        \n",
    "        if sim < neg_threshold:  # V2: Uses stricter threshold\n",
    "            pairs.append(InputExample(\n",
    "                texts=[df.at[i1, 'text'], df.at[i2, 'text']],\n",
    "                label=0.0\n",
    "            ))\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    actual_hard = len(pairs) - current_len\n",
    "    \n",
    "    # ============================================\n",
    "    # 3. EASY NEGATIVES: Cross-category, low TF-IDF\n",
    "    # ============================================\n",
    "    current_len = len(pairs)\n",
    "    pbar = tqdm(total=easy_neg_target, desc=\"Easy Negatives (cross-cat)\")\n",
    "    attempts, max_attempts = 0, easy_neg_target * 50\n",
    "    \n",
    "    while (len(pairs) - current_len) < easy_neg_target and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        i1, i2 = random.sample(all_indices, 2)\n",
    "        \n",
    "        # Must be different categories\n",
    "        if df.at[i1, 'category_id'] == df.at[i2, 'category_id']:\n",
    "            continue\n",
    "        \n",
    "        sim = tfidf_calc.similarity(i1, i2)\n",
    "        if sim < neg_threshold:\n",
    "            pairs.append(InputExample(\n",
    "                texts=[df.at[i1, 'text'], df.at[i2, 'text']],\n",
    "                label=0.0\n",
    "            ))\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    actual_easy = len(pairs) - current_len\n",
    "    \n",
    "    # Summary\n",
    "    total_pos = sum(1 for p in pairs if p.label == 1.0)\n",
    "    total_neg = len(pairs) - total_pos\n",
    "    \n",
    "    log(f\"‚úÖ Generated {len(pairs):,} pairs:\")\n",
    "    log(f\"   Positives: {actual_pos:,} ({actual_pos/len(pairs)*100:.1f}%)\")\n",
    "    log(f\"   Hard Neg:  {actual_hard:,} ({actual_hard/len(pairs)*100:.1f}%)\")\n",
    "    log(f\"   Easy Neg:  {actual_easy:,} ({actual_easy/len(pairs)*100:.1f}%)\")\n",
    "    \n",
    "    # Clean up TF-IDF to free memory\n",
    "    del tfidf_calc\n",
    "    gc.collect()\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "def generate_borderline_pairs(df, target_count, config, desc=\"Borderline\"):\n",
    "    \"\"\"\n",
    "    V2 NEW: Generate borderline/ambiguous pairs for harder evaluation.\n",
    "    \n",
    "    These are pairs with TF-IDF similarity in the 0.25-0.35 range - \n",
    "    cases where it's genuinely hard to determine similarity.\n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    tfidf_calc = TFIDFSimilarityCalculator(df['text'].tolist())\n",
    "    \n",
    "    borderline_pairs = []\n",
    "    all_indices = list(df.index)\n",
    "    \n",
    "    # TF-IDF range for borderline cases\n",
    "    low_threshold = 0.25\n",
    "    high_threshold = 0.35\n",
    "    \n",
    "    log(f\"\\nüéØ Generating {target_count:,} borderline pairs ({desc}):\")\n",
    "    log(f\"   TF-IDF range: {low_threshold} - {high_threshold}\")\n",
    "    \n",
    "    pbar = tqdm(total=target_count, desc=\"Borderline pairs\")\n",
    "    attempts, max_attempts = 0, target_count * 100\n",
    "    \n",
    "    while len(borderline_pairs) < target_count and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        i1, i2 = random.sample(all_indices, 2)\n",
    "        if i1 == i2:\n",
    "            continue\n",
    "        \n",
    "        sim = tfidf_calc.similarity(i1, i2)\n",
    "        \n",
    "        # Borderline: medium TF-IDF similarity (ambiguous)\n",
    "        if low_threshold <= sim <= high_threshold:\n",
    "            # Label based on same category (proxy for similarity)\n",
    "            same_cat = df.at[i1, 'category_id'] == df.at[i2, 'category_id']\n",
    "            label = 1.0 if same_cat else 0.0\n",
    "            \n",
    "            borderline_pairs.append(InputExample(\n",
    "                texts=[df.at[i1, 'text'], df.at[i2, 'text']],\n",
    "                label=label\n",
    "            ))\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    pos_count = sum(1 for p in borderline_pairs if p.label == 1.0)\n",
    "    log(f\"‚úÖ Generated {len(borderline_pairs):,} borderline pairs:\")\n",
    "    log(f\"   Positives: {pos_count:,} ({pos_count/len(borderline_pairs)*100:.1f}%)\")\n",
    "    log(f\"   Negatives: {len(borderline_pairs)-pos_count:,}\")\n",
    "    \n",
    "    del tfidf_calc\n",
    "    gc.collect()\n",
    "    \n",
    "    return borderline_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1053863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Building TF-IDF matrix...\n",
      "‚úÖ TF-IDF matrix: (8021, 15000) (vocab size: 15000)\n",
      "\n",
      "üéØ Generating 37,500 pairs for Training:\n",
      "   Target: 11,250 positives (30%), 16,875 hard neg (45%), 9,375 easy neg\n",
      "   Neg TF-IDF threshold: 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positives (high TF-IDF):  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 6966/11250 [02:09<01:19, 53.63it/s]\n",
      "Hard Negatives (same cat, TF-IDF<0.12): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16875/16875 [00:07<00:00, 2270.13it/s]\n",
      "Easy Negatives (cross-cat): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9375/9375 [00:02<00:00, 3165.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 33,216 pairs:\n",
      "   Positives: 6,966 (21.0%)\n",
      "   Hard Neg:  16,875 (50.8%)\n",
      "   Easy Neg:  9,375 (28.2%)\n",
      "‚è≥ Building TF-IDF matrix...\n",
      "‚úÖ TF-IDF matrix: (1416, 5532) (vocab size: 5532)\n",
      "\n",
      "üéØ Generating 7,500 pairs for Evaluation:\n",
      "   Target: 2,250 positives (30%), 3,375 hard neg (45%), 1,875 easy neg\n",
      "   Neg TF-IDF threshold: 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positives (high TF-IDF):  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 2230/2250 [00:26<00:00, 84.91it/s] \n",
      "Hard Negatives (same cat, TF-IDF<0.12): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3375/3375 [00:01<00:00, 1886.43it/s]\n",
      "Easy Negatives (cross-cat): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1875/1875 [00:00<00:00, 3150.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 7,480 pairs:\n",
      "   Positives: 2,230 (29.8%)\n",
      "   Hard Neg:  3,375 (45.1%)\n",
      "   Easy Neg:  1,875 (25.1%)\n",
      "‚è≥ Building TF-IDF matrix...\n",
      "‚úÖ TF-IDF matrix: (1049, 5256) (vocab size: 5256)\n",
      "\n",
      "üéØ Generating 5,000 pairs for Holdout:\n",
      "   Target: 1,500 positives (30%), 2,250 hard neg (45%), 1,250 easy neg\n",
      "   Neg TF-IDF threshold: 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positives (high TF-IDF): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [00:16<00:00, 93.49it/s] \n",
      "Hard Negatives (same cat, TF-IDF<0.12): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2250/2250 [00:01<00:00, 1756.70it/s]\n",
      "Easy Negatives (cross-cat): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [00:00<00:00, 3059.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 5,000 pairs:\n",
      "   Positives: 1,500 (30.0%)\n",
      "   Hard Neg:  2,250 (45.0%)\n",
      "   Easy Neg:  1,250 (25.0%)\n",
      "‚è≥ Building TF-IDF matrix...\n",
      "‚úÖ TF-IDF matrix: (1049, 5256) (vocab size: 5256)\n",
      "\n",
      "üéØ Generating 2,500 borderline pairs (Borderline Test):\n",
      "   TF-IDF range: 0.25 - 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Borderline pairs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [00:15<00:00, 156.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 2,500 borderline pairs:\n",
      "   Positives: 727 (29.1%)\n",
      "   Negatives: 1,773\n",
      "\n",
      "üì¶ Final Pair Counts:\n",
      "   Train:      33,216\n",
      "   Eval:       7,480\n",
      "   Holdout:    5,000\n",
      "   Borderline: 2,500 (V2 NEW - harder test)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate pairs for each split\n",
    "# Scale pair counts based on split sizes\n",
    "train_pair_count = int(CONFIG['num_pairs'] * 0.75)  # 75% for training\n",
    "eval_pair_count = int(CONFIG['num_pairs'] * 0.15)   # 15% for eval\n",
    "holdout_pair_count = int(CONFIG['num_pairs'] * 0.10) # 10% for holdout\n",
    "\n",
    "train_examples = generate_training_pairs(train_df, train_pair_count, CONFIG, desc=\"Training\")\n",
    "eval_examples = generate_training_pairs(eval_df, eval_pair_count, CONFIG, desc=\"Evaluation\")\n",
    "holdout_examples = generate_training_pairs(holdout_df, holdout_pair_count, CONFIG, desc=\"Holdout\")\n",
    "\n",
    "# V2 NEW: Generate borderline test set for harder evaluation\n",
    "borderline_count = int(CONFIG['num_pairs'] * 0.05)  # 5% as borderline test\n",
    "borderline_examples = generate_borderline_pairs(holdout_df, borderline_count, CONFIG, desc=\"Borderline Test\")\n",
    "\n",
    "log(f\"\\nüì¶ Final Pair Counts:\")\n",
    "log(f\"   Train:      {len(train_examples):,}\")\n",
    "log(f\"   Eval:       {len(eval_examples):,}\")\n",
    "log(f\"   Holdout:    {len(holdout_examples):,}\")\n",
    "log(f\"   Borderline: {len(borderline_examples):,} (V2 NEW - harder test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c4f9e",
   "metadata": {},
   "source": [
    "# 6. Model Training (V2: Curriculum Learning)\n",
    "\n",
    "Train `all-mpnet-base-v2` with MultipleNegativesRankingLoss.\n",
    "\n",
    "**V2 Curriculum Strategy:**\n",
    "- **Phase 1 (Epochs 1-2):** Easier pairs (25% hard negatives, threshold 0.15)\n",
    "- **Phase 2 (Epochs 3-4):** Harder pairs (55% hard negatives, threshold 0.10)\n",
    "\n",
    "This progressive difficulty helps the model learn basic patterns first, then refine on edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a5cd19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading model: nomic-ai/nomic-embed-text-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\donov\\anaconda3\\envs\\itsm\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\donov\\.cache\\huggingface\\hub\\models--nomic-ai--nomic-embed-text-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded on cuda, max_seq_length=512\n",
      "üìä Filtering to positive pairs for MultipleNegativesRankingLoss...\n",
      "   Train positives: 6,966 (was 33,216 total)\n",
      "   Eval positives:  2,230 (was 7,480 total)\n",
      "üîß Using MultipleNegativesRankingLoss (in-batch negatives)\n",
      "   Effective negatives per sample: 15\n",
      "\n",
      "üìä Training Setup:\n",
      "   Batches per epoch: 436\n",
      "   Total training steps: 1744\n",
      "   Warmup steps: 174\n",
      "   Curriculum learning: True\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers.evaluation import SentenceEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "# --- Custom Evaluator ---\n",
    "class ITSMEvaluator(SentenceEvaluator):\n",
    "    \"\"\"Evaluator for ITSM ticket similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, examples, batch_size=16, name=\"\"):\n",
    "        self.examples = examples\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "        \n",
    "        self.texts1 = [ex.texts[0] for ex in examples]\n",
    "        self.texts2 = [ex.texts[1] for ex in examples]\n",
    "        self.labels = np.array([ex.label for ex in examples])\n",
    "        \n",
    "        self.csv_file = f\"{name}_eval_results.csv\"\n",
    "        self.csv_headers = [\"epoch\", \"steps\", \"spearman\", \"pearson\", \"roc_auc\", \"pr_auc\"]\n",
    "    \n",
    "    def __call__(self, model, output_path=None, epoch=-1, steps=-1):\n",
    "        model.eval()\n",
    "        \n",
    "        # Encode pairs\n",
    "        emb1 = model.encode(self.texts1, batch_size=self.batch_size, \n",
    "                          show_progress_bar=False, convert_to_numpy=True)\n",
    "        emb2 = model.encode(self.texts2, batch_size=self.batch_size, \n",
    "                          show_progress_bar=False, convert_to_numpy=True)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        scores = np.sum(emb1 * emb2, axis=1) / (\n",
    "            np.linalg.norm(emb1, axis=1) * np.linalg.norm(emb2, axis=1) + 1e-8\n",
    "        )\n",
    "        \n",
    "        # Compute metrics\n",
    "        spearman, _ = spearmanr(self.labels, scores)\n",
    "        pearson, _ = pearsonr(self.labels, scores)\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(self.labels, scores)\n",
    "            pr_auc = average_precision_score(self.labels, scores)\n",
    "        except ValueError:\n",
    "            roc_auc, pr_auc = 0.0, 0.0\n",
    "        \n",
    "        log(f\"  [{self.name}] Epoch {epoch}: Spearman={spearman:.4f}, ROC-AUC={roc_auc:.4f}, PR-AUC={pr_auc:.4f}\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_path:\n",
    "            csv_path = Path(output_path) / self.csv_file\n",
    "            if not csv_path.exists():\n",
    "                with open(csv_path, 'w') as f:\n",
    "                    f.write(','.join(self.csv_headers) + '\\n')\n",
    "            with open(csv_path, 'a') as f:\n",
    "                f.write(f\"{epoch},{steps},{spearman},{pearson},{roc_auc},{pr_auc}\\n\")\n",
    "        \n",
    "        return spearman  # Primary metric\n",
    "\n",
    "\n",
    "# --- Model Initialization ---\n",
    "def init_model(config, device):\n",
    "    \"\"\"Initialize model with GPU memory management.\"\"\"\n",
    "    # Clear GPU memory\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    log(f\"üîß Loading model: {config['model_name']}\")\n",
    "    model = SentenceTransformer(config['model_name'], device=device,trust_remote_code=True)\n",
    "    model.max_seq_length = config['max_seq_length']\n",
    "    log(f\"‚úÖ Model loaded on {device}, max_seq_length={model.max_seq_length}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# --- V2: Cross-Validated Threshold Selection ---\n",
    "def get_cv_threshold(examples, model, n_folds=5):\n",
    "    \"\"\"\n",
    "    V2 NEW: Use k-fold cross-validation to find robust threshold.\n",
    "    \n",
    "    Returns the average best threshold across folds.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve, f1_score\n",
    "    \n",
    "    texts1 = [ex.texts[0] for ex in examples]\n",
    "    texts2 = [ex.texts[1] for ex in examples]\n",
    "    labels = np.array([ex.label for ex in examples])\n",
    "    \n",
    "    # Encode all pairs\n",
    "    emb1 = model.encode(texts1, batch_size=CONFIG['batch_size'], show_progress_bar=False)\n",
    "    emb2 = model.encode(texts2, batch_size=CONFIG['batch_size'], show_progress_bar=False)\n",
    "    scores = np.sum(emb1 * emb2, axis=1) / (np.linalg.norm(emb1, axis=1) * np.linalg.norm(emb2, axis=1) + 1e-8)\n",
    "    \n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=CONFIG['seed'])\n",
    "    best_thresholds = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(scores)):\n",
    "        val_scores = scores[val_idx]\n",
    "        val_labels = labels[val_idx]\n",
    "        \n",
    "        precision, recall, thresholds = precision_recall_curve(val_labels, val_scores)\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        best_threshold = thresholds[best_idx-1] if 0 < best_idx < len(thresholds)+1 else 0.5\n",
    "        best_thresholds.append(best_threshold)\n",
    "    \n",
    "    avg_threshold = np.mean(best_thresholds)\n",
    "    std_threshold = np.std(best_thresholds)\n",
    "    \n",
    "    log(f\"üìä CV Threshold ({n_folds}-fold): {avg_threshold:.4f} ¬± {std_threshold:.4f}\")\n",
    "    log(f\"   Per-fold thresholds: {[f'{t:.3f}' for t in best_thresholds]}\")\n",
    "    \n",
    "    return avg_threshold, std_threshold\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = init_model(CONFIG, DEVICE)\n",
    "\n",
    "# Filter to positive pairs only for MNRL\n",
    "log(f\"üìä Filtering to positive pairs for MultipleNegativesRankingLoss...\")\n",
    "train_positives = [ex for ex in train_examples if ex.label == 1.0]\n",
    "eval_positives = [ex for ex in eval_examples if ex.label == 1.0]\n",
    "\n",
    "log(f\"   Train positives: {len(train_positives):,} (was {len(train_examples):,} total)\")\n",
    "log(f\"   Eval positives:  {len(eval_positives):,} (was {len(eval_examples):,} total)\")\n",
    "\n",
    "# Create DataLoader with ONLY positives\n",
    "train_dataloader = DataLoader(\n",
    "    train_positives,\n",
    "    shuffle=True,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=0,\n",
    "    pin_memory=(DEVICE in ['cuda', 'mps'])\n",
    ")\n",
    "\n",
    "# Use MultipleNegativesRankingLoss\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "log(f\"üîß Using MultipleNegativesRankingLoss (in-batch negatives)\")\n",
    "log(f\"   Effective negatives per sample: {CONFIG['batch_size'] - 1}\")\n",
    "\n",
    "# Evaluator\n",
    "evaluator = ITSMEvaluator(eval_examples, batch_size=CONFIG['batch_size'], name=\"eval\")\n",
    "\n",
    "log(f\"\\nüìä Training Setup:\")\n",
    "log(f\"   Batches per epoch: {len(train_dataloader)}\")\n",
    "log(f\"   Total training steps: {len(train_dataloader) * CONFIG['epochs']}\")\n",
    "log(f\"   Warmup steps: {int(len(train_dataloader) * CONFIG['epochs'] * CONFIG['warmup_ratio'])}\")\n",
    "log(f\"   Curriculum learning: {CONFIG['use_curriculum']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e2fb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting Training (V2)...\n",
      "   Output: models\\real_servicenow_finetuned_nomic\\real_servicenow_v2_20251211_2201\n",
      "   Epochs: 4\n",
      "   Device: cuda\n",
      "\n",
      "üìö CURRICULUM LEARNING ENABLED\n",
      "\n",
      "============================================================\n",
      "üìö Phase 1: 2 epochs\n",
      "   Hard neg ratio: 25%\n",
      "   Neg threshold: 0.15\n",
      "============================================================\n",
      "‚è≥ Building TF-IDF matrix...\n",
      "‚úÖ TF-IDF matrix: (8021, 15000) (vocab size: 15000)\n",
      "\n",
      "üéØ Generating 6,966 pairs for Phase 1:\n",
      "   Target: 2,089 positives (30%), 1,741 hard neg (25%), 3,136 easy neg\n",
      "   Neg TF-IDF threshold: 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positives (high TF-IDF):  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1278/2089 [00:23<00:14, 54.89it/s]\n",
      "Hard Negatives (same cat, TF-IDF<0.15): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1741/1741 [00:00<00:00, 2716.05it/s]\n",
      "Easy Negatives (cross-cat): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3136/3136 [00:00<00:00, 3590.90it/s]\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 6,155 pairs:\n",
      "   Positives: 1,278 (20.8%)\n",
      "   Hard Neg:  1,741 (28.3%)\n",
      "   Easy Neg:  3,136 (51.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 3:31:45, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Evaluator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.789111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.790025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [eval] Epoch 1.0: Spearman=0.7891, ROC-AUC=0.9980, PR-AUC=0.9965\n",
      "  [eval] Epoch 2.0: Spearman=0.7900, ROC-AUC=0.9985, PR-AUC=0.9977\n",
      "‚úÖ Phase 1 complete!\n",
      "\n",
      "============================================================\n",
      "üìö Phase 2: 2 epochs\n",
      "   Hard neg ratio: 55%\n",
      "   Neg threshold: 0.1\n",
      "============================================================\n",
      "‚è≥ Building TF-IDF matrix...\n",
      "‚úÖ TF-IDF matrix: (8021, 15000) (vocab size: 15000)\n",
      "\n",
      "üéØ Generating 6,966 pairs for Phase 2:\n",
      "   Target: 2,089 positives (30%), 3,831 hard neg (55%), 1,046 easy neg\n",
      "   Neg TF-IDF threshold: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positives (high TF-IDF):  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1269/2089 [00:23<00:15, 53.46it/s]\n",
      "Hard Negatives (same cat, TF-IDF<0.1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3831/3831 [00:01<00:00, 2037.43it/s]\n",
      "Easy Negatives (cross-cat): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1046/1046 [00:00<00:00, 3401.62it/s]\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 6,146 pairs:\n",
      "   Positives: 1,269 (20.6%)\n",
      "   Hard Neg:  3,831 (62.3%)\n",
      "   Easy Neg:  1,046 (17.0%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='161' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 6:58:23, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Evaluator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.790490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [eval] Epoch 1.0: Spearman=0.7905, ROC-AUC=0.9988, PR-AUC=0.9983\n"
     ]
    }
   ],
   "source": [
    "# --- Training Execution (V2: with Curriculum Learning) ---\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "save_path = Path(CONFIG['output_dir']) / f\"real_servicenow_v2_{timestamp}\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log(f\"\\nüöÄ Starting Training (V2)...\")\n",
    "log(f\"   Output: {save_path}\")\n",
    "log(f\"   Epochs: {CONFIG['epochs']}\")\n",
    "log(f\"   Device: {DEVICE}\")\n",
    "\n",
    "# Calculate warmup steps\n",
    "total_steps = len(train_dataloader) * CONFIG['epochs']\n",
    "warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "eval_steps = max(100, len(train_dataloader) // 2)  # Evaluate twice per epoch\n",
    "\n",
    "try:\n",
    "    if CONFIG['use_curriculum']:\n",
    "        # V2: Curriculum Learning - train in phases\n",
    "        log(\"\\nüìö CURRICULUM LEARNING ENABLED\")\n",
    "        \n",
    "        for phase_idx, phase in enumerate(CONFIG['curriculum_phases']):\n",
    "            log(f\"\\n{'='*60}\")\n",
    "            log(f\"üìö Phase {phase_idx + 1}: {phase['epochs']} epochs\")\n",
    "            log(f\"   Hard neg ratio: {phase['hard_neg_ratio']*100:.0f}%\")\n",
    "            log(f\"   Neg threshold: {phase['neg_threshold']}\")\n",
    "            log(f\"{'='*60}\")\n",
    "            \n",
    "            # Regenerate training pairs for this phase\n",
    "            phase_train_examples = generate_training_pairs(\n",
    "                train_df, \n",
    "                len(train_positives),  # Same count as original positives\n",
    "                CONFIG, \n",
    "                desc=f\"Phase {phase_idx+1}\",\n",
    "                phase_config=phase\n",
    "            )\n",
    "            phase_positives = [ex for ex in phase_train_examples if ex.label == 1.0]\n",
    "            \n",
    "            phase_dataloader = DataLoader(\n",
    "                phase_positives,\n",
    "                shuffle=True,\n",
    "                batch_size=CONFIG['batch_size'],\n",
    "                num_workers=0,\n",
    "                pin_memory=(DEVICE in ['cuda', 'mps'])\n",
    "            )\n",
    "            \n",
    "            phase_warmup = int(len(phase_dataloader) * phase['epochs'] * CONFIG['warmup_ratio'])\n",
    "            \n",
    "            # Create phase-specific checkpoint directory\n",
    "            phase_checkpoint_dir = Path(save_path) / f\"phase_{phase_idx + 1}_checkpoint\"\n",
    "            phase_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            model.fit(\n",
    "                train_objectives=[(phase_dataloader, train_loss)],\n",
    "                evaluator=evaluator,\n",
    "                epochs=phase['epochs'],\n",
    "                warmup_steps=phase_warmup,\n",
    "                optimizer_params={'lr': CONFIG['lr']},\n",
    "                output_path=str(save_path),\n",
    "                evaluation_steps=eval_steps,\n",
    "                save_best_model=True,\n",
    "                show_progress_bar=True\n",
    "            )\n",
    "            \n",
    "            # Save phase checkpoint\n",
    "            log(f\"üíæ Saving Phase {phase_idx + 1} checkpoint...\")\n",
    "            model.save(str(phase_checkpoint_dir))\n",
    "            \n",
    "            # Also save phase metadata\n",
    "            phase_metadata = {\n",
    "                \"phase\": phase_idx + 1,\n",
    "                \"total_phases\": len(CONFIG['curriculum_phases']),\n",
    "                \"phase_config\": phase,\n",
    "                \"checkpoint_timestamp\": datetime.now().isoformat(),\n",
    "                \"model_path\": str(phase_checkpoint_dir)\n",
    "            }\n",
    "            with open(phase_checkpoint_dir / \"phase_metadata.json\", 'w') as f:\n",
    "                json.dump(phase_metadata, f, indent=2)\n",
    "            \n",
    "            log(f\"‚úÖ Phase {phase_idx + 1} complete! Checkpoint saved to: {phase_checkpoint_dir}\")\n",
    "            \n",
    "            # Clear GPU memory between phases\n",
    "            if DEVICE == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            elif DEVICE == 'mps':\n",
    "                torch.mps.empty_cache()\n",
    "            gc.collect()\n",
    "            log(f\"üßπ GPU memory cleared between phases\")\n",
    "    else:\n",
    "        # Standard training (no curriculum)\n",
    "        model.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            evaluator=evaluator,\n",
    "            epochs=CONFIG['epochs'],\n",
    "            warmup_steps=warmup_steps,\n",
    "            optimizer_params={'lr': CONFIG['lr']},\n",
    "            output_path=str(save_path),\n",
    "            evaluation_steps=eval_steps,\n",
    "            save_best_model=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "    \n",
    "    log(\"\\n‚úÖ Training complete!\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        log(f\"‚ùå OOM Error: {e}\")\n",
    "        log(\"üí° Try reducing batch_size or num_pairs\")\n",
    "        raise\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Reload best model\n",
    "log(\"\\nüìä Loading best model for final evaluation...\")\n",
    "best_model = SentenceTransformer(str(save_path), device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85294299",
   "metadata": {},
   "source": [
    "# 7. Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d094e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, precision_recall_curve, confusion_matrix,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_score, recall_score, f1_score, accuracy_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def comprehensive_eval(examples, model, name=\"\", use_cv_threshold=False):\n",
    "    \"\"\"Run comprehensive evaluation on a set of pairs (V2: with CV threshold option).\"\"\"\n",
    "    texts1 = [ex.texts[0] for ex in examples]\n",
    "    texts2 = [ex.texts[1] for ex in examples]\n",
    "    labels = np.array([ex.label for ex in examples])\n",
    "    \n",
    "    # Encode\n",
    "    log(f\"‚è≥ Encoding {len(examples)} pairs for {name}...\")\n",
    "    emb1 = model.encode(texts1, batch_size=CONFIG['batch_size'], \n",
    "                       show_progress_bar=True, convert_to_numpy=True)\n",
    "    emb2 = model.encode(texts2, batch_size=CONFIG['batch_size'], \n",
    "                       show_progress_bar=True, convert_to_numpy=True)\n",
    "    \n",
    "    # Cosine similarity\n",
    "    scores = np.sum(emb1 * emb2, axis=1) / (\n",
    "        np.linalg.norm(emb1, axis=1) * np.linalg.norm(emb2, axis=1) + 1e-8\n",
    "    )\n",
    "    \n",
    "    # Metrics\n",
    "    spearman, _ = spearmanr(labels, scores)\n",
    "    pearson, _ = pearsonr(labels, scores)\n",
    "    roc_auc = roc_auc_score(labels, scores)\n",
    "    pr_auc = average_precision_score(labels, scores)\n",
    "    \n",
    "    # Find best threshold\n",
    "    fpr, tpr, roc_thresholds = roc_curve(labels, scores)\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(labels, scores)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = pr_thresholds[best_idx-1] if 0 < best_idx < len(pr_thresholds)+1 else 0.5\n",
    "    \n",
    "    # V2: Use CV threshold if requested\n",
    "    if use_cv_threshold and 'cv_threshold' in globals():\n",
    "        best_threshold = cv_threshold\n",
    "        log(f\"   Using CV threshold: {best_threshold:.4f}\")\n",
    "    \n",
    "    # Metrics at best threshold\n",
    "    preds = (scores >= best_threshold).astype(int)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds)\n",
    "    rec = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "    log(f\"\\nüìä {name} Results:\")\n",
    "    log(f\"   Spearman:  {spearman:.4f}\")\n",
    "    log(f\"   Pearson:   {pearson:.4f}\")\n",
    "    log(f\"   ROC-AUC:   {roc_auc:.4f}\")\n",
    "    log(f\"   PR-AUC:    {pr_auc:.4f}\")\n",
    "    log(f\"   Best Threshold: {best_threshold:.3f}\")\n",
    "    log(f\"   F1 @ best: {f1:.4f}\")\n",
    "    log(f\"   Precision: {prec:.4f}\")\n",
    "    log(f\"   Recall:    {rec:.4f}\")\n",
    "    log(f\"   Accuracy:  {acc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'labels': labels, 'scores': scores,\n",
    "        'spearman': spearman, 'pearson': pearson,\n",
    "        'roc_auc': roc_auc, 'pr_auc': pr_auc,\n",
    "        'fpr': fpr, 'tpr': tpr,\n",
    "        'precision': precision, 'recall': recall,\n",
    "        'best_threshold': best_threshold,\n",
    "        'f1': f1, 'prec': prec, 'rec': rec, 'acc': acc,\n",
    "        'cm': cm, 'texts1': texts1, 'texts2': texts2  # V2: Keep texts for error analysis\n",
    "    }\n",
    "\n",
    "# V2: Get cross-validated threshold first\n",
    "log(\"=\"*60)\n",
    "log(\"üìà CROSS-VALIDATED THRESHOLD SELECTION (V2)\")\n",
    "log(\"=\"*60)\n",
    "cv_threshold, cv_std = get_cv_threshold(eval_examples, best_model, n_folds=CONFIG['threshold_cv_folds'])\n",
    "\n",
    "# Evaluate on all sets\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"üìà FINAL EVALUATION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "eval_results = comprehensive_eval(eval_examples, best_model, \"Eval Set\")\n",
    "holdout_results = comprehensive_eval(holdout_examples, best_model, \"Holdout Set\")\n",
    "\n",
    "# V2 NEW: Evaluate on borderline (harder) test set\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"üìà BORDERLINE TEST (V2 - Harder Evaluation)\")\n",
    "log(\"=\"*60)\n",
    "borderline_results = comprehensive_eval(borderline_examples, best_model, \"Borderline Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48806de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization (V2: Added borderline results)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 14))\n",
    "\n",
    "# ROC Curves\n",
    "axes[0,0].plot(eval_results['fpr'], eval_results['tpr'], \n",
    "               label=f\"Eval ROC-AUC = {eval_results['roc_auc']:.3f}\")\n",
    "axes[0,0].plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "axes[0,0].set_title('ROC Curve (Eval)')\n",
    "axes[0,0].set_xlabel('False Positive Rate')\n",
    "axes[0,0].set_ylabel('True Positive Rate')\n",
    "axes[0,0].legend()\n",
    "\n",
    "axes[0,1].plot(holdout_results['fpr'], holdout_results['tpr'], \n",
    "               label=f\"Holdout ROC-AUC = {holdout_results['roc_auc']:.3f}\", color='orange')\n",
    "axes[0,1].plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "axes[0,1].set_title('ROC Curve (Holdout)')\n",
    "axes[0,1].set_xlabel('False Positive Rate')\n",
    "axes[0,1].set_ylabel('True Positive Rate')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# V2 NEW: Borderline ROC\n",
    "axes[0,2].plot(borderline_results['fpr'], borderline_results['tpr'], \n",
    "               label=f\"Borderline ROC-AUC = {borderline_results['roc_auc']:.3f}\", color='red')\n",
    "axes[0,2].plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "axes[0,2].set_title('ROC Curve (Borderline - V2)')\n",
    "axes[0,2].set_xlabel('False Positive Rate')\n",
    "axes[0,2].set_ylabel('True Positive Rate')\n",
    "axes[0,2].legend()\n",
    "\n",
    "# PR Curves\n",
    "axes[1,0].plot(eval_results['recall'], eval_results['precision'], \n",
    "               label=f\"Eval PR-AUC = {eval_results['pr_auc']:.3f}\")\n",
    "axes[1,0].scatter([eval_results['rec']], [eval_results['prec']], \n",
    "                  color='red', s=100, zorder=5,\n",
    "                  label=f\"Best F1={eval_results['f1']:.3f}\")\n",
    "axes[1,0].set_title('Precision-Recall (Eval)')\n",
    "axes[1,0].set_xlabel('Recall')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].legend()\n",
    "\n",
    "axes[1,1].plot(holdout_results['recall'], holdout_results['precision'], \n",
    "               label=f\"Holdout PR-AUC = {holdout_results['pr_auc']:.3f}\", color='orange')\n",
    "axes[1,1].scatter([holdout_results['rec']], [holdout_results['prec']], \n",
    "                  color='red', s=100, zorder=5,\n",
    "                  label=f\"Best F1={holdout_results['f1']:.3f}\")\n",
    "axes[1,1].set_title('Precision-Recall (Holdout)')\n",
    "axes[1,1].set_xlabel('Recall')\n",
    "axes[1,1].set_ylabel('Precision')\n",
    "axes[1,1].legend()\n",
    "\n",
    "# V2 NEW: Borderline PR\n",
    "axes[1,2].plot(borderline_results['recall'], borderline_results['precision'], \n",
    "               label=f\"Borderline PR-AUC = {borderline_results['pr_auc']:.3f}\", color='red')\n",
    "axes[1,2].scatter([borderline_results['rec']], [borderline_results['prec']], \n",
    "                  color='green', s=100, zorder=5,\n",
    "                  label=f\"Best F1={borderline_results['f1']:.3f}\")\n",
    "axes[1,2].set_title('Precision-Recall (Borderline - V2)')\n",
    "axes[1,2].set_xlabel('Recall')\n",
    "axes[1,2].set_ylabel('Precision')\n",
    "axes[1,2].legend()\n",
    "\n",
    "# Score Distributions\n",
    "for idx, (results, name, color) in enumerate([\n",
    "    (eval_results, 'Eval', 'blue'), \n",
    "    (holdout_results, 'Holdout', 'orange'),\n",
    "    (borderline_results, 'Borderline', 'red')\n",
    "]):\n",
    "    ax = axes[2, idx]\n",
    "    neg_scores = results['scores'][results['labels'] == 0]\n",
    "    pos_scores = results['scores'][results['labels'] == 1]\n",
    "    \n",
    "    ax.hist(neg_scores, bins=30, alpha=0.6, label='Negative (0)', color='blue')\n",
    "    ax.hist(pos_scores, bins=30, alpha=0.6, label='Positive (1)', color='orange')\n",
    "    ax.axvline(results['best_threshold'], color='red', linestyle='--', \n",
    "               label=f\"Threshold={results['best_threshold']:.3f}\")\n",
    "    ax.axvline(cv_threshold, color='green', linestyle=':', \n",
    "               label=f\"CV Threshold={cv_threshold:.3f}\")\n",
    "    ax.set_title(f'Score Distribution ({name})')\n",
    "    ax.set_xlabel('Cosine Similarity')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path / 'evaluation_plots_v2.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "log(f\"\\nüìä Plots saved to {save_path / 'evaluation_plots_v2.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa08e9",
   "metadata": {},
   "source": [
    "# 7.1 Error Analysis (V2 NEW)\n",
    "\n",
    "Systematically analyze failure patterns to identify:\n",
    "1. **Worst False Positives**: High-scoring pairs that should be dissimilar\n",
    "2. **Worst False Negatives**: Low-scoring pairs that should be similar\n",
    "3. **Category/text length patterns** in errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda41096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# V2 NEW: ERROR ANALYSIS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def analyze_errors(results, name=\"\", top_k=10):\n",
    "    \"\"\"\n",
    "    Analyze systematic failure patterns in model predictions.\n",
    "    \n",
    "    Returns insights about:\n",
    "    - Worst false positives (high score, label=0)\n",
    "    - Worst false negatives (low score, label=1)\n",
    "    - Text length patterns\n",
    "    \"\"\"\n",
    "    labels = results['labels']\n",
    "    scores = results['scores']\n",
    "    texts1 = results['texts1']\n",
    "    texts2 = results['texts2']\n",
    "    threshold = results['best_threshold']\n",
    "    \n",
    "    # Identify errors\n",
    "    preds = (scores >= threshold).astype(int)\n",
    "    \n",
    "    # False Positives: predicted 1, actual 0\n",
    "    fp_mask = (preds == 1) & (labels == 0)\n",
    "    fp_indices = np.where(fp_mask)[0]\n",
    "    fp_scores = scores[fp_mask]\n",
    "    \n",
    "    # False Negatives: predicted 0, actual 1\n",
    "    fn_mask = (preds == 0) & (labels == 1)\n",
    "    fn_indices = np.where(fn_mask)[0]\n",
    "    fn_scores = scores[fn_mask]\n",
    "    \n",
    "    log(f\"\\n{'='*60}\")\n",
    "    log(f\"üîç ERROR ANALYSIS: {name}\")\n",
    "    log(f\"{'='*60}\")\n",
    "    log(f\"Total pairs: {len(labels):,}\")\n",
    "    log(f\"False Positives: {len(fp_indices):,} ({len(fp_indices)/len(labels)*100:.2f}%)\")\n",
    "    log(f\"False Negatives: {len(fn_indices):,} ({len(fn_indices)/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    # Worst False Positives (highest scoring negatives)\n",
    "    if len(fp_indices) > 0:\n",
    "        log(f\"\\nüìõ WORST FALSE POSITIVES (Top {min(top_k, len(fp_indices))}):\")\n",
    "        log(f\"   These pairs scored HIGH but should be DISSIMILAR\")\n",
    "        worst_fp_order = np.argsort(fp_scores)[::-1][:top_k]\n",
    "        \n",
    "        for rank, idx in enumerate(worst_fp_order):\n",
    "            orig_idx = fp_indices[idx]\n",
    "            score = scores[orig_idx]\n",
    "            t1, t2 = texts1[orig_idx], texts2[orig_idx]\n",
    "            log(f\"\\n   [{rank+1}] Score: {score:.4f}\")\n",
    "            log(f\"       Text 1: {t1[:100]}...\")\n",
    "            log(f\"       Text 2: {t2[:100]}...\")\n",
    "    \n",
    "    # Worst False Negatives (lowest scoring positives)\n",
    "    if len(fn_indices) > 0:\n",
    "        log(f\"\\nüìõ WORST FALSE NEGATIVES (Top {min(top_k, len(fn_indices))}):\")\n",
    "        log(f\"   These pairs scored LOW but should be SIMILAR\")\n",
    "        worst_fn_order = np.argsort(fn_scores)[:top_k]\n",
    "        \n",
    "        for rank, idx in enumerate(worst_fn_order):\n",
    "            orig_idx = fn_indices[idx]\n",
    "            score = scores[orig_idx]\n",
    "            t1, t2 = texts1[orig_idx], texts2[orig_idx]\n",
    "            log(f\"\\n   [{rank+1}] Score: {score:.4f}\")\n",
    "            log(f\"       Text 1: {t1[:100]}...\")\n",
    "            log(f\"       Text 2: {t2[:100]}...\")\n",
    "    \n",
    "    # Text length analysis\n",
    "    log(f\"\\nüìè TEXT LENGTH ANALYSIS:\")\n",
    "    all_lengths = [len(t) for t in texts1 + texts2]\n",
    "    fp_lengths = [len(texts1[i]) + len(texts2[i]) for i in fp_indices] if len(fp_indices) > 0 else [0]\n",
    "    fn_lengths = [len(texts1[i]) + len(texts2[i]) for i in fn_indices] if len(fn_indices) > 0 else [0]\n",
    "    \n",
    "    log(f\"   Overall avg length: {np.mean(all_lengths):.0f} chars\")\n",
    "    log(f\"   FP pairs avg length: {np.mean(fp_lengths):.0f} chars\")\n",
    "    log(f\"   FN pairs avg length: {np.mean(fn_lengths):.0f} chars\")\n",
    "    \n",
    "    # Score distribution in errors\n",
    "    log(f\"\\nüìä SCORE DISTRIBUTION IN ERRORS:\")\n",
    "    if len(fp_scores) > 0:\n",
    "        log(f\"   FP scores: min={fp_scores.min():.4f}, max={fp_scores.max():.4f}, mean={fp_scores.mean():.4f}\")\n",
    "    if len(fn_scores) > 0:\n",
    "        log(f\"   FN scores: min={fn_scores.min():.4f}, max={fn_scores.max():.4f}, mean={fn_scores.mean():.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'fp_count': len(fp_indices),\n",
    "        'fn_count': len(fn_indices),\n",
    "        'fp_scores': fp_scores,\n",
    "        'fn_scores': fn_scores,\n",
    "        'fp_avg_length': np.mean(fp_lengths),\n",
    "        'fn_avg_length': np.mean(fn_lengths)\n",
    "    }\n",
    "\n",
    "# Run error analysis on all sets\n",
    "eval_errors = analyze_errors(eval_results, \"Eval Set\", top_k=5)\n",
    "holdout_errors = analyze_errors(holdout_results, \"Holdout Set\", top_k=5)\n",
    "borderline_errors = analyze_errors(borderline_results, \"Borderline Set\", top_k=5)\n",
    "\n",
    "# Summary comparison\n",
    "log(f\"\\n{'='*60}\")\n",
    "log(f\"üìä ERROR SUMMARY COMPARISON\")\n",
    "log(f\"{'='*60}\")\n",
    "log(f\"{'Set':<15} {'FP Count':<12} {'FN Count':<12} {'FP Rate':<12} {'FN Rate':<12}\")\n",
    "log(f\"{'-'*60}\")\n",
    "for name, errors, results in [\n",
    "    (\"Eval\", eval_errors, eval_results),\n",
    "    (\"Holdout\", holdout_errors, holdout_results),\n",
    "    (\"Borderline\", borderline_errors, borderline_results)\n",
    "]:\n",
    "    total = len(results['labels'])\n",
    "    fp_rate = errors['fp_count'] / total * 100\n",
    "    fn_rate = errors['fn_count'] / total * 100\n",
    "    log(f\"{name:<15} {errors['fp_count']:<12} {errors['fn_count']:<12} {fp_rate:<12.2f}% {fn_rate:<12.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f0d5d",
   "metadata": {},
   "source": [
    "# 8. Adversarial Diagnostic (V2: Enhanced)\n",
    "\n",
    "**Critical validation**: Test if the model learned semantic content or is exploiting category shortcuts.\n",
    "\n",
    "- **Hard Positives**: Cross-category pairs with HIGH content similarity\n",
    "- **Hard Negatives**: Same-category pairs with LOW content similarity\n",
    "\n",
    "**V2 Enhancements:**\n",
    "- Stricter TF-IDF thresholds for adversarial pairs\n",
    "- Compare with borderline test performance\n",
    "\n",
    "**Pass Criteria**: ROC-AUC ‚â• 0.70 AND F1 ‚â• 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94893294",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"=\"*60)\n",
    "log(\"üî¨ ADVERSARIAL DIAGNOSTIC: Testing Category Leakage\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Use holdout data for adversarial test (completely unseen)\n",
    "diag_df = holdout_df.reset_index(drop=True)\n",
    "\n",
    "# Build content-only text (remove category context to test pure semantic understanding)\n",
    "diag_df['content_only'] = diag_df['Description'].str.strip()\n",
    "\n",
    "# Build TF-IDF on content-only text\n",
    "log(\"‚è≥ Building TF-IDF for adversarial pair mining...\")\n",
    "diag_tfidf = TFIDFSimilarityCalculator(diag_df['content_only'].tolist(), max_features=10000)\n",
    "\n",
    "# Generate adversarial pairs\n",
    "hard_positives = []  # Cross-category, high TF-IDF\n",
    "hard_negatives = []  # Same-category, low TF-IDF\n",
    "\n",
    "target_each = 300\n",
    "attempts, max_attempts = 0, 100000\n",
    "\n",
    "log(\"‚è≥ Mining adversarial pairs...\")\n",
    "pbar = tqdm(total=target_each * 2, desc=\"Adversarial pairs\")\n",
    "\n",
    "while (len(hard_positives) < target_each or len(hard_negatives) < target_each) and attempts < max_attempts:\n",
    "    attempts += 1\n",
    "    i1, i2 = random.sample(range(len(diag_df)), 2)\n",
    "    \n",
    "    cat1 = diag_df.at[i1, 'category_id']\n",
    "    cat2 = diag_df.at[i2, 'category_id']\n",
    "    tfidf_sim = diag_tfidf.similarity(i1, i2)\n",
    "    \n",
    "    # Hard Positive: DIFFERENT category but HIGH content similarity\n",
    "    if len(hard_positives) < target_each and cat1 != cat2 and tfidf_sim > 0.4:\n",
    "        hard_positives.append(InputExample(\n",
    "            texts=[diag_df.at[i1, 'content_only'], diag_df.at[i2, 'content_only']],\n",
    "            label=1.0\n",
    "        ))\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Hard Negative: SAME category but LOW content similarity\n",
    "    if len(hard_negatives) < target_each and cat1 == cat2 and tfidf_sim < 0.15:\n",
    "        hard_negatives.append(InputExample(\n",
    "            texts=[diag_df.at[i1, 'content_only'], diag_df.at[i2, 'content_only']],\n",
    "            label=0.0\n",
    "        ))\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "log(f\"‚úÖ Generated {len(hard_positives)} hard positives, {len(hard_negatives)} hard negatives\")\n",
    "\n",
    "# Evaluate on adversarial pairs\n",
    "adversarial_examples = hard_positives + hard_negatives\n",
    "if len(adversarial_examples) >= 100:\n",
    "    adv_results = comprehensive_eval(adversarial_examples, best_model, \"Adversarial\")\n",
    "    \n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"üéØ ADVERSARIAL DIAGNOSTIC RESULTS\")\n",
    "    log(\"=\"*60)\n",
    "    log(f\"Standard Eval ROC-AUC:     {eval_results['roc_auc']:.4f}\")\n",
    "    log(f\"Adversarial ROC-AUC:       {adv_results['roc_auc']:.4f}\")\n",
    "    log(f\"Adversarial F1 @ best:     {adv_results['f1']:.4f}\")\n",
    "    \n",
    "    # Verdict\n",
    "    if adv_results['roc_auc'] >= 0.70 and adv_results['f1'] >= 0.65:\n",
    "        log(\"\\n‚úÖ VERDICT: Model is ROBUST to category shortcuts!\")\n",
    "        log(\"   ‚Üí Performance holds when categories don't predict similarity\")\n",
    "        log(\"   ‚Üí Model learned semantic content understanding\")\n",
    "        DIAGNOSTIC_PASSED = True\n",
    "    else:\n",
    "        log(\"\\n‚ö†Ô∏è VERDICT: Model may be exploiting category shortcuts\")\n",
    "        log(\"   ‚Üí Consider increasing hard negatives ratio\")\n",
    "        log(\"   ‚Üí Or remove category context from training text\")\n",
    "        DIAGNOSTIC_PASSED = False\n",
    "else:\n",
    "    log(\"‚ö†Ô∏è Could not generate enough adversarial pairs\")\n",
    "    DIAGNOSTIC_PASSED = None\n",
    "\n",
    "# Cleanup\n",
    "del diag_tfidf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd02e18",
   "metadata": {},
   "source": [
    "# 9. Save Training Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4145588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TRAINING METADATA EXPORT (V2)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def save_training_metadata(output_dir: str, config: dict, metrics: dict, \n",
    "                          data_stats: dict, adversarial_results: dict = None,\n",
    "                          error_analysis: dict = None):\n",
    "    \"\"\"\n",
    "    Save comprehensive training metadata for reproducibility (V2: includes error analysis).\n",
    "    \n",
    "    Following project convention: all model outputs include training_metadata.json\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"training_timestamp\": datetime.now().isoformat(),\n",
    "        \"model_version\": \"V2\",\n",
    "        \"model_name\": config.get('model_name', 'all-mpnet-finetuned'),\n",
    "        \"base_model\": config.get('model_name', 'sentence-transformers/all-mpnet-base-v2'),\n",
    "        \n",
    "        # Hyperparameters\n",
    "        \"hyperparameters\": {\n",
    "            \"epochs\": config.get('epochs'),\n",
    "            \"batch_size\": config.get('batch_size'),\n",
    "            \"learning_rate\": config.get('lr'),\n",
    "            \"max_seq_length\": config.get('max_seq_length'),\n",
    "            \"warmup_ratio\": config.get('warmup_ratio'),\n",
    "            \"loss_function\": \"MultipleNegativesRankingLoss\"\n",
    "        },\n",
    "        \n",
    "        # V2: Curriculum learning config\n",
    "        \"curriculum_learning\": {\n",
    "            \"enabled\": config.get('use_curriculum', False),\n",
    "            \"phases\": config.get('curriculum_phases', [])\n",
    "        },\n",
    "        \n",
    "        # Data configuration (V2: stricter thresholds)\n",
    "        \"data_config\": {\n",
    "            \"source_data\": config.get('source_data'),\n",
    "            \"num_pairs\": config.get('num_pairs'),\n",
    "            \"min_text_length\": config.get('min_text_length'),\n",
    "            \"eval_split\": config.get('eval_split'),\n",
    "            \"holdout_split\": config.get('holdout_split'),\n",
    "            \"pos_tfidf_threshold\": config.get('pos_tfidf_threshold'),\n",
    "            \"neg_tfidf_threshold\": config.get('neg_tfidf_threshold'),\n",
    "            \"pair_ratios\": {\n",
    "                \"positives\": config.get('pos_ratio', 0.30),\n",
    "                \"hard_negatives\": config.get('hard_neg_ratio', 0.45),\n",
    "                \"easy_negatives\": config.get('easy_neg_ratio', 0.25)\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Data statistics\n",
    "        \"data_statistics\": data_stats,\n",
    "        \n",
    "        # TF-IDF configuration\n",
    "        \"tfidf_config\": {\n",
    "            \"max_features\": 15000,\n",
    "            \"ngram_range\": [1, 2],\n",
    "            \"min_df\": 2,\n",
    "            \"max_df\": 0.95\n",
    "        },\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        \"evaluation_metrics\": metrics,\n",
    "        \n",
    "        # V2: Cross-validated threshold\n",
    "        \"cv_threshold\": {\n",
    "            \"value\": float(cv_threshold) if 'cv_threshold' in dir() else None,\n",
    "            \"std\": float(cv_std) if 'cv_std' in dir() else None,\n",
    "            \"n_folds\": config.get('threshold_cv_folds', 5)\n",
    "        },\n",
    "        \n",
    "        # Adversarial diagnostic results\n",
    "        \"adversarial_diagnostic\": adversarial_results,\n",
    "        \n",
    "        # V2: Error analysis summary\n",
    "        \"error_analysis\": error_analysis,\n",
    "        \n",
    "        # Environment info\n",
    "        \"environment\": {\n",
    "            \"device\": DEVICE,\n",
    "            \"random_seed\": config.get('seed'),\n",
    "            \"python_version\": __import__('sys').version,\n",
    "            \"torch_version\": torch.__version__,\n",
    "            \"transformers_version\": __import__('transformers').__version__,\n",
    "            \"sentence_transformers_version\": __import__('sentence_transformers').__version__\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(output_dir, \"training_metadata.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    log(f\"üìù Training metadata saved to: {metadata_path}\")\n",
    "    return metadata_path\n",
    "\n",
    "# Collect data statistics\n",
    "data_stats = {\n",
    "    \"total_records\": len(df_incidents),\n",
    "    \"train_size\": len(train_df),\n",
    "    \"eval_size\": len(eval_df),\n",
    "    \"holdout_size\": len(holdout_df),\n",
    "    \"unique_categories\": df_incidents['Category'].nunique() if 'Category' in df_incidents.columns else None,\n",
    "    \"unique_subcategories\": df_incidents['Subcategory'].nunique() if 'Subcategory' in df_incidents.columns else None,\n",
    "    \"unique_assignment_groups\": df_incidents['Assignment group'].nunique() if 'Assignment group' in df_incidents.columns else None,\n",
    "    \"avg_text_length\": df_incidents['text'].str.len().mean(),\n",
    "    \"tfidf_vocabulary_size\": None\n",
    "}\n",
    "\n",
    "# Collect metrics\n",
    "try:\n",
    "    eval_metrics = {\n",
    "        \"eval_roc_auc\": float(eval_results['roc_auc']),\n",
    "        \"eval_pr_auc\": float(eval_results['pr_auc']),\n",
    "        \"eval_spearman\": float(eval_results['spearman']),\n",
    "        \"eval_pearson\": float(eval_results['pearson']),\n",
    "        \"eval_f1\": float(eval_results['f1']),\n",
    "        \"holdout_roc_auc\": float(holdout_results['roc_auc']),\n",
    "        \"holdout_pr_auc\": float(holdout_results['pr_auc']),\n",
    "        \"holdout_spearman\": float(holdout_results['spearman']),\n",
    "        \"holdout_pearson\": float(holdout_results['pearson']),\n",
    "        \"holdout_f1\": float(holdout_results['f1']),\n",
    "        # V2: Borderline metrics\n",
    "        \"borderline_roc_auc\": float(borderline_results['roc_auc']),\n",
    "        \"borderline_pr_auc\": float(borderline_results['pr_auc']),\n",
    "        \"borderline_f1\": float(borderline_results['f1']),\n",
    "    }\n",
    "except:\n",
    "    eval_metrics = {\"note\": \"Run evaluation cells first\"}\n",
    "\n",
    "# Collect adversarial results\n",
    "try:\n",
    "    adversarial_results_dict = {\n",
    "        \"roc_auc\": float(adv_results['roc_auc']) if 'adv_results' in dir() else None,\n",
    "        \"f1_score\": float(adv_results['f1']) if 'adv_results' in dir() else None,\n",
    "        \"pass_status\": DIAGNOSTIC_PASSED if 'DIAGNOSTIC_PASSED' in dir() else None\n",
    "    }\n",
    "except:\n",
    "    adversarial_results_dict = {\"note\": \"Run adversarial diagnostic first\"}\n",
    "\n",
    "# V2: Collect error analysis summary\n",
    "try:\n",
    "    error_analysis_dict = {\n",
    "        \"eval_fp_count\": eval_errors['fp_count'],\n",
    "        \"eval_fn_count\": eval_errors['fn_count'],\n",
    "        \"holdout_fp_count\": holdout_errors['fp_count'],\n",
    "        \"holdout_fn_count\": holdout_errors['fn_count'],\n",
    "        \"borderline_fp_count\": borderline_errors['fp_count'],\n",
    "        \"borderline_fn_count\": borderline_errors['fn_count'],\n",
    "    }\n",
    "except:\n",
    "    error_analysis_dict = {\"note\": \"Run error analysis first\"}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = save_training_metadata(\n",
    "    output_dir=str(save_path),\n",
    "    config=CONFIG,\n",
    "    metrics=eval_metrics,\n",
    "    data_stats=data_stats,\n",
    "    adversarial_results=adversarial_results_dict,\n",
    "    error_analysis=error_analysis_dict\n",
    ")\n",
    "\n",
    "log(\"‚úÖ Training pipeline V2 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a7ca0",
   "metadata": {},
   "source": [
    "# 10. Usage Examples & V2 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc44fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# USAGE EXAMPLES & V2 SUMMARY\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def demonstrate_model_usage():\n",
    "    \"\"\"\n",
    "    Demonstrate how to use the fine-tuned model for ITSM ticket similarity.\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Try to use best_model if available (just trained), otherwise load from disk\n",
    "    if 'best_model' in globals():\n",
    "        model = globals()['best_model']\n",
    "        model_source = f\"memory (just trained: {globals()['save_path']})\"\n",
    "    else:\n",
    "        # Get output directory\n",
    "        if 'CONFIG' in globals():\n",
    "            output_dir = globals()['CONFIG']['output_dir']\n",
    "        else:\n",
    "            output_dir = 'models/real_servicenow_finetuned_v2'\n",
    "            log(\"‚ö†Ô∏è CONFIG not loaded. Using default model directory.\")\n",
    "        \n",
    "        # Look for most recent trained model\n",
    "        base_dir = Path(output_dir)\n",
    "        if base_dir.exists():\n",
    "            model_dirs = [d for d in base_dir.iterdir() if d.is_dir() and d.name.startswith('real_servicenow_')]\n",
    "            if model_dirs:\n",
    "                latest_model = max(model_dirs, key=lambda d: d.stat().st_mtime)\n",
    "                log(f\"üìÇ Loading most recent trained model from: {latest_model}\")\n",
    "                model = SentenceTransformer(str(latest_model))\n",
    "                model_source = str(latest_model)\n",
    "            else:\n",
    "                log(\"‚ùå No trained model found. Please run training cells first.\")\n",
    "                return None, None\n",
    "        else:\n",
    "            log(\"‚ùå No trained model found. Please run training cells first.\")\n",
    "            return None, None\n",
    "    \n",
    "    log(f\"üì¶ Using model from: {model_source}\")\n",
    "    \n",
    "    # Example tickets\n",
    "    example_tickets = [\n",
    "        \"User cannot login to SAP system. Error message: authentication failed. Tried resetting password but issue persists.\",\n",
    "        \"SAP login issue - getting access denied error when trying to connect to production system.\",\n",
    "        \"Outlook keeps crashing when opening large attachments. Have tried restarting but problem continues.\",\n",
    "        \"Email client crashes randomly. Users report Outlook freezing when opening emails with attachments.\",\n",
    "        \"Request to provision new laptop for incoming employee starting next Monday.\",\n",
    "    ]\n",
    "    \n",
    "    # Encode all tickets\n",
    "    embeddings = model.encode(example_tickets, show_progress_bar=False)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Display results\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"SIMILARITY MATRIX DEMO\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nTickets:\")\n",
    "    for i, ticket in enumerate(example_tickets):\n",
    "        print(f\"  [{i}] {ticket[:80]}...\")\n",
    "    \n",
    "    print(\"\\nSimilarity Matrix:\")\n",
    "    print(\"     \", end=\"\")\n",
    "    for i in range(len(example_tickets)):\n",
    "        print(f\"  [{i}]  \", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for i, row in enumerate(sim_matrix):\n",
    "        print(f\"[{i}]  \", end=\"\")\n",
    "        for val in row:\n",
    "            print(f\" {val:.3f} \", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Find similar ticket pairs\n",
    "    log(\"\\n\" + \"-\"*60)\n",
    "    log(\"HIGH SIMILARITY PAIRS (> 0.7):\")\n",
    "    for i in range(len(example_tickets)):\n",
    "        for j in range(i+1, len(example_tickets)):\n",
    "            if sim_matrix[i][j] > 0.7:\n",
    "                print(f\"  Tickets [{i}] & [{j}]: {sim_matrix[i][j]:.3f}\")\n",
    "                print(f\"    [{i}]: {example_tickets[i][:60]}...\")\n",
    "                print(f\"    [{j}]: {example_tickets[j][:60]}...\")\n",
    "                print()\n",
    "    \n",
    "    return model, embeddings\n",
    "\n",
    "# Run demonstration\n",
    "demo_model, demo_embeddings = demonstrate_model_usage()\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# V2 IMPROVEMENT SUMMARY\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"üìä V2 IMPROVEMENT SUMMARY\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "üÜï V2 ENHANCEMENTS IMPLEMENTED:\n",
    "\n",
    "1. HARDER NEGATIVE MINING\n",
    "   ‚îú‚îÄ Increased hard_neg_ratio: 35% ‚Üí 45%\n",
    "   ‚îú‚îÄ Stricter neg_tfidf_threshold: 0.20 ‚Üí 0.12\n",
    "   ‚îî‚îÄ Reduced easy negatives: 30% ‚Üí 25%\n",
    "\n",
    "2. CURRICULUM LEARNING\n",
    "   ‚îú‚îÄ Phase 1 (Epochs 1-2): Easier pairs (25% hard neg, threshold 0.15)\n",
    "   ‚îî‚îÄ Phase 2 (Epochs 3-4): Harder pairs (55% hard neg, threshold 0.10)\n",
    "\n",
    "3. BORDERLINE TEST SET\n",
    "   ‚îú‚îÄ TF-IDF range: 0.25 - 0.35 (ambiguous cases)\n",
    "   ‚îî‚îÄ Tests model on genuinely difficult pairs\n",
    "\n",
    "4. ERROR ANALYSIS\n",
    "   ‚îú‚îÄ Identifies worst false positives/negatives\n",
    "   ‚îú‚îÄ Analyzes text length patterns\n",
    "   ‚îî‚îÄ Provides systematic failure insights\n",
    "\n",
    "5. CROSS-VALIDATED THRESHOLD\n",
    "   ‚îú‚îÄ 5-fold CV for robust threshold selection\n",
    "   ‚îî‚îÄ Reports threshold variance across folds\n",
    "\n",
    "NEXT STEPS FOR FURTHER IMPROVEMENT:\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "üìà BASED ON RESULTS, CONSIDER:\n",
    "\n",
    "If borderline ROC-AUC < 0.80:\n",
    "   ‚Üí Increase curriculum phases to 3 (add intermediate difficulty)\n",
    "   ‚Üí Add triplet loss fine-tuning stage\n",
    "\n",
    "If many false negatives on short texts:\n",
    "   ‚Üí Reduce min_text_length filter\n",
    "   ‚Üí Add text augmentation (paraphrasing)\n",
    "\n",
    "If many false positives on same-category pairs:\n",
    "   ‚Üí Increase hard_neg_ratio further (45% ‚Üí 55%)\n",
    "   ‚Üí Remove category context from training text\n",
    "\n",
    "PRODUCTION DEPLOYMENT:\n",
    "   ‚Üí Model path: {CONFIG['output_dir']}\n",
    "   ‚Üí Use CV threshold: {cv_threshold:.4f} (¬± {cv_std:.4f})\n",
    "   ‚Üí Pre-compute embeddings for ticket corpus\n",
    "\"\"\")\n",
    "\n",
    "log(\"‚úÖ V2 training and evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

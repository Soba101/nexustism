{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072e3276",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae1f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SentenceTransformers and evaluation imports\n",
    "from sentence_transformers import SentenceTransformer, evaluation\n",
    "import torch\n",
    "\n",
    "# Sklearn metrics\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine_similarity\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e043c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect GPU and configure batch size\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.2f} GB\")\n",
    "    \n",
    "    # Auto-configure batch size based on available memory\n",
    "    if gpu_memory >= 24:\n",
    "        BATCH_SIZE = 128\n",
    "    elif gpu_memory >= 16:\n",
    "        BATCH_SIZE = 96\n",
    "    elif gpu_memory >= 12:\n",
    "        BATCH_SIZE = 64\n",
    "    elif gpu_memory >= 8:\n",
    "        BATCH_SIZE = 48\n",
    "    elif gpu_memory >= 6:\n",
    "        BATCH_SIZE = 32\n",
    "    else:\n",
    "        BATCH_SIZE = 16\n",
    "    \n",
    "    print(f\"Auto-configured batch size: {BATCH_SIZE}\")\n",
    "else:\n",
    "    BATCH_SIZE = 16  # Conservative for CPU\n",
    "    print(f\"CPU mode - using conservative batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('data_new')\n",
    "MODELS_DIR = Path('models')\n",
    "RESULTS_DIR = MODELS_DIR / 'results'\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\n✓ Configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a312cd",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e276dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ServiceNow incident data\n",
    "data_path = DATA_DIR / 'SNow_incident_ticket_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Loaded {len(df)} ServiceNow incidents\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68680ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine short_description and description for complete context\n",
    "def create_combined_text(row):\n",
    "    \"\"\"Combine short_description and description with proper handling of NaN\"\"\"\n",
    "    short_desc = str(row.get('short_description', '')).strip() if pd.notna(row.get('short_description')) else ''\n",
    "    desc = str(row.get('description', '')).strip() if pd.notna(row.get('description')) else ''\n",
    "    \n",
    "    if short_desc and desc:\n",
    "        return f\"{short_desc}. {desc}\"\n",
    "    elif short_desc:\n",
    "        return short_desc\n",
    "    elif desc:\n",
    "        return desc\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "df['combined_text'] = df.apply(create_combined_text, axis=1)\n",
    "\n",
    "# Filter out empty texts\n",
    "df = df[df['combined_text'].str.len() > 10].reset_index(drop=True)\n",
    "\n",
    "print(f\"After filtering: {len(df)} valid incidents\")\n",
    "print(f\"\\nSample combined text:\")\n",
    "print(df['combined_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d417a",
   "metadata": {},
   "source": [
    "## 3. Generate Test Pairs\n",
    "\n",
    "Creates three types of test pairs:\n",
    "1. **Positive pairs**: Same category/similar incidents (label=1)\n",
    "2. **Easy negatives**: Different categories (label=0)\n",
    "3. **Hard negatives**: Different categories but high TF-IDF similarity (label=0)\n",
    "\n",
    "This adversarial test set ensures robust evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_pairs(df: pd.DataFrame, \n",
    "                       num_positives: int = 500,\n",
    "                       num_easy_negatives: int = 500,\n",
    "                       num_hard_negatives: int = 500,\n",
    "                       random_state: int = 42) -> Tuple[List[str], List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    Generate test pairs for evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        texts1, texts2, labels\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    texts1, texts2, labels = [], [], []\n",
    "    \n",
    "    # Check if we have category information\n",
    "    has_categories = 'category' in df.columns\n",
    "    \n",
    "    if has_categories:\n",
    "        print(\"Using category-based pair generation\")\n",
    "        categories = df['category'].dropna().unique()\n",
    "        print(f\"Found {len(categories)} categories\")\n",
    "        \n",
    "        # 1. Positive pairs - same category\n",
    "        for _ in range(num_positives):\n",
    "            cat = np.random.choice(categories)\n",
    "            cat_incidents = df[df['category'] == cat]\n",
    "            if len(cat_incidents) >= 2:\n",
    "                idx1, idx2 = np.random.choice(cat_incidents.index, size=2, replace=False)\n",
    "                texts1.append(df.loc[idx1, 'combined_text'])\n",
    "                texts2.append(df.loc[idx2, 'combined_text'])\n",
    "                labels.append(1)\n",
    "        \n",
    "        # 2. Easy negatives - different categories\n",
    "        for _ in range(num_easy_negatives):\n",
    "            cat1, cat2 = np.random.choice(categories, size=2, replace=False)\n",
    "            incidents1 = df[df['category'] == cat1]\n",
    "            incidents2 = df[df['category'] == cat2]\n",
    "            if len(incidents1) > 0 and len(incidents2) > 0:\n",
    "                idx1 = np.random.choice(incidents1.index)\n",
    "                idx2 = np.random.choice(incidents2.index)\n",
    "                texts1.append(df.loc[idx1, 'combined_text'])\n",
    "                texts2.append(df.loc[idx2, 'combined_text'])\n",
    "                labels.append(0)\n",
    "    else:\n",
    "        print(\"No category column found - using random pair generation\")\n",
    "        # Random positive pairs (assume some similarity in random selection)\n",
    "        for _ in range(num_positives):\n",
    "            idx1, idx2 = np.random.choice(len(df), size=2, replace=False)\n",
    "            texts1.append(df.loc[idx1, 'combined_text'])\n",
    "            texts2.append(df.loc[idx2, 'combined_text'])\n",
    "            labels.append(1)\n",
    "        \n",
    "        # Random negative pairs\n",
    "        for _ in range(num_easy_negatives):\n",
    "            idx1, idx2 = np.random.choice(len(df), size=2, replace=False)\n",
    "            texts1.append(df.loc[idx1, 'combined_text'])\n",
    "            texts2.append(df.loc[idx2, 'combined_text'])\n",
    "            labels.append(0)\n",
    "    \n",
    "    # 3. Hard negatives - high TF-IDF similarity but different incidents\n",
    "    print(\"\\nGenerating hard negatives with TF-IDF...\")\n",
    "    sample_size = min(2000, len(df))  # Limit for performance\n",
    "    sample_df = df.sample(n=sample_size, random_state=random_state)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(sample_df['combined_text'])\n",
    "    \n",
    "    hard_neg_count = 0\n",
    "    attempts = 0\n",
    "    max_attempts = num_hard_negatives * 10\n",
    "    \n",
    "    while hard_neg_count < num_hard_negatives and attempts < max_attempts:\n",
    "        idx1 = np.random.randint(0, len(sample_df))\n",
    "        \n",
    "        # Compute similarity with all other incidents\n",
    "        similarities = sklearn_cosine_similarity(tfidf_matrix[idx1:idx1+1], tfidf_matrix).flatten()\n",
    "        \n",
    "        # Find incidents with similarity in range [0.3, 0.6] (hard negatives)\n",
    "        hard_neg_candidates = np.where((similarities > 0.3) & (similarities < 0.6))[0]\n",
    "        hard_neg_candidates = hard_neg_candidates[hard_neg_candidates != idx1]\n",
    "        \n",
    "        if len(hard_neg_candidates) > 0:\n",
    "            idx2 = np.random.choice(hard_neg_candidates)\n",
    "            \n",
    "            # If we have categories, ensure they're different\n",
    "            if has_categories:\n",
    "                cat1 = sample_df.iloc[idx1].get('category')\n",
    "                cat2 = sample_df.iloc[idx2].get('category')\n",
    "                if cat1 == cat2:\n",
    "                    attempts += 1\n",
    "                    continue\n",
    "            \n",
    "            texts1.append(sample_df.iloc[idx1]['combined_text'])\n",
    "            texts2.append(sample_df.iloc[idx2]['combined_text'])\n",
    "            labels.append(0)\n",
    "            hard_neg_count += 1\n",
    "        \n",
    "        attempts += 1\n",
    "    \n",
    "    print(f\"\\nGenerated {hard_neg_count} hard negatives ({attempts} attempts)\")\n",
    "    print(f\"\\nTotal test pairs: {len(labels)}\")\n",
    "    print(f\"  Positive pairs: {sum(labels)}\")\n",
    "    print(f\"  Negative pairs: {len(labels) - sum(labels)}\")\n",
    "    \n",
    "    return texts1, texts2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81652df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test pairs\n",
    "print(\"Generating test pairs...\\n\")\n",
    "test_texts1, test_texts2, test_labels = generate_test_pairs(\n",
    "    df,\n",
    "    num_positives=500,\n",
    "    num_easy_negatives=500,\n",
    "    num_hard_negatives=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Test set ready: {len(test_labels)} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc31289",
   "metadata": {},
   "source": [
    "## 4. Discover Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b88fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_available_models(models_dir: Path) -> Dict[str, List[Path]]:\n",
    "    \"\"\"\n",
    "    List all available trained models in the models directory.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping model names to their paths\n",
    "    \"\"\"\n",
    "    available_models = {}\n",
    "    \n",
    "    # Look for model directories (those containing config.json)\n",
    "    for model_path in models_dir.iterdir():\n",
    "        if model_path.is_dir():\n",
    "            # Check if it's a valid model directory\n",
    "            config_file = model_path / 'config.json'\n",
    "            if config_file.exists():\n",
    "                available_models[model_path.name] = model_path\n",
    "            else:\n",
    "                # Check for timestamped subdirectories (e.g., model_20251130_1451)\n",
    "                for subdir in model_path.iterdir():\n",
    "                    if subdir.is_dir() and (subdir / 'config.json').exists():\n",
    "                        model_name = f\"{model_path.name}/{subdir.name}\"\n",
    "                        available_models[model_name] = subdir\n",
    "    \n",
    "    return available_models\n",
    "\n",
    "# Discover models\n",
    "available_models = list_available_models(MODELS_DIR)\n",
    "\n",
    "print(f\"Found {len(available_models)} trained models:\\n\")\n",
    "for i, (name, path) in enumerate(sorted(available_models.items()), 1):\n",
    "    print(f\"{i:2d}. {name}\")\n",
    "    print(f\"    Path: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f2bef",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b291657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluator for ITSM similarity models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int = 32):\n",
    "        self.batch_size = batch_size\n",
    "        self.results = {}\n",
    "    \n",
    "    def evaluate_model(self,\n",
    "                      model: SentenceTransformer,\n",
    "                      model_name: str,\n",
    "                      texts1: List[str],\n",
    "                      texts2: List[str],\n",
    "                      labels: List[int],\n",
    "                      verbose: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate a model on the given test pairs.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all evaluation metrics\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\nEvaluating: {model_name}\")\n",
    "            print(\"=\" * 60)\n",
    "        \n",
    "        # Encode pairs\n",
    "        if verbose:\n",
    "            print(\"Encoding text pairs...\")\n",
    "        \n",
    "        embeddings1 = model.encode(\n",
    "            texts1,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=verbose,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        embeddings2 = model.encode(\n",
    "            texts2,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=verbose,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # Compute normalized cosine similarity\n",
    "        cosine_scores = np.sum(embeddings1 * embeddings2, axis=1) / (\n",
    "            np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n",
    "        )\n",
    "        \n",
    "        # Convert labels to numpy array\n",
    "        labels_array = np.array(labels)\n",
    "        \n",
    "        # Compute correlation metrics\n",
    "        eval_pearson, _ = pearsonr(labels_array, cosine_scores)\n",
    "        eval_spearman, _ = spearmanr(labels_array, cosine_scores)\n",
    "        \n",
    "        # Compute classification metrics\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(labels_array, cosine_scores)\n",
    "            pr_auc = average_precision_score(labels_array, cosine_scores)\n",
    "        except ValueError as e:\n",
    "            if verbose:\n",
    "                print(f\"Warning: Could not compute ROC/PR curves: {e}\")\n",
    "            roc_auc = pr_auc = None\n",
    "        \n",
    "        # Find optimal threshold for F1 score\n",
    "        thresholds = np.linspace(0, 1, 100)\n",
    "        f1_scores = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            predictions = (cosine_scores >= threshold).astype(int)\n",
    "            if len(np.unique(predictions)) > 1:  # Avoid division by zero\n",
    "                f1 = f1_score(labels_array, predictions)\n",
    "                f1_scores.append(f1)\n",
    "            else:\n",
    "                f1_scores.append(0)\n",
    "        \n",
    "        best_threshold_idx = np.argmax(f1_scores)\n",
    "        best_threshold = thresholds[best_threshold_idx]\n",
    "        best_f1 = f1_scores[best_threshold_idx]\n",
    "        \n",
    "        # Compute metrics at best threshold\n",
    "        predictions = (cosine_scores >= best_threshold).astype(int)\n",
    "        precision = precision_score(labels_array, predictions)\n",
    "        recall = recall_score(labels_array, predictions)\n",
    "        \n",
    "        # Store results\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'spearman': eval_spearman,\n",
    "            'pearson': eval_pearson,\n",
    "            'roc_auc': roc_auc,\n",
    "            'pr_auc': pr_auc,\n",
    "            'best_threshold': best_threshold,\n",
    "            'best_f1': best_f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'cosine_scores': cosine_scores,\n",
    "            'labels': labels_array\n",
    "        }\n",
    "        \n",
    "        self.results[model_name] = results\n",
    "        \n",
    "        # Print summary\n",
    "        if verbose:\n",
    "            print(\"\\nMetrics:\")\n",
    "            print(f\"  Spearman Correlation: {eval_spearman:.4f}\")\n",
    "            print(f\"  Pearson Correlation:  {eval_pearson:.4f}\")\n",
    "            if roc_auc is not None:\n",
    "                print(f\"  ROC-AUC:             {roc_auc:.4f}\")\n",
    "                print(f\"  PR-AUC:              {pr_auc:.4f}\")\n",
    "            print(f\"\\nClassification (threshold={best_threshold:.3f}):\")\n",
    "            print(f\"  F1 Score:            {best_f1:.4f}\")\n",
    "            print(f\"  Precision:           {precision:.4f}\")\n",
    "            print(f\"  Recall:              {recall:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compare_models(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a comparison table of all evaluated models.\n",
    "        \"\"\"\n",
    "        comparison_data = []\n",
    "        \n",
    "        for model_name, results in self.results.items():\n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'Spearman': results['spearman'],\n",
    "                'Pearson': results['pearson'],\n",
    "                'ROC-AUC': results['roc_auc'],\n",
    "                'PR-AUC': results['pr_auc'],\n",
    "                'Best F1': results['best_f1'],\n",
    "                'Threshold': results['best_threshold'],\n",
    "                'Precision': results['precision'],\n",
    "                'Recall': results['recall']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(comparison_data)\n",
    "    \n",
    "    def plot_roc_curves(self, figsize=(10, 8)):\n",
    "        \"\"\"\n",
    "        Plot ROC curves for all evaluated models.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        for model_name, results in self.results.items():\n",
    "            if results['roc_auc'] is not None:\n",
    "                fpr, tpr, _ = roc_curve(results['labels'], results['cosine_scores'])\n",
    "                plt.plot(fpr, tpr, label=f\"{model_name} (AUC={results['roc_auc']:.4f})\", linewidth=2)\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC=0.5000)', linewidth=1)\n",
    "        plt.xlabel('False Positive Rate', fontsize=12)\n",
    "        plt.ylabel('True Positive Rate', fontsize=12)\n",
    "        plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "        plt.legend(loc='lower right', fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_pr_curves(self, figsize=(10, 8)):\n",
    "        \"\"\"\n",
    "        Plot Precision-Recall curves for all evaluated models.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        for model_name, results in self.results.items():\n",
    "            if results['pr_auc'] is not None:\n",
    "                precision, recall, _ = precision_recall_curve(results['labels'], results['cosine_scores'])\n",
    "                plt.plot(recall, precision, label=f\"{model_name} (AP={results['pr_auc']:.4f})\", linewidth=2)\n",
    "        \n",
    "        # Baseline (proportion of positive class)\n",
    "        baseline = np.mean(list(self.results.values())[0]['labels'])\n",
    "        plt.plot([0, 1], [baseline, baseline], 'k--', label=f'Random (AP={baseline:.4f})', linewidth=1)\n",
    "        \n",
    "        plt.xlabel('Recall', fontsize=12)\n",
    "        plt.ylabel('Precision', fontsize=12)\n",
    "        plt.title('Precision-Recall Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "        plt.legend(loc='lower left', fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_score_distributions(self, figsize=(14, 6)):\n",
    "        \"\"\"\n",
    "        Plot similarity score distributions for all evaluated models.\n",
    "        \"\"\"\n",
    "        n_models = len(self.results)\n",
    "        fig, axes = plt.subplots(1, n_models, figsize=figsize, sharey=True)\n",
    "        \n",
    "        if n_models == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, (model_name, results) in enumerate(self.results.items()):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Separate scores by label\n",
    "            pos_scores = results['cosine_scores'][results['labels'] == 1]\n",
    "            neg_scores = results['cosine_scores'][results['labels'] == 0]\n",
    "            \n",
    "            # Plot histograms\n",
    "            ax.hist(neg_scores, bins=50, alpha=0.6, label='Negative (0)', color='red', edgecolor='black')\n",
    "            ax.hist(pos_scores, bins=50, alpha=0.6, label='Positive (1)', color='green', edgecolor='black')\n",
    "            \n",
    "            # Mark optimal threshold\n",
    "            ax.axvline(results['best_threshold'], color='blue', linestyle='--', linewidth=2,\n",
    "                      label=f\"Threshold={results['best_threshold']:.3f}\")\n",
    "            \n",
    "            ax.set_xlabel('Cosine Similarity Score', fontsize=11)\n",
    "            if idx == 0:\n",
    "                ax.set_ylabel('Frequency', fontsize=11)\n",
    "            ax.set_title(model_name, fontsize=12, fontweight='bold')\n",
    "            ax.legend(fontsize=9)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_results(self, output_path: Path):\n",
    "        \"\"\"\n",
    "        Save evaluation results to CSV.\n",
    "        \"\"\"\n",
    "        comparison_df = self.compare_models()\n",
    "        comparison_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\n✓ Results saved to: {output_path}\")\n",
    "\n",
    "print(\"✓ ModelEvaluator class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c579f50",
   "metadata": {},
   "source": [
    "## 6. Evaluate Baseline Model (Raw MPNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1977da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load baseline model (raw MPNet without fine-tuning)\n",
    "print(\"Loading baseline model: sentence-transformers/all-mpnet-base-v2\")\n",
    "baseline_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_results = evaluator.evaluate_model(\n",
    "    model=baseline_model,\n",
    "    model_name='Baseline (Raw MPNet)',\n",
    "    texts1=test_texts1,\n",
    "    texts2=test_texts2,\n",
    "    labels=test_labels,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Baseline evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0202b3",
   "metadata": {},
   "source": [
    "## 7. Evaluate Fine-Tuned Models\n",
    "\n",
    "Select models to evaluate from the list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad35966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select models to evaluate (modify this list as needed)\n",
    "# Examples:\n",
    "#   - 'v6_refactored_finetuned'\n",
    "#   - 'real_servicenow_finetuned_mpnet'\n",
    "#   - 'real_servicenow_finetuned_nomic'\n",
    "#   - 'all-mpnet-finetuned-v6'\n",
    "\n",
    "models_to_evaluate = [\n",
    "    'v6_refactored_finetuned',\n",
    "    'real_servicenow_finetuned_mpnet',\n",
    "    'real_servicenow_finetuned_nomic',\n",
    "    'all-mpnet-finetuned-v6'\n",
    "]\n",
    "\n",
    "print(f\"Selected {len(models_to_evaluate)} models for evaluation:\\n\")\n",
    "for model_name in models_to_evaluate:\n",
    "    if model_name in available_models:\n",
    "        print(f\"  ✓ {model_name}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {model_name} (not found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each selected model\n",
    "for model_name in models_to_evaluate:\n",
    "    if model_name not in available_models:\n",
    "        print(f\"\\n⚠ Skipping {model_name} - not found in models directory\")\n",
    "        continue\n",
    "    \n",
    "    model_path = available_models[model_name]\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        print(f\"\\nLoading model from: {model_path}\")\n",
    "        model = SentenceTransformer(str(model_path), device=device)\n",
    "        \n",
    "        # Evaluate\n",
    "        evaluator.evaluate_model(\n",
    "            model=model,\n",
    "            model_name=model_name,\n",
    "            texts1=test_texts1,\n",
    "            texts2=test_texts2,\n",
    "            labels=test_labels,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error evaluating {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ All model evaluations complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6808c",
   "metadata": {},
   "source": [
    "## 8. Model Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2339dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison table\n",
    "comparison_df = evaluator.compare_models()\n",
    "\n",
    "# Sort by Spearman correlation (primary metric)\n",
    "comparison_df = comparison_df.sort_values('Spearman', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Styled DataFrame for better visualization\n",
    "styled_df = comparison_df.style.background_gradient(\n",
    "    subset=['Spearman', 'Pearson', 'ROC-AUC', 'PR-AUC', 'Best F1'],\n",
    "    cmap='RdYlGn',\n",
    "    vmin=0.0,\n",
    "    vmax=1.0\n",
    ").format({\n",
    "    'Spearman': '{:.4f}',\n",
    "    'Pearson': '{:.4f}',\n",
    "    'ROC-AUC': '{:.4f}',\n",
    "    'PR-AUC': '{:.4f}',\n",
    "    'Best F1': '{:.4f}',\n",
    "    'Threshold': '{:.3f}',\n",
    "    'Precision': '{:.4f}',\n",
    "    'Recall': '{:.4f}'\n",
    "})\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf488280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "evaluator.plot_roc_curves(figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ebd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curves\n",
    "evaluator.plot_pr_curves(figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity score distributions\n",
    "evaluator.plot_score_distributions(figsize=(16, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853a074",
   "metadata": {},
   "source": [
    "## 9. Metric Analysis by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a54838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparing key metrics across models\n",
    "metrics_to_plot = ['Spearman', 'Pearson', 'ROC-AUC', 'PR-AUC', 'Best F1']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Sort by metric value\n",
    "    sorted_df = comparison_df.sort_values(metric, ascending=True)\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    colors = ['red' if model == 'Baseline (Raw MPNet)' else 'steelblue' \n",
    "              for model in sorted_df['Model']]\n",
    "    \n",
    "    ax.barh(sorted_df['Model'], sorted_df[metric], color=colors, edgecolor='black')\n",
    "    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (model, value) in enumerate(zip(sorted_df['Model'], sorted_df[metric])):\n",
    "        if pd.notna(value):\n",
    "            ax.text(value + 0.01, i, f'{value:.4f}', va='center', fontsize=9)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f118ed3",
   "metadata": {},
   "source": [
    "## 10. Performance Delta vs Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement over baseline\n",
    "baseline_name = 'Baseline (Raw MPNet)'\n",
    "\n",
    "if baseline_name in evaluator.results:\n",
    "    baseline_metrics = {\n",
    "        'Spearman': evaluator.results[baseline_name]['spearman'],\n",
    "        'Pearson': evaluator.results[baseline_name]['pearson'],\n",
    "        'ROC-AUC': evaluator.results[baseline_name]['roc_auc'],\n",
    "        'PR-AUC': evaluator.results[baseline_name]['pr_auc'],\n",
    "        'Best F1': evaluator.results[baseline_name]['best_f1']\n",
    "    }\n",
    "    \n",
    "    improvement_data = []\n",
    "    \n",
    "    for model_name, results in evaluator.results.items():\n",
    "        if model_name == baseline_name:\n",
    "            continue\n",
    "        \n",
    "        improvement_data.append({\n",
    "            'Model': model_name,\n",
    "            'Δ Spearman': results['spearman'] - baseline_metrics['Spearman'],\n",
    "            'Δ Pearson': results['pearson'] - baseline_metrics['Pearson'],\n",
    "            'Δ ROC-AUC': results['roc_auc'] - baseline_metrics['ROC-AUC'] if results['roc_auc'] else None,\n",
    "            'Δ PR-AUC': results['pr_auc'] - baseline_metrics['PR-AUC'] if results['pr_auc'] else None,\n",
    "            'Δ F1': results['best_f1'] - baseline_metrics['Best F1'],\n",
    "        })\n",
    "    \n",
    "    improvement_df = pd.DataFrame(improvement_data)\n",
    "    improvement_df = improvement_df.sort_values('Δ Spearman', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"IMPROVEMENT OVER BASELINE (Raw MPNet)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(improvement_df.to_string(index=False))\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Styled version\n",
    "    styled_improvement = improvement_df.style.background_gradient(\n",
    "        subset=['Δ Spearman', 'Δ Pearson', 'Δ ROC-AUC', 'Δ PR-AUC', 'Δ F1'],\n",
    "        cmap='RdYlGn',\n",
    "        vmin=-0.1,\n",
    "        vmax=0.3\n",
    "    ).format({\n",
    "        'Δ Spearman': '{:+.4f}',\n",
    "        'Δ Pearson': '{:+.4f}',\n",
    "        'Δ ROC-AUC': '{:+.4f}',\n",
    "        'Δ PR-AUC': '{:+.4f}',\n",
    "        'Δ F1': '{:+.4f}'\n",
    "    })\n",
    "    \n",
    "    display(styled_improvement)\n",
    "else:\n",
    "    print(\"Baseline model not found in results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08673d01",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36632629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "results_file = RESULTS_DIR / f'model_evaluation_{timestamp}.csv'\n",
    "\n",
    "evaluator.save_results(results_file)\n",
    "\n",
    "# Also save improvement metrics if available\n",
    "if 'improvement_df' in locals():\n",
    "    improvement_file = RESULTS_DIR / f'model_improvement_{timestamp}.csv'\n",
    "    improvement_df.to_csv(improvement_file, index=False)\n",
    "    print(f\"✓ Improvement metrics saved to: {improvement_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c0d0a",
   "metadata": {},
   "source": [
    "## 12. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTest Set Size: {len(test_labels)} pairs\")\n",
    "print(f\"  - Positive pairs: {sum(test_labels)} ({100*sum(test_labels)/len(test_labels):.1f}%)\")\n",
    "print(f\"  - Negative pairs: {len(test_labels) - sum(test_labels)} ({100*(len(test_labels)-sum(test_labels))/len(test_labels):.1f}%)\")\n",
    "\n",
    "print(f\"\\nModels Evaluated: {len(evaluator.results)}\")\n",
    "for model_name in evaluator.results.keys():\n",
    "    print(f\"  - {model_name}\")\n",
    "\n",
    "# Best model for each metric\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"BEST PERFORMING MODELS BY METRIC\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for metric in ['Spearman', 'Pearson', 'ROC-AUC', 'PR-AUC', 'Best F1']:\n",
    "    best_row = comparison_df.loc[comparison_df[metric].idxmax()]\n",
    "    print(f\"\\n{metric:15s}: {best_row['Model']}\")\n",
    "    print(f\"                 Value: {best_row[metric]:.4f}\")\n",
    "\n",
    "# Overall recommendation\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Use Spearman as primary metric (most important for ranking)\n",
    "best_overall = comparison_df.loc[comparison_df['Spearman'].idxmax()]\n",
    "print(f\"\\nBest Overall Model: {best_overall['Model']}\")\n",
    "print(f\"  Spearman: {best_overall['Spearman']:.4f}\")\n",
    "print(f\"  Pearson:  {best_overall['Pearson']:.4f}\")\n",
    "print(f\"  ROC-AUC:  {best_overall['ROC-AUC']:.4f}\")\n",
    "print(f\"  PR-AUC:   {best_overall['PR-AUC']:.4f}\")\n",
    "\n",
    "if baseline_name in evaluator.results:\n",
    "    baseline_spearman = evaluator.results[baseline_name]['spearman']\n",
    "    improvement_pct = 100 * (best_overall['Spearman'] - baseline_spearman) / baseline_spearman\n",
    "    print(f\"\\nImprovement over baseline: {improvement_pct:+.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf813d1",
   "metadata": {},
   "source": [
    "## 13. Optional: Evaluate Additional Models\n",
    "\n",
    "You can add more models to evaluate by modifying the `models_to_evaluate` list in cell 7 and re-running from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b86fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell can be used to quickly evaluate a single additional model\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# additional_model_name = 'all-mpnet-finetuned-v5'\n",
    "# \n",
    "# if additional_model_name in available_models:\n",
    "#     model_path = available_models[additional_model_name]\n",
    "#     model = SentenceTransformer(str(model_path), device=device)\n",
    "#     \n",
    "#     evaluator.evaluate_model(\n",
    "#         model=model,\n",
    "#         model_name=additional_model_name,\n",
    "#         texts1=test_texts1,\n",
    "#         texts2=test_texts2,\n",
    "#         labels=test_labels,\n",
    "#         verbose=True\n",
    "#     )\n",
    "#     \n",
    "#     del model\n",
    "#     if device == 'cuda':\n",
    "#         torch.cuda.empty_cache()\n",
    "# else:\n",
    "#     print(f\"Model '{additional_model_name}' not found\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

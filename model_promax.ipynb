{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184dfd2c",
   "metadata": {},
   "source": [
    "# ğŸš€ ITSM Ticket Similarity - Real ServiceNow Data Training\n",
    "\n",
    "**Model:** `sentence-transformers/all-mpnet-base-v2` (768-dim embeddings)  \n",
    "**Data:** Real ServiceNow incidents (Mar 2024 â†’ Sep 2025, ~30K+ tickets)  \n",
    "**Use Case:** Find similar tickets, detect duplicates, assist routing\n",
    "\n",
    "## Key Differences from Dummy Data Pipeline\n",
    "1. **No Short Description** â€” Real data only has `Description` field\n",
    "2. **Rich vocabulary** â€” 10K+ unique terms vs 111 in dummy data\n",
    "3. **Realistic metrics expected** â€” ROC-AUC 0.85-0.90, Spearman 0.65-0.75\n",
    "4. **No Resolution notes in training text** â€” Avoids data leakage for new tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9dc0f3",
   "metadata": {},
   "source": [
    "# 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118e0c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All packages installed\n"
     ]
    }
   ],
   "source": [
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment variables to suppress warnings\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_SILENT'] = 'true'\n",
    "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Install required packages\n",
    "def ensure_packages():\n",
    "    try:\n",
    "        import importlib.metadata as importlib_metadata\n",
    "    except ImportError:\n",
    "        import importlib_metadata\n",
    "    \n",
    "    required = {\n",
    "        'sentence-transformers': 'sentence-transformers>=2.2.2',\n",
    "        'torch': 'torch',\n",
    "        'scikit-learn': 'scikit-learn>=1.3.0',\n",
    "        'pandas': 'pandas',\n",
    "        'numpy': 'numpy>=1.24.0',\n",
    "        'tqdm': 'tqdm',\n",
    "        'matplotlib': 'matplotlib',\n",
    "        'seaborn': 'seaborn',\n",
    "    }\n",
    "    \n",
    "    missing = []\n",
    "    for name, spec in required.items():\n",
    "        try:\n",
    "            importlib_metadata.version(name)\n",
    "        except importlib_metadata.PackageNotFoundError:\n",
    "            missing.append(spec)\n",
    "    \n",
    "    if missing:\n",
    "        print(f'ğŸ“¦ Installing: {\", \".join(missing)}')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', *missing])\n",
    "    else:\n",
    "        print('âœ… All packages installed')\n",
    "\n",
    "ensure_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1ad25",
   "metadata": {},
   "source": [
    "# 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae876b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No GPU detected. Running on CPU.\n",
      "ğŸ“Š Device: cpu, Batch Size: 8\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Simple logging\n",
    "def log(msg, level=logging.INFO):\n",
    "    print(msg)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'sentence-transformers/all-mpnet-base-v2',\n",
    "    'output_dir': 'models/real_servicenow_finetuned',\n",
    "    \n",
    "    # Data\n",
    "    'source_data': 'data_new/incident ticket volume from 18th Mar\\'24 till date 4th Sep\\'25 @5PM SGT.csv',\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'epochs': 4,              # Real data needs fewer epochs (more signal)\n",
    "    'batch_size': 16,         # Will auto-reduce for MPS if needed\n",
    "    'lr': 2e-5,\n",
    "    'max_seq_length': 384,\n",
    "    'warmup_ratio': 0.1,\n",
    "    \n",
    "    # Pair generation\n",
    "    'num_pairs': 50000,       # 50K pairs for real data\n",
    "    'pos_ratio': 0.35,        # 35% positives\n",
    "    'hard_neg_ratio': 0.35,   # 35% hard negatives (same category, low similarity)\n",
    "    'easy_neg_ratio': 0.30,   # 30% easy negatives (cross-category)\n",
    "    'pos_tfidf_threshold': 0.35,   # Lower threshold for real data (more variety)\n",
    "    'neg_tfidf_threshold': 0.20,   # Upper bound for negatives\n",
    "    \n",
    "    # Data splits\n",
    "    'eval_split': 0.15,\n",
    "    'holdout_split': 0.10,\n",
    "    \n",
    "    # Minimum text length (filter short descriptions)\n",
    "    'min_text_length': 25,\n",
    "    \n",
    "    # Seed\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "\n",
    "# Device detection: CUDA > MPS > CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    torch.cuda.manual_seed_all(CONFIG['seed'])\n",
    "    log(f\"ğŸš€ CUDA Detected: {torch.cuda.get_device_name(0)}\")\n",
    "    # Keep batch_size=16 for CUDA\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    torch.mps.manual_seed(CONFIG['seed'])\n",
    "    log(\"ğŸ MPS (Apple Silicon) Detected\")\n",
    "    CONFIG['batch_size'] = 8  # Reduce for MPS memory\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    log(\"âš ï¸ No GPU detected. Running on CPU.\")\n",
    "    CONFIG['batch_size'] = 8\n",
    "\n",
    "log(f\"ğŸ“Š Device: {DEVICE}, Batch Size: {CONFIG['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b3f39",
   "metadata": {},
   "source": [
    "# 3. Data Loading & Preprocessing\n",
    "\n",
    "Load real ServiceNow incident data. Key differences from dummy data:\n",
    "- Only `Description` field (no Short Description)\n",
    "- Multi-line text with embedded newlines\n",
    "- Rich technical vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e629653",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Data file not found: data_new/incident ticket volume from 18th Mar'24 till date 4th Sep'25 @5PM SGT.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3219831685.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mdf_incidents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_real_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nâœ… Loaded {len(df_incidents):,} incidents ready for training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3219831685.py\u001b[0m in \u001b[0;36mload_and_preprocess_real_data\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msource_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msource_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Data file not found: {source_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ğŸ“‚ Loading real ServiceNow data from: {source_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Data file not found: data_new/incident ticket volume from 18th Mar'24 till date 4th Sep'25 @5PM SGT.csv"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def load_and_preprocess_real_data(config):\n",
    "    \"\"\"\n",
    "    Load and preprocess real ServiceNow incident data.\n",
    "    \n",
    "    Key differences from dummy data:\n",
    "    - Only 'Description' column (no 'Short Description')\n",
    "    - Real data has multi-line descriptions with embedded newlines\n",
    "    - Richer vocabulary and more varied text\n",
    "    \"\"\"\n",
    "    source_path = Path(config['source_data'])\n",
    "    if not source_path.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {source_path}\")\n",
    "    \n",
    "    log(f\"ğŸ“‚ Loading real ServiceNow data from: {source_path}\")\n",
    "    \n",
    "    # Load CSV - handle multi-line fields\n",
    "    df = pd.read_csv(source_path, encoding='utf-8', on_bad_lines='skip')\n",
    "    initial_count = len(df)\n",
    "    log(f\"ğŸ“Š Loaded {initial_count:,} raw records\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['Number', 'Description', 'Category', 'Subcategory', \n",
    "                     'Service', 'Service offering', 'Assignment group']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        log(f\"âš ï¸ Missing columns: {missing_cols}\")\n",
    "        # Use available columns\n",
    "        required_cols = [col for col in required_cols if col in df.columns]\n",
    "    \n",
    "    log(f\"âœ… Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Clean text function\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text) or text is None:\n",
    "            return \"\"\n",
    "        text = str(text).strip()\n",
    "        # Normalize whitespace (collapse multiple spaces/newlines)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove common boilerplate\n",
    "        text = re.sub(r'Note\\s*:\\s*This is an automated.*?\\.', '', text, flags=re.IGNORECASE)\n",
    "        return text.strip()\n",
    "    \n",
    "    # Clean Description\n",
    "    df['Description'] = df['Description'].apply(clean_text)\n",
    "    \n",
    "    # Fill NA for context columns\n",
    "    context_cols = ['Category', 'Subcategory', 'Service', 'Service offering', 'Assignment group']\n",
    "    for col in context_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('').astype(str).str.strip().str.lower()\n",
    "    \n",
    "    # Build contextual text representation\n",
    "    # Format: \"Description (Context: [Service|Offering] [Category|Subcategory] Group: X)\"\n",
    "    def build_context(row):\n",
    "        parts = []\n",
    "        \n",
    "        # Service context\n",
    "        service_parts = []\n",
    "        if row.get('Service', ''):\n",
    "            service_parts.append(row['Service'])\n",
    "        if row.get('Service offering', ''):\n",
    "            service_parts.append(row['Service offering'])\n",
    "        if service_parts:\n",
    "            parts.append(f\"[{' | '.join(service_parts)}]\")\n",
    "        \n",
    "        # Category context\n",
    "        cat_parts = []\n",
    "        if row.get('Category', ''):\n",
    "            cat_parts.append(row['Category'])\n",
    "        if row.get('Subcategory', ''):\n",
    "            cat_parts.append(row['Subcategory'])\n",
    "        if cat_parts:\n",
    "            parts.append(f\"[{' | '.join(cat_parts)}]\")\n",
    "        \n",
    "        # Assignment group\n",
    "        if row.get('Assignment group', ''):\n",
    "            parts.append(f\"Group: {row['Assignment group']}\")\n",
    "        \n",
    "        return ' '.join(parts) if parts else ''\n",
    "    \n",
    "    # Build full text: Description + Context suffix\n",
    "    df['context'] = df.apply(build_context, axis=1)\n",
    "    df['text'] = df.apply(\n",
    "        lambda row: f\"{row['Description']} (Context: {row['context']})\" if row['context'] else row['Description'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Filter out short/empty descriptions\n",
    "    df = df[df['Description'].str.len() >= config['min_text_length']].copy()\n",
    "    log(f\"ğŸ“‰ After filtering short descriptions: {len(df):,} records (dropped {initial_count - len(df):,})\")\n",
    "    \n",
    "    # Create category_id for stratified splitting\n",
    "    if 'Category' in df.columns and 'Subcategory' in df.columns:\n",
    "        df['category_id'] = df.groupby(['Category', 'Subcategory']).ngroup()\n",
    "    else:\n",
    "        df['category_id'] = 0\n",
    "    \n",
    "    # Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Summary stats\n",
    "    log(f\"\\nğŸ“Š Data Summary:\")\n",
    "    log(f\"   Total records: {len(df):,}\")\n",
    "    log(f\"   Unique categories: {df['category_id'].nunique()}\")\n",
    "    log(f\"   Avg text length: {df['text'].str.len().mean():.0f} chars\")\n",
    "    log(f\"   Min text length: {df['text'].str.len().min()} chars\")\n",
    "    log(f\"   Max text length: {df['text'].str.len().max()} chars\")\n",
    "    \n",
    "    # Sample text\n",
    "    log(f\"\\nğŸ“ Sample preprocessed text:\")\n",
    "    log(f\"   '{df['text'].iloc[0][:200]}...'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "df_incidents = load_and_preprocess_real_data(CONFIG)\n",
    "print(f\"\\nâœ… Loaded {len(df_incidents):,} incidents ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35dc0c",
   "metadata": {},
   "source": [
    "# 4. Data Splitting\n",
    "\n",
    "Split into Train / Eval / Holdout sets with stratification by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d80a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df, config):\n",
    "    \"\"\"\n",
    "    Three-way split: Train / Eval / Holdout\n",
    "    - Holdout is completely unseen (for adversarial diagnostic)\n",
    "    - Stratified by category to ensure representation\n",
    "    \"\"\"\n",
    "    # Handle rare categories: group categories with <2 samples\n",
    "    category_counts = df['category_id'].value_counts()\n",
    "    rare_categories = category_counts[category_counts < 2].index\n",
    "    \n",
    "    # Create stratification column: use category_id for common categories, -1 for rare\n",
    "    df['stratify_col'] = df['category_id'].copy()\n",
    "    df.loc[df['category_id'].isin(rare_categories), 'stratify_col'] = -1\n",
    "    \n",
    "    # Check if we can stratify (need at least 2 samples per class)\n",
    "    stratify_counts = df['stratify_col'].value_counts()\n",
    "    can_stratify = all(stratify_counts >= 2)\n",
    "    \n",
    "    if can_stratify:\n",
    "        # First split: separate holdout set (stratified)\n",
    "        train_eval_df, holdout_df = train_test_split(\n",
    "            df,\n",
    "            test_size=config['holdout_split'],\n",
    "            stratify=df['stratify_col'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "        \n",
    "        # Second split: train/eval from remaining (stratified)\n",
    "        train_df, eval_df = train_test_split(\n",
    "            train_eval_df,\n",
    "            test_size=config['eval_split'],\n",
    "            stratify=train_eval_df['stratify_col'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "        log(\"âœ… Using stratified split\")\n",
    "    else:\n",
    "        # Fallback: random split without stratification\n",
    "        log(\"âš ï¸ Using random split (categories too imbalanced for stratification)\")\n",
    "        train_eval_df, holdout_df = train_test_split(\n",
    "            df,\n",
    "            test_size=config['holdout_split'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "        \n",
    "        train_df, eval_df = train_test_split(\n",
    "            train_eval_df,\n",
    "            test_size=config['eval_split'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "    \n",
    "    return train_df, eval_df, holdout_df\n",
    "\n",
    "# Split the data\n",
    "train_df, eval_df, holdout_df = split_data(df_incidents, CONFIG)\n",
    "\n",
    "log(f\"ğŸ“Š Data Splits:\")\n",
    "log(f\"   Train:   {len(train_df):,} incidents ({len(train_df)/len(df_incidents)*100:.1f}%)\")\n",
    "log(f\"   Eval:    {len(eval_df):,} incidents ({len(eval_df)/len(df_incidents)*100:.1f}%)\")\n",
    "log(f\"   Holdout: {len(holdout_df):,} incidents ({len(holdout_df)/len(df_incidents)*100:.1f}%)\")\n",
    "\n",
    "# Check for data leakage\n",
    "def check_overlap(df1, df2, name1, name2):\n",
    "    overlap = len(set(df1['Number']) & set(df2['Number']))\n",
    "    log(f\"   {name1} âˆ© {name2}: {overlap} incidents\")\n",
    "\n",
    "log(f\"\\nğŸ” Overlap Check:\")\n",
    "check_overlap(train_df, eval_df, \"Train\", \"Eval\")\n",
    "check_overlap(train_df, holdout_df, \"Train\", \"Holdout\")\n",
    "check_overlap(eval_df, holdout_df, \"Eval\", \"Holdout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfadea",
   "metadata": {},
   "source": [
    "# 5. Pair Generation\n",
    "\n",
    "Generate training pairs using TF-IDF similarity mining:\n",
    "- **35% Positives**: High TF-IDF similarity (> 0.35)\n",
    "- **35% Hard Negatives**: Same category, low TF-IDF (< 0.20)\n",
    "- **30% Easy Negatives**: Different category, low TF-IDF\n",
    "\n",
    "This forces the model to learn semantic content, not just category matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import InputExample\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "class TFIDFSimilarityCalculator:\n",
    "    \"\"\"Efficient TF-IDF similarity calculator for large datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, max_features=15000):\n",
    "        log(\"â³ Building TF-IDF matrix...\")\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),  # Unigrams + bigrams for better matching\n",
    "            min_df=2,           # Ignore very rare terms\n",
    "            max_df=0.95         # Ignore very common terms\n",
    "        )\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "        log(f\"âœ… TF-IDF matrix: {self.tfidf_matrix.shape} (vocab size: {len(self.vectorizer.vocabulary_)})\")\n",
    "    \n",
    "    def similarity(self, idx1, idx2):\n",
    "        \"\"\"Compute cosine similarity between two documents.\"\"\"\n",
    "        if idx1 >= self.tfidf_matrix.shape[0] or idx2 >= self.tfidf_matrix.shape[0]:\n",
    "            return 0.0\n",
    "        vec1 = self.tfidf_matrix[idx1]\n",
    "        vec2 = self.tfidf_matrix[idx2]\n",
    "        return (vec1 @ vec2.T).toarray()[0][0]\n",
    "\n",
    "\n",
    "def generate_training_pairs(df, target_count, config, desc=\"\"):\n",
    "    \"\"\"\n",
    "    Generate training pairs with 35/35/30 split:\n",
    "    - 35% positives (high TF-IDF similarity)\n",
    "    - 35% hard negatives (same category, low TF-IDF)\n",
    "    - 30% easy negatives (cross-category, low TF-IDF)\n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Build TF-IDF for this split\n",
    "    tfidf_calc = TFIDFSimilarityCalculator(df['text'].tolist())\n",
    "    \n",
    "    # Calculate targets\n",
    "    pos_target = int(target_count * config['pos_ratio'])\n",
    "    hard_neg_target = int(target_count * config['hard_neg_ratio'])\n",
    "    easy_neg_target = target_count - pos_target - hard_neg_target\n",
    "    \n",
    "    pairs = []\n",
    "    all_indices = list(df.index)\n",
    "    \n",
    "    # Group by category for hard negatives\n",
    "    category_groups = df.groupby('category_id').indices\n",
    "    valid_groups = {k: list(v) for k, v in category_groups.items() if len(v) >= 2}\n",
    "    \n",
    "    log(f\"\\nğŸ¯ Generating {target_count:,} pairs for {desc}:\")\n",
    "    log(f\"   Target: {pos_target:,} positives, {hard_neg_target:,} hard neg, {easy_neg_target:,} easy neg\")\n",
    "    \n",
    "    # ============================================\n",
    "    # 1. POSITIVES: High TF-IDF similarity (> threshold)\n",
    "    # ============================================\n",
    "    pbar = tqdm(total=pos_target, desc=\"Positives (high TF-IDF)\")\n",
    "    attempts, max_attempts = 0, pos_target * 50\n",
    "    \n",
    "    while len(pairs) < pos_target and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        i1, i2 = random.sample(all_indices, 2)\n",
    "        if i1 == i2:\n",
    "            continue\n",
    "        \n",
    "        sim = tfidf_calc.similarity(i1, i2)\n",
    "        if sim > config['pos_tfidf_threshold']:\n",
    "            pairs.append(InputExample(\n",
    "                texts=[df.at[i1, 'text'], df.at[i2, 'text']],\n",
    "                label=1.0\n",
    "            ))\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    actual_pos = len(pairs)\n",
    "    \n",
    "    # ============================================\n",
    "    # 2. HARD NEGATIVES: Same category, low TF-IDF\n",
    "    # ============================================\n",
    "    current_len = len(pairs)\n",
    "    pbar = tqdm(total=hard_neg_target, desc=\"Hard Negatives (same cat)\")\n",
    "    attempts, max_attempts = 0, hard_neg_target * 50\n",
    "    \n",
    "    while (len(pairs) - current_len) < hard_neg_target and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        if not valid_groups:\n",
    "            break\n",
    "        \n",
    "        # Pick a random category with 2+ members\n",
    "        gid = random.choice(list(valid_groups.keys()))\n",
    "        g_indices = valid_groups[gid]\n",
    "        if len(g_indices) < 2:\n",
    "            continue\n",
    "        \n",
    "        i1, i2 = random.sample(g_indices, 2)\n",
    "        sim = tfidf_calc.similarity(i1, i2)\n",
    "        \n",
    "        if sim < config['neg_tfidf_threshold']:\n",
    "            pairs.append(InputExample(\n",
    "                texts=[df.at[i1, 'text'], df.at[i2, 'text']],\n",
    "                label=0.0\n",
    "            ))\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    actual_hard = len(pairs) - current_len\n",
    "    \n",
    "    # ============================================\n",
    "    # 3. EASY NEGATIVES: Cross-category, low TF-IDF\n",
    "    # ============================================\n",
    "    current_len = len(pairs)\n",
    "    pbar = tqdm(total=easy_neg_target, desc=\"Easy Negatives (cross-cat)\")\n",
    "    attempts, max_attempts = 0, easy_neg_target * 50\n",
    "    \n",
    "    while (len(pairs) - current_len) < easy_neg_target and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        i1, i2 = random.sample(all_indices, 2)\n",
    "        \n",
    "        # Must be different categories\n",
    "        if df.at[i1, 'category_id'] == df.at[i2, 'category_id']:\n",
    "            continue\n",
    "        \n",
    "        sim = tfidf_calc.similarity(i1, i2)\n",
    "        if sim < config['neg_tfidf_threshold']:\n",
    "            pairs.append(InputExample(\n",
    "                texts=[df.at[i1, 'text'], df.at[i2, 'text']],\n",
    "                label=0.0\n",
    "            ))\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    actual_easy = len(pairs) - current_len\n",
    "    \n",
    "    # Summary\n",
    "    total_pos = sum(1 for p in pairs if p.label == 1.0)\n",
    "    total_neg = len(pairs) - total_pos\n",
    "    \n",
    "    log(f\"âœ… Generated {len(pairs):,} pairs:\")\n",
    "    log(f\"   Positives: {actual_pos:,} ({actual_pos/len(pairs)*100:.1f}%)\")\n",
    "    log(f\"   Hard Neg:  {actual_hard:,} ({actual_hard/len(pairs)*100:.1f}%)\")\n",
    "    log(f\"   Easy Neg:  {actual_easy:,} ({actual_easy/len(pairs)*100:.1f}%)\")\n",
    "    \n",
    "    # Clean up TF-IDF to free memory\n",
    "    del tfidf_calc\n",
    "    gc.collect()\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1053863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pairs for each split\n",
    "# Scale pair counts based on split sizes\n",
    "train_pair_count = int(CONFIG['num_pairs'] * 0.75)  # 75% for training\n",
    "eval_pair_count = int(CONFIG['num_pairs'] * 0.15)   # 15% for eval\n",
    "holdout_pair_count = int(CONFIG['num_pairs'] * 0.10) # 10% for holdout\n",
    "\n",
    "train_examples = generate_training_pairs(train_df, train_pair_count, CONFIG, desc=\"Training\")\n",
    "eval_examples = generate_training_pairs(eval_df, eval_pair_count, CONFIG, desc=\"Evaluation\")\n",
    "holdout_examples = generate_training_pairs(holdout_df, holdout_pair_count, CONFIG, desc=\"Holdout\")\n",
    "\n",
    "log(f\"\\nğŸ“¦ Final Pair Counts:\")\n",
    "log(f\"   Train:   {len(train_examples):,}\")\n",
    "log(f\"   Eval:    {len(eval_examples):,}\")\n",
    "log(f\"   Holdout: {len(holdout_examples):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c4f9e",
   "metadata": {},
   "source": [
    "# 6. Model Training\n",
    "\n",
    "Train `all-mpnet-base-v2` with CosineSimilarityLoss on the generated pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5cd19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers.evaluation import SentenceEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "# --- Custom Evaluator ---\n",
    "class ITSMEvaluator(SentenceEvaluator):\n",
    "    \"\"\"Evaluator for ITSM ticket similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, examples, batch_size=16, name=\"\"):\n",
    "        self.examples = examples\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "        \n",
    "        self.texts1 = [ex.texts[0] for ex in examples]\n",
    "        self.texts2 = [ex.texts[1] for ex in examples]\n",
    "        self.labels = np.array([ex.label for ex in examples])\n",
    "        \n",
    "        self.csv_file = f\"{name}_eval_results.csv\"\n",
    "        self.csv_headers = [\"epoch\", \"steps\", \"spearman\", \"pearson\", \"roc_auc\", \"pr_auc\"]\n",
    "    \n",
    "    def __call__(self, model, output_path=None, epoch=-1, steps=-1):\n",
    "        model.eval()\n",
    "        \n",
    "        # Encode pairs\n",
    "        emb1 = model.encode(self.texts1, batch_size=self.batch_size, \n",
    "                          show_progress_bar=False, convert_to_numpy=True)\n",
    "        emb2 = model.encode(self.texts2, batch_size=self.batch_size, \n",
    "                          show_progress_bar=False, convert_to_numpy=True)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        scores = np.sum(emb1 * emb2, axis=1) / (\n",
    "            np.linalg.norm(emb1, axis=1) * np.linalg.norm(emb2, axis=1) + 1e-8\n",
    "        )\n",
    "        \n",
    "        # Compute metrics\n",
    "        spearman, _ = spearmanr(self.labels, scores)\n",
    "        pearson, _ = pearsonr(self.labels, scores)\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(self.labels, scores)\n",
    "            pr_auc = average_precision_score(self.labels, scores)\n",
    "        except ValueError:\n",
    "            roc_auc, pr_auc = 0.0, 0.0\n",
    "        \n",
    "        log(f\"  [{self.name}] Epoch {epoch}: Spearman={spearman:.4f}, ROC-AUC={roc_auc:.4f}, PR-AUC={pr_auc:.4f}\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_path:\n",
    "            csv_path = Path(output_path) / self.csv_file\n",
    "            if not csv_path.exists():\n",
    "                with open(csv_path, 'w') as f:\n",
    "                    f.write(','.join(self.csv_headers) + '\\n')\n",
    "            with open(csv_path, 'a') as f:\n",
    "                f.write(f\"{epoch},{steps},{spearman},{pearson},{roc_auc},{pr_auc}\\n\")\n",
    "        \n",
    "        return spearman  # Primary metric\n",
    "\n",
    "\n",
    "# --- Model Initialization ---\n",
    "def init_model(config, device):\n",
    "    \"\"\"Initialize model with GPU memory management.\"\"\"\n",
    "    # Clear GPU memory\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    log(f\"ğŸ”§ Loading model: {config['model_name']}\")\n",
    "    model = SentenceTransformer(config['model_name'], device=device)\n",
    "    model.max_seq_length = config['max_seq_length']\n",
    "    log(f\"âœ… Model loaded on {device}, max_seq_length={model.max_seq_length}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = init_model(CONFIG, DEVICE)\n",
    "\n",
    "# Filter to positive pairs only\n",
    "log(f\"ğŸ“Š Filtering to positive pairs for MultipleNegativesRankingLoss...\")\n",
    "train_positives = [ex for ex in train_examples if ex.label == 1.0]\n",
    "eval_positives = [ex for ex in eval_examples if ex.label == 1.0]\n",
    "\n",
    "log(f\"   Train positives: {len(train_positives):,} (was {len(train_examples):,} total)\")\n",
    "log(f\"   Eval positives:  {len(eval_positives):,} (was {len(eval_examples):,} total)\")\n",
    "\n",
    "# Create DataLoader with ONLY positives\n",
    "train_dataloader = DataLoader(\n",
    "    train_positives,  # Changed from train_examples\n",
    "    shuffle=True,\n",
    "    batch_size=CONFIG['batch_size'],  # Consider increasing to 16-32 for more in-batch negatives\n",
    "    num_workers=0,\n",
    "    pin_memory=(DEVICE in ['cuda', 'mps'])\n",
    ")\n",
    "\n",
    "# Use MultipleNegativesRankingLoss instead of CosineSimilarityLoss\n",
    "from sentence_transformers import losses\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "log(f\"ğŸ”§ Using MultipleNegativesRankingLoss (in-batch negatives)\")\n",
    "log(f\"   Effective negatives per sample: {CONFIG['batch_size'] - 1}\")\n",
    "# Evaluator\n",
    "evaluator = ITSMEvaluator(eval_examples, batch_size=CONFIG['batch_size'], name=\"eval\")\n",
    "\n",
    "log(f\"\\nğŸ“Š Training Setup:\")\n",
    "log(f\"   Batches per epoch: {len(train_dataloader)}\")\n",
    "log(f\"   Total training steps: {len(train_dataloader) * CONFIG['epochs']}\")\n",
    "log(f\"   Warmup steps: {int(len(train_dataloader) * CONFIG['epochs'] * CONFIG['warmup_ratio'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e2fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Execution ---\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "save_path = Path(CONFIG['output_dir']) / f\"real_servicenow_{timestamp}\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log(f\"\\nğŸš€ Starting Training...\")\n",
    "log(f\"   Output: {save_path}\")\n",
    "log(f\"   Epochs: {CONFIG['epochs']}\")\n",
    "log(f\"   Device: {DEVICE}\")\n",
    "\n",
    "# Calculate warmup steps\n",
    "total_steps = len(train_dataloader) * CONFIG['epochs']\n",
    "warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "eval_steps = max(100, len(train_dataloader) // 2)  # Evaluate twice per epoch\n",
    "\n",
    "try:\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        evaluator=evaluator,\n",
    "        epochs=CONFIG['epochs'],\n",
    "        warmup_steps=warmup_steps,\n",
    "        optimizer_params={'lr': CONFIG['lr']},\n",
    "        output_path=str(save_path),\n",
    "        evaluation_steps=eval_steps,\n",
    "        save_best_model=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    log(\"âœ… Training complete!\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        log(f\"âŒ OOM Error: {e}\")\n",
    "        log(\"ğŸ’¡ Try reducing batch_size or num_pairs\")\n",
    "        raise\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Reload best model\n",
    "log(\"\\nğŸ“Š Loading best model for final evaluation...\")\n",
    "best_model = SentenceTransformer(str(save_path), device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85294299",
   "metadata": {},
   "source": [
    "# 7. Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d094e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, precision_recall_curve, confusion_matrix,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    precision_score, recall_score, f1_score, accuracy_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def comprehensive_eval(examples, model, name=\"\"):\n",
    "    \"\"\"Run comprehensive evaluation on a set of pairs.\"\"\"\n",
    "    texts1 = [ex.texts[0] for ex in examples]\n",
    "    texts2 = [ex.texts[1] for ex in examples]\n",
    "    labels = np.array([ex.label for ex in examples])\n",
    "    \n",
    "    # Encode\n",
    "    log(f\"â³ Encoding {len(examples)} pairs for {name}...\")\n",
    "    emb1 = model.encode(texts1, batch_size=CONFIG['batch_size'], \n",
    "                       show_progress_bar=True, convert_to_numpy=True)\n",
    "    emb2 = model.encode(texts2, batch_size=CONFIG['batch_size'], \n",
    "                       show_progress_bar=True, convert_to_numpy=True)\n",
    "    \n",
    "    # Cosine similarity\n",
    "    scores = np.sum(emb1 * emb2, axis=1) / (\n",
    "        np.linalg.norm(emb1, axis=1) * np.linalg.norm(emb2, axis=1) + 1e-8\n",
    "    )\n",
    "    \n",
    "    # Metrics\n",
    "    spearman, _ = spearmanr(labels, scores)\n",
    "    pearson, _ = pearsonr(labels, scores)\n",
    "    roc_auc = roc_auc_score(labels, scores)\n",
    "    pr_auc = average_precision_score(labels, scores)\n",
    "    \n",
    "    # Find best threshold\n",
    "    fpr, tpr, roc_thresholds = roc_curve(labels, scores)\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(labels, scores)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = pr_thresholds[best_idx-1] if 0 < best_idx < len(pr_thresholds)+1 else 0.5\n",
    "    \n",
    "    # Metrics at best threshold\n",
    "    preds = (scores >= best_threshold).astype(int)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds)\n",
    "    rec = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "    log(f\"\\nğŸ“Š {name} Results:\")\n",
    "    log(f\"   Spearman:  {spearman:.4f}\")\n",
    "    log(f\"   Pearson:   {pearson:.4f}\")\n",
    "    log(f\"   ROC-AUC:   {roc_auc:.4f}\")\n",
    "    log(f\"   PR-AUC:    {pr_auc:.4f}\")\n",
    "    log(f\"   Best Threshold: {best_threshold:.3f}\")\n",
    "    log(f\"   F1 @ best: {f1:.4f}\")\n",
    "    log(f\"   Precision: {prec:.4f}\")\n",
    "    log(f\"   Recall:    {rec:.4f}\")\n",
    "    log(f\"   Accuracy:  {acc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'labels': labels, 'scores': scores,\n",
    "        'spearman': spearman, 'pearson': pearson,\n",
    "        'roc_auc': roc_auc, 'pr_auc': pr_auc,\n",
    "        'fpr': fpr, 'tpr': tpr,\n",
    "        'precision': precision, 'recall': recall,\n",
    "        'best_threshold': best_threshold,\n",
    "        'f1': f1, 'prec': prec, 'rec': rec, 'acc': acc,\n",
    "        'cm': cm\n",
    "    }\n",
    "\n",
    "# Evaluate on all sets\n",
    "log(\"=\"*60)\n",
    "log(\"ğŸ“ˆ FINAL EVALUATION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "eval_results = comprehensive_eval(eval_examples, best_model, \"Eval Set\")\n",
    "holdout_results = comprehensive_eval(holdout_examples, best_model, \"Holdout Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48806de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# ROC Curves\n",
    "axes[0,0].plot(eval_results['fpr'], eval_results['tpr'], \n",
    "               label=f\"Eval ROC-AUC = {eval_results['roc_auc']:.3f}\")\n",
    "axes[0,0].plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "axes[0,0].set_title('ROC Curve (Eval)')\n",
    "axes[0,0].set_xlabel('False Positive Rate')\n",
    "axes[0,0].set_ylabel('True Positive Rate')\n",
    "axes[0,0].legend()\n",
    "\n",
    "axes[0,1].plot(holdout_results['fpr'], holdout_results['tpr'], \n",
    "               label=f\"Holdout ROC-AUC = {holdout_results['roc_auc']:.3f}\", color='orange')\n",
    "axes[0,1].plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "axes[0,1].set_title('ROC Curve (Holdout)')\n",
    "axes[0,1].set_xlabel('False Positive Rate')\n",
    "axes[0,1].set_ylabel('True Positive Rate')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# PR Curves\n",
    "axes[1,0].plot(eval_results['recall'], eval_results['precision'], \n",
    "               label=f\"Eval PR-AUC = {eval_results['pr_auc']:.3f}\")\n",
    "axes[1,0].scatter([eval_results['rec']], [eval_results['prec']], \n",
    "                  color='red', s=100, zorder=5,\n",
    "                  label=f\"Best F1={eval_results['f1']:.3f}\")\n",
    "axes[1,0].set_title('Precision-Recall (Eval)')\n",
    "axes[1,0].set_xlabel('Recall')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].legend()\n",
    "\n",
    "axes[1,1].plot(holdout_results['recall'], holdout_results['precision'], \n",
    "               label=f\"Holdout PR-AUC = {holdout_results['pr_auc']:.3f}\", color='orange')\n",
    "axes[1,1].scatter([holdout_results['rec']], [holdout_results['prec']], \n",
    "                  color='red', s=100, zorder=5,\n",
    "                  label=f\"Best F1={holdout_results['f1']:.3f}\")\n",
    "axes[1,1].set_title('Precision-Recall (Holdout)')\n",
    "axes[1,1].set_xlabel('Recall')\n",
    "axes[1,1].set_ylabel('Precision')\n",
    "axes[1,1].legend()\n",
    "\n",
    "# Score Distributions\n",
    "for idx, (results, name) in enumerate([(eval_results, 'Eval'), (holdout_results, 'Holdout')]):\n",
    "    ax = axes[idx, 2]\n",
    "    neg_scores = results['scores'][results['labels'] == 0]\n",
    "    pos_scores = results['scores'][results['labels'] == 1]\n",
    "    \n",
    "    ax.hist(neg_scores, bins=30, alpha=0.6, label='Negative (0)', color='blue')\n",
    "    ax.hist(pos_scores, bins=30, alpha=0.6, label='Positive (1)', color='orange')\n",
    "    ax.axvline(results['best_threshold'], color='red', linestyle='--', \n",
    "               label=f\"Threshold={results['best_threshold']:.3f}\")\n",
    "    ax.set_title(f'Score Distribution ({name})')\n",
    "    ax.set_xlabel('Cosine Similarity')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path / 'evaluation_plots.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "log(f\"\\nğŸ“Š Plots saved to {save_path / 'evaluation_plots.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f0d5d",
   "metadata": {},
   "source": [
    "# 8. Adversarial Diagnostic\n",
    "\n",
    "**Critical validation**: Test if the model learned semantic content or is exploiting category shortcuts.\n",
    "\n",
    "- **Hard Positives**: Cross-category pairs with HIGH content similarity\n",
    "- **Hard Negatives**: Same-category pairs with LOW content similarity\n",
    "\n",
    "**Pass Criteria**: ROC-AUC â‰¥ 0.70 AND F1 â‰¥ 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94893294",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"=\"*60)\n",
    "log(\"ğŸ”¬ ADVERSARIAL DIAGNOSTIC: Testing Category Leakage\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Use holdout data for adversarial test (completely unseen)\n",
    "diag_df = holdout_df.reset_index(drop=True)\n",
    "\n",
    "# Build content-only text (remove category context to test pure semantic understanding)\n",
    "diag_df['content_only'] = diag_df['Description'].str.strip()\n",
    "\n",
    "# Build TF-IDF on content-only text\n",
    "log(\"â³ Building TF-IDF for adversarial pair mining...\")\n",
    "diag_tfidf = TFIDFSimilarityCalculator(diag_df['content_only'].tolist(), max_features=10000)\n",
    "\n",
    "# Generate adversarial pairs\n",
    "hard_positives = []  # Cross-category, high TF-IDF\n",
    "hard_negatives = []  # Same-category, low TF-IDF\n",
    "\n",
    "target_each = 300\n",
    "attempts, max_attempts = 0, 100000\n",
    "\n",
    "log(\"â³ Mining adversarial pairs...\")\n",
    "pbar = tqdm(total=target_each * 2, desc=\"Adversarial pairs\")\n",
    "\n",
    "while (len(hard_positives) < target_each or len(hard_negatives) < target_each) and attempts < max_attempts:\n",
    "    attempts += 1\n",
    "    i1, i2 = random.sample(range(len(diag_df)), 2)\n",
    "    \n",
    "    cat1 = diag_df.at[i1, 'category_id']\n",
    "    cat2 = diag_df.at[i2, 'category_id']\n",
    "    tfidf_sim = diag_tfidf.similarity(i1, i2)\n",
    "    \n",
    "    # Hard Positive: DIFFERENT category but HIGH content similarity\n",
    "    if len(hard_positives) < target_each and cat1 != cat2 and tfidf_sim > 0.4:\n",
    "        hard_positives.append(InputExample(\n",
    "            texts=[diag_df.at[i1, 'content_only'], diag_df.at[i2, 'content_only']],\n",
    "            label=1.0\n",
    "        ))\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Hard Negative: SAME category but LOW content similarity\n",
    "    if len(hard_negatives) < target_each and cat1 == cat2 and tfidf_sim < 0.15:\n",
    "        hard_negatives.append(InputExample(\n",
    "            texts=[diag_df.at[i1, 'content_only'], diag_df.at[i2, 'content_only']],\n",
    "            label=0.0\n",
    "        ))\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "log(f\"âœ… Generated {len(hard_positives)} hard positives, {len(hard_negatives)} hard negatives\")\n",
    "\n",
    "# Evaluate on adversarial pairs\n",
    "adversarial_examples = hard_positives + hard_negatives\n",
    "if len(adversarial_examples) >= 100:\n",
    "    adv_results = comprehensive_eval(adversarial_examples, best_model, \"Adversarial\")\n",
    "    \n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"ğŸ¯ ADVERSARIAL DIAGNOSTIC RESULTS\")\n",
    "    log(\"=\"*60)\n",
    "    log(f\"Standard Eval ROC-AUC:     {eval_results['roc_auc']:.4f}\")\n",
    "    log(f\"Adversarial ROC-AUC:       {adv_results['roc_auc']:.4f}\")\n",
    "    log(f\"Adversarial F1 @ best:     {adv_results['f1']:.4f}\")\n",
    "    \n",
    "    # Verdict\n",
    "    if adv_results['roc_auc'] >= 0.70 and adv_results['f1'] >= 0.65:\n",
    "        log(\"\\nâœ… VERDICT: Model is ROBUST to category shortcuts!\")\n",
    "        log(\"   â†’ Performance holds when categories don't predict similarity\")\n",
    "        log(\"   â†’ Model learned semantic content understanding\")\n",
    "        DIAGNOSTIC_PASSED = True\n",
    "    else:\n",
    "        log(\"\\nâš ï¸ VERDICT: Model may be exploiting category shortcuts\")\n",
    "        log(\"   â†’ Consider increasing hard negatives ratio\")\n",
    "        log(\"   â†’ Or remove category context from training text\")\n",
    "        DIAGNOSTIC_PASSED = False\n",
    "else:\n",
    "    log(\"âš ï¸ Could not generate enough adversarial pairs\")\n",
    "    DIAGNOSTIC_PASSED = None\n",
    "\n",
    "# Cleanup\n",
    "del diag_tfidf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd02e18",
   "metadata": {},
   "source": [
    "# 9. Save Training Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4145588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# TRAINING METADATA EXPORT\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def save_training_metadata(output_dir: str, config: dict, metrics: dict, \n",
    "                          data_stats: dict, adversarial_results: dict = None):\n",
    "    \"\"\"\n",
    "    Save comprehensive training metadata for reproducibility.\n",
    "    \n",
    "    Following project convention: all model outputs include training_metadata.json\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"training_timestamp\": datetime.now().isoformat(),\n",
    "        \"model_name\": config.get('model_name', 'all-mpnet-finetuned'),\n",
    "        \"base_model\": config['base_model'],\n",
    "        \n",
    "        # Hyperparameters\n",
    "        \"hyperparameters\": {\n",
    "            \"epochs\": config['epochs'],\n",
    "            \"batch_size\": config['batch_size'],\n",
    "            \"learning_rate\": config['learning_rate'],\n",
    "            \"warmup_ratio\": config['warmup_ratio'],\n",
    "            \"weight_decay\": config['weight_decay'],\n",
    "            \"loss_function\": \"MultipleNegativesRankingLoss\"\n",
    "        },\n",
    "        \n",
    "        # Data configuration\n",
    "        \"data_config\": {\n",
    "            \"data_path\": config['data_path'],\n",
    "            \"num_pairs\": config['num_pairs'],\n",
    "            \"min_text_length\": config['min_text_length'],\n",
    "            \"train_ratio\": config['train_ratio'],\n",
    "            \"eval_ratio\": config['eval_ratio'],\n",
    "            \"positive_threshold\": config['positive_threshold'],\n",
    "            \"negative_threshold\": config['negative_threshold'],\n",
    "            \"pair_ratios\": {\n",
    "                \"positives\": 0.35,\n",
    "                \"hard_negatives\": 0.35,\n",
    "                \"easy_negatives\": 0.30\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Data statistics\n",
    "        \"data_statistics\": data_stats,\n",
    "        \n",
    "        # TF-IDF configuration\n",
    "        \"tfidf_config\": {\n",
    "            \"max_features\": config['tfidf_max_features'],\n",
    "            \"ngram_range\": [1, 2],\n",
    "            \"min_df\": 2,\n",
    "            \"max_df\": 0.95\n",
    "        },\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        \"evaluation_metrics\": metrics,\n",
    "        \n",
    "        # Adversarial diagnostic results\n",
    "        \"adversarial_diagnostic\": adversarial_results,\n",
    "        \n",
    "        # Environment info\n",
    "        \"environment\": {\n",
    "            \"device\": config['device'],\n",
    "            \"random_seed\": config['seed'],\n",
    "            \"python_version\": __import__('sys').version,\n",
    "            \"torch_version\": torch.__version__,\n",
    "            \"transformers_version\": __import__('transformers').__version__,\n",
    "            \"sentence_transformers_version\": __import__('sentence_transformers').__version__\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(output_dir, \"training_metadata.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    log(f\"ğŸ“ Training metadata saved to: {metadata_path}\")\n",
    "    return metadata_path\n",
    "\n",
    "# Collect data statistics\n",
    "data_stats = {\n",
    "    \"total_records\": len(df),\n",
    "    \"train_size\": len(train_df),\n",
    "    \"eval_size\": len(eval_df),\n",
    "    \"holdout_size\": len(holdout_df),\n",
    "    \"unique_categories\": df['Category'].nunique() if 'Category' in df.columns else None,\n",
    "    \"unique_subcategories\": df['Subcategory'].nunique() if 'Subcategory' in df.columns else None,\n",
    "    \"unique_assignment_groups\": df['Assignment group'].nunique() if 'Assignment group' in df.columns else None,\n",
    "    \"avg_text_length\": df['text'].str.len().mean(),\n",
    "    \"tfidf_vocabulary_size\": tfidf.vocabulary_.__len__() if 'tfidf' in dir() else None\n",
    "}\n",
    "\n",
    "# Collect metrics (if evaluation was run)\n",
    "try:\n",
    "    eval_metrics = {\n",
    "        \"holdout_roc_auc\": float(roc_auc_holdout) if 'roc_auc_holdout' in dir() else None,\n",
    "        \"holdout_pr_auc\": float(pr_auc_holdout) if 'pr_auc_holdout' in dir() else None,\n",
    "        \"holdout_spearman\": float(spearman_holdout) if 'spearman_holdout' in dir() else None,\n",
    "        \"holdout_pearson\": float(pearson_holdout) if 'pearson_holdout' in dir() else None,\n",
    "        \"holdout_f1\": float(f1_holdout) if 'f1_holdout' in dir() else None,\n",
    "    }\n",
    "except:\n",
    "    eval_metrics = {\"note\": \"Run evaluation cells first\"}\n",
    "\n",
    "# Collect adversarial results (if diagnostic was run)\n",
    "try:\n",
    "    adversarial_results = {\n",
    "        \"roc_auc\": float(adv_roc_auc) if 'adv_roc_auc' in dir() else None,\n",
    "        \"f1_score\": float(adv_f1) if 'adv_f1' in dir() else None,\n",
    "        \"pass_status\": adv_pass if 'adv_pass' in dir() else None\n",
    "    }\n",
    "except:\n",
    "    adversarial_results = {\"note\": \"Run adversarial diagnostic first\"}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = save_training_metadata(\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    config=CONFIG,\n",
    "    metrics=eval_metrics,\n",
    "    data_stats=data_stats,\n",
    "    adversarial_results=adversarial_results\n",
    ")\n",
    "\n",
    "log(\"âœ… Training pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a7ca0",
   "metadata": {},
   "source": [
    "# 10. Usage Examples & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc44fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# USAGE EXAMPLES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def demonstrate_model_usage():\n",
    "    \"\"\"\n",
    "    Demonstrate how to use the fine-tuned model for ITSM ticket similarity.\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    # Load the fine-tuned model\n",
    "    model_path = CONFIG['output_dir']\n",
    "    model = SentenceTransformer(model_path)\n",
    "    log(f\"ğŸ“¦ Loaded model from: {model_path}\")\n",
    "    \n",
    "    # Example tickets (using real-world patterns)\n",
    "    example_tickets = [\n",
    "        \"User cannot login to SAP system. Error message: authentication failed. Tried resetting password but issue persists.\",\n",
    "        \"SAP login issue - getting access denied error when trying to connect to production system.\",\n",
    "        \"Outlook keeps crashing when opening large attachments. Have tried restarting but problem continues.\",\n",
    "        \"Email client crashes randomly. Users report Outlook freezing when opening emails with attachments.\",\n",
    "        \"Request to provision new laptop for incoming employee starting next Monday.\",\n",
    "    ]\n",
    "    \n",
    "    # Encode all tickets\n",
    "    embeddings = model.encode(example_tickets, show_progress_bar=False)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Display results\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"SIMILARITY MATRIX DEMO\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nTickets:\")\n",
    "    for i, ticket in enumerate(example_tickets):\n",
    "        print(f\"  [{i}] {ticket[:80]}...\")\n",
    "    \n",
    "    print(\"\\nSimilarity Matrix:\")\n",
    "    print(\"     \", end=\"\")\n",
    "    for i in range(len(example_tickets)):\n",
    "        print(f\"  [{i}]  \", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for i, row in enumerate(sim_matrix):\n",
    "        print(f\"[{i}]  \", end=\"\")\n",
    "        for val in row:\n",
    "            print(f\" {val:.3f} \", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Find similar ticket pairs\n",
    "    log(\"\\n\" + \"-\"*60)\n",
    "    log(\"HIGH SIMILARITY PAIRS (> 0.7):\")\n",
    "    for i in range(len(example_tickets)):\n",
    "        for j in range(i+1, len(example_tickets)):\n",
    "            if sim_matrix[i][j] > 0.7:\n",
    "                print(f\"  Tickets [{i}] & [{j}]: {sim_matrix[i][j]:.3f}\")\n",
    "                print(f\"    [{i}]: {example_tickets[i][:60]}...\")\n",
    "                print(f\"    [{j}]: {example_tickets[j][:60]}...\")\n",
    "                print()\n",
    "    \n",
    "    return model, embeddings\n",
    "\n",
    "# Run demonstration\n",
    "demo_model, demo_embeddings = demonstrate_model_usage()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# INFERENCE FUNCTION FOR PRODUCTION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def find_similar_tickets(query_text: str, ticket_corpus: list, model, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Find the most similar tickets to a query.\n",
    "    \n",
    "    Args:\n",
    "        query_text: The new ticket description\n",
    "        ticket_corpus: List of existing ticket descriptions\n",
    "        model: The loaded SentenceTransformer model\n",
    "        top_k: Number of similar tickets to return\n",
    "        \n",
    "    Returns:\n",
    "        List of (index, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = model.encode([query_text], show_progress_bar=False)\n",
    "    \n",
    "    # Encode corpus (in production, pre-compute and cache these)\n",
    "    corpus_embeddings = model.encode(ticket_corpus, show_progress_bar=False)\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = cosine_similarity(query_embedding, corpus_embeddings)[0]\n",
    "    \n",
    "    # Get top-k\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    results = [(idx, similarities[idx]) for idx in top_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "log(\"\\nâœ… Usage examples complete!\")\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"NEXT STEPS:\")\n",
    "log(\"=\"*60)\n",
    "print(\"\"\"\n",
    "1. PRODUCTION DEPLOYMENT:\n",
    "   - Copy model from: {output_dir}\n",
    "   - Load with: SentenceTransformer('{output_dir}')\n",
    "   - Pre-compute embeddings for existing tickets\n",
    "   \n",
    "2. PERFORMANCE TUNING:\n",
    "   - If metrics are too low, try:\n",
    "     * Increase epochs (5 â†’ 8-10)\n",
    "     * Increase num_pairs (50K â†’ 100K)\n",
    "     * Lower positive_threshold (0.4 â†’ 0.35)\n",
    "   \n",
    "3. DATA QUALITY:\n",
    "   - Review low-confidence predictions\n",
    "   - Add more training data for rare categories\n",
    "   \n",
    "4. MONITORING:\n",
    "   - Track inference latency\n",
    "   - Monitor similarity distribution drift\n",
    "   - Retrain monthly with new tickets\n",
    "   \n",
    "5. INTEGRATION:\n",
    "   - ServiceNow: Use REST API to query similar tickets\n",
    "   - Chatbot: Real-time duplicate detection\n",
    "   - Dashboard: Ticket clustering visualization\n",
    "\"\"\".format(output_dir=CONFIG['output_dir']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

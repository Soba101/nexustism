{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b616e298",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "DATA_DIR = Path('data_new')\n",
    "MODELS_DIR = Path('models')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_DIR / 'SNow_incident_ticket_data.csv')\n",
    "print(f\"Loaded {len(df)} incidents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate combined text (same logic as evaluate_model.ipynb)\n",
    "def create_combined_text(row):\n",
    "    text_parts = []\n",
    "    for col in ['Number', 'Description', 'User input', 'Resolution notes']:\n",
    "        if col in row.index:\n",
    "            value = str(row.get(col, '')).strip() if pd.notna(row.get(col)) else ''\n",
    "            if value and value.lower() != 'nan':\n",
    "                text_parts.append(value)\n",
    "    return ' '.join(text_parts) if text_parts else ''\n",
    "\n",
    "df['combined_text'] = df.apply(create_combined_text, axis=1)\n",
    "df['combined_text'] = df['combined_text'].astype(str)\n",
    "df = df[df['combined_text'].str.len() > 10].reset_index(drop=True)\n",
    "\n",
    "print(f\"After filtering: {len(df)} valid incidents\")\n",
    "print(f\"\\nData columns: {df.columns.tolist()}\")\n",
    "print(f\"Has 'Category' column: {'Category' in df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9029e25",
   "metadata": {},
   "source": [
    "## 2. Generate Test Pairs (Same as evaluate_model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12812cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine_similarity\n",
    "\n",
    "def generate_test_pairs(df: pd.DataFrame, \n",
    "                       num_positives: int = 500,\n",
    "                       num_easy_negatives: int = 500,\n",
    "                       num_hard_negatives: int = 500,\n",
    "                       random_state: int = 42) -> Tuple[List[str], List[str], List[int]]:\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    texts1, texts2, labels = [], [], []\n",
    "    has_categories = 'Category' in df.columns\n",
    "    \n",
    "    if has_categories:\n",
    "        print(\"Using category-based pair generation\")\n",
    "        categories = df['Category'].dropna().unique()\n",
    "        print(f\"Found {len(categories)} categories\")\n",
    "        \n",
    "        # Positive pairs - same category\n",
    "        for _ in range(num_positives):\n",
    "            cat = np.random.choice(categories)\n",
    "            cat_incidents = df[df['Category'] == cat]\n",
    "            if len(cat_incidents) >= 2:\n",
    "                idx1, idx2 = np.random.choice(cat_incidents.index, size=2, replace=False)\n",
    "                texts1.append(df.loc[idx1, 'combined_text'])\n",
    "                texts2.append(df.loc[idx2, 'combined_text'])\n",
    "                labels.append(1)\n",
    "        \n",
    "        # Easy negatives - different categories\n",
    "        for _ in range(num_easy_negatives):\n",
    "            cat1, cat2 = np.random.choice(categories, size=2, replace=False)\n",
    "            incidents1 = df[df['Category'] == cat1]\n",
    "            incidents2 = df[df['Category'] == cat2]\n",
    "            if len(incidents1) > 0 and len(incidents2) > 0:\n",
    "                idx1 = np.random.choice(incidents1.index)\n",
    "                idx2 = np.random.choice(incidents2.index)\n",
    "                texts1.append(df.loc[idx1, 'combined_text'])\n",
    "                texts2.append(df.loc[idx2, 'combined_text'])\n",
    "                labels.append(0)\n",
    "    else:\n",
    "        print(\"No category column found - using random pair generation\")\n",
    "        for _ in range(num_positives):\n",
    "            idx1, idx2 = np.random.choice(len(df), size=2, replace=False)\n",
    "            texts1.append(df.loc[idx1, 'combined_text'])\n",
    "            texts2.append(df.loc[idx2, 'combined_text'])\n",
    "            labels.append(1)\n",
    "        for _ in range(num_easy_negatives):\n",
    "            idx1, idx2 = np.random.choice(len(df), size=2, replace=False)\n",
    "            texts1.append(df.loc[idx1, 'combined_text'])\n",
    "            texts2.append(df.loc[idx2, 'combined_text'])\n",
    "            labels.append(0)\n",
    "    \n",
    "    # Hard negatives\n",
    "    print(\"\\nGenerating hard negatives with TF-IDF...\")\n",
    "    sample_size = min(2000, len(df))\n",
    "    sample_df = df.sample(n=sample_size, random_state=random_state)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(sample_df['combined_text'])\n",
    "    \n",
    "    hard_neg_count = 0\n",
    "    attempts = 0\n",
    "    max_attempts = num_hard_negatives * 10\n",
    "    \n",
    "    while hard_neg_count < num_hard_negatives and attempts < max_attempts:\n",
    "        idx1 = np.random.randint(0, len(sample_df))\n",
    "        similarities = sklearn_cosine_similarity(tfidf_matrix[idx1:idx1+1], tfidf_matrix).flatten()\n",
    "        hard_neg_candidates = np.where((similarities > 0.3) & (similarities < 0.6))[0]\n",
    "        hard_neg_candidates = hard_neg_candidates[hard_neg_candidates != idx1]\n",
    "        \n",
    "        if len(hard_neg_candidates) > 0:\n",
    "            idx2 = np.random.choice(hard_neg_candidates)\n",
    "            if has_categories:\n",
    "                cat1 = sample_df.iloc[idx1].get('Category')\n",
    "                cat2 = sample_df.iloc[idx2].get('Category')\n",
    "                if cat1 == cat2:\n",
    "                    attempts += 1\n",
    "                    continue\n",
    "            \n",
    "            texts1.append(sample_df.iloc[idx1]['combined_text'])\n",
    "            texts2.append(sample_df.iloc[idx2]['combined_text'])\n",
    "            labels.append(0)\n",
    "            hard_neg_count += 1\n",
    "        \n",
    "        attempts += 1\n",
    "    \n",
    "    print(f\"Generated {hard_neg_count} hard negatives ({attempts} attempts)\")\n",
    "    print(f\"\\nTotal test pairs: {len(labels)}\")\n",
    "    print(f\"  Positive pairs: {sum(labels)}\")\n",
    "    print(f\"  Negative pairs: {len(labels) - sum(labels)}\")\n",
    "    \n",
    "    return texts1, texts2, labels\n",
    "\n",
    "# Generate pairs\n",
    "test_texts1, test_texts2, test_labels = generate_test_pairs(df, random_state=42)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51bb4f",
   "metadata": {},
   "source": [
    "## 3. Inspect Test Pair Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d8050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of positive pairs\n",
    "print(\"=\"*80)\n",
    "print(\"POSITIVE PAIR EXAMPLES (Label=1 - Should be similar)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pos_indices = np.where(test_labels == 1)[0]\n",
    "for i in range(min(3, len(pos_indices))):\n",
    "    idx = pos_indices[i]\n",
    "    print(f\"\\n--- Positive Pair {i+1} ---\")\n",
    "    print(f\"Text 1 ({len(test_texts1[idx])} chars):\")\n",
    "    print(test_texts1[idx][:200] + \"...\" if len(test_texts1[idx]) > 200 else test_texts1[idx])\n",
    "    print(f\"\\nText 2 ({len(test_texts2[idx])} chars):\")\n",
    "    print(test_texts2[idx][:200] + \"...\" if len(test_texts2[idx]) > 200 else test_texts2[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f792f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of negative pairs\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEGATIVE PAIR EXAMPLES (Label=0 - Should be different)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "neg_indices = np.where(test_labels == 0)[0]\n",
    "for i in range(min(3, len(neg_indices))):\n",
    "    idx = neg_indices[i]\n",
    "    print(f\"\\n--- Negative Pair {i+1} ---\")\n",
    "    print(f\"Text 1 ({len(test_texts1[idx])} chars):\")\n",
    "    print(test_texts1[idx][:200] + \"...\" if len(test_texts1[idx]) > 200 else test_texts1[idx])\n",
    "    print(f\"\\nText 2 ({len(test_texts2[idx])} chars):\")\n",
    "    print(test_texts2[idx][:200] + \"...\" if len(test_texts2[idx]) > 200 else test_texts2[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e0295",
   "metadata": {},
   "source": [
    "## 4. Calculate Baseline Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model\n",
    "print(\"Loading baseline model...\")\n",
    "baseline_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "# Encode test pairs\n",
    "print(\"Encoding text pairs...\")\n",
    "embeddings1_baseline = baseline_model.encode(test_texts1, batch_size=64, show_progress_bar=True, convert_to_numpy=True)\n",
    "embeddings2_baseline = baseline_model.encode(test_texts2, batch_size=64, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "# Compute similarity scores\n",
    "baseline_scores = np.sum(embeddings1_baseline * embeddings2_baseline, axis=1) / (\n",
    "    np.linalg.norm(embeddings1_baseline, axis=1) * np.linalg.norm(embeddings2_baseline, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"\\nBaseline scores: min={baseline_scores.min():.4f}, max={baseline_scores.max():.4f}, mean={baseline_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f304827",
   "metadata": {},
   "source": [
    "## 5. Analyze Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d259493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate by label\n",
    "pos_scores_baseline = baseline_scores[test_labels == 1]\n",
    "neg_scores_baseline = baseline_scores[test_labels == 0]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BASELINE MODEL SCORE DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPositive pairs (label=1):\")\n",
    "print(f\"  Mean: {pos_scores_baseline.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(pos_scores_baseline):.4f}\")\n",
    "print(f\"  Std: {pos_scores_baseline.std():.4f}\")\n",
    "print(f\"  Min-Max: [{pos_scores_baseline.min():.4f}, {pos_scores_baseline.max():.4f}]\")\n",
    "\n",
    "print(f\"\\nNegative pairs (label=0):\")\n",
    "print(f\"  Mean: {neg_scores_baseline.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(neg_scores_baseline):.4f}\")\n",
    "print(f\"  Std: {neg_scores_baseline.std():.4f}\")\n",
    "print(f\"  Min-Max: [{neg_scores_baseline.min():.4f}, {neg_scores_baseline.max():.4f}]\")\n",
    "\n",
    "# Overlap analysis\n",
    "overlap = np.sum((pos_scores_baseline < neg_scores_baseline.max()) & (pos_scores_baseline > neg_scores_baseline.min())) / len(pos_scores_baseline)\n",
    "print(f\"\\nScore overlap (% of positives within negative score range): {overlap*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f261f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histograms\n",
    "ax = axes[0]\n",
    "ax.hist(neg_scores_baseline, bins=50, alpha=0.6, label='Negative (label=0)', color='red', edgecolor='black')\n",
    "ax.hist(pos_scores_baseline, bins=50, alpha=0.6, label='Positive (label=1)', color='green', edgecolor='black')\n",
    "ax.set_xlabel('Cosine Similarity Score')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Baseline Model: Score Distributions')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plots\n",
    "ax = axes[1]\n",
    "data_to_plot = [neg_scores_baseline, pos_scores_baseline]\n",
    "bp = ax.boxplot(data_to_plot, labels=['Negative (0)', 'Positive (1)'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('red')\n",
    "bp['boxes'][0].set_alpha(0.6)\n",
    "bp['boxes'][1].set_facecolor('green')\n",
    "bp['boxes'][1].set_alpha(0.6)\n",
    "ax.set_ylabel('Cosine Similarity Score')\n",
    "ax.set_title('Baseline Model: Score Distributions (Box Plot)')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSeparability: Mean positive ({pos_scores_baseline.mean():.4f}) vs Mean negative ({neg_scores_baseline.mean():.4f})\")\n",
    "print(f\"Difference: {pos_scores_baseline.mean() - neg_scores_baseline.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131618d3",
   "metadata": {},
   "source": [
    "## 6. Analyze Hard Negatives - Are They Actually Hard?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14797a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify which negatives are \"hard\" (high TF-IDF similarity)\n",
    "# We'll use a separate baseline encoding to compute similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform([t for t in test_texts1 + test_texts2])\n",
    "\n",
    "# For simplicity, estimate which negatives are hard by checking text similarity\n",
    "print(\"Analyzing negative pairs for semantic overlap...\\n\")\n",
    "\n",
    "neg_indices = np.where(test_labels == 0)[0]\n",
    "neg_example_similarities = []\n",
    "\n",
    "for idx in neg_indices[:100]:  # Sample first 100 negatives\n",
    "    # Simple word overlap ratio\n",
    "    words1 = set(test_texts1[idx].lower().split())\n",
    "    words2 = set(test_texts2[idx].lower().split())\n",
    "    overlap = len(words1 & words2) / (len(words1 | words2) + 1e-10)\n",
    "    neg_example_similarities.append(overlap)\n",
    "\n",
    "print(f\"Negative pair word overlap (sample of 100):\")\n",
    "print(f\"  Mean: {np.mean(neg_example_similarities):.4f}\")\n",
    "print(f\"  Median: {np.median(neg_example_similarities):.4f}\")\n",
    "print(f\"  Max: {np.max(neg_example_similarities):.4f}\")\n",
    "print(f\"\\nThis tells us if 'different category' pairs actually share significant vocabulary...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd96ef0",
   "metadata": {},
   "source": [
    "## 7. Check for Label Noise in Positive Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da077c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For positive pairs labeled as \"same category\", check if they're actually similar\n",
    "print(\"Analyzing positive pairs for semantic similarity...\\n\")\n",
    "\n",
    "pos_indices = np.where(test_labels == 1)[0]\n",
    "pos_example_similarities = []\n",
    "\n",
    "for idx in pos_indices[:100]:  # Sample first 100 positives\n",
    "    words1 = set(test_texts1[idx].lower().split())\n",
    "    words2 = set(test_texts2[idx].lower().split())\n",
    "    overlap = len(words1 & words2) / (len(words1 | words2) + 1e-10)\n",
    "    pos_example_similarities.append(overlap)\n",
    "\n",
    "print(f\"Positive pair word overlap (sample of 100):\")\n",
    "print(f\"  Mean: {np.mean(pos_example_similarities):.4f}\")\n",
    "print(f\"  Median: {np.median(pos_example_similarities):.4f}\")\n",
    "print(f\"  Min: {np.min(pos_example_similarities):.4f}\")\n",
    "print(f\"\\nProblem detected if mean overlap is similar to negative pairs!\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Positive pairs mean overlap: {np.mean(pos_example_similarities):.4f}\")\n",
    "print(f\"  Negative pairs mean overlap: {np.mean(neg_example_similarities):.4f}\")\n",
    "print(f\"  Difference: {np.mean(pos_example_similarities) - np.mean(neg_example_similarities):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab47ab",
   "metadata": {},
   "source": [
    "## 8. Visualize Pair Quality Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134140de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Positive pairs\n",
    "ax = axes[0]\n",
    "ax.hist(pos_example_similarities, bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Word Overlap Ratio')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Positive Pairs: Word Overlap Distribution\\n(Same Category - Should Be High)')\n",
    "ax.axvline(np.mean(pos_example_similarities), color='darkgreen', linestyle='--', linewidth=2, label=f'Mean: {np.mean(pos_example_similarities):.3f}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Negative pairs\n",
    "ax = axes[1]\n",
    "ax.hist(neg_example_similarities, bins=30, color='red', alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Word Overlap Ratio')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Negative Pairs: Word Overlap Distribution\\n(Different Category - Should Be Low)')\n",
    "ax.axvline(np.mean(neg_example_similarities), color='darkred', linestyle='--', linewidth=2, label=f'Mean: {np.mean(neg_example_similarities):.3f}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b34d4",
   "metadata": {},
   "source": [
    "## 9. Diagnose: Key Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSIS: Why Fine-Tuned Models Underperform\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check 1: Score separability\n",
    "separability = pos_scores_baseline.mean() - neg_scores_baseline.mean()\n",
    "print(f\"\\n1. BASELINE MODEL SEPARABILITY\")\n",
    "print(f\"   Positive mean: {pos_scores_baseline.mean():.4f}\")\n",
    "print(f\"   Negative mean: {neg_scores_baseline.mean():.4f}\")\n",
    "print(f\"   Difference: {separability:.4f}\")\n",
    "if separability < 0.1:\n",
    "    print(f\"   ⚠️  WARNING: Poor separability! Scores barely different.\")\n",
    "else:\n",
    "    print(f\"   ✓ Good separability\")\n",
    "\n",
    "# Check 2: Overlap\n",
    "print(f\"\\n2. SCORE DISTRIBUTION OVERLAP\")\n",
    "print(f\"   % of positive scores < mean negative score: {(pos_scores_baseline < neg_scores_baseline.mean()).sum()/len(pos_scores_baseline)*100:.1f}%\")\n",
    "print(f\"   % of negative scores > mean positive score: {(neg_scores_baseline > pos_scores_baseline.mean()).sum()/len(neg_scores_baseline)*100:.1f}%\")\n",
    "if overlap > 0.5:\n",
    "    print(f\"   ⚠️  WARNING: High overlap suggests noisy labels!\")\n",
    "\n",
    "# Check 3: Word overlap in pairs\n",
    "print(f\"\\n3. SEMANTIC OVERLAP IN PAIRS\")\n",
    "print(f\"   Positive pairs word overlap: {np.mean(pos_example_similarities):.4f}\")\n",
    "print(f\"   Negative pairs word overlap: {np.mean(neg_example_similarities):.4f}\")\n",
    "print(f\"   Difference: {np.mean(pos_example_similarities) - np.mean(neg_example_similarities):.4f}\")\n",
    "if abs(np.mean(pos_example_similarities) - np.mean(neg_example_similarities)) < 0.05:\n",
    "    print(f\"   ⚠️  WARNING: Positive and negative pairs are too similar!\")\n",
    "else:\n",
    "    print(f\"   ✓ Good semantic separation\")\n",
    "\n",
    "# Check 4: Category-based label validation\n",
    "print(f\"\\n4. TEST SET COMPOSITION\")\n",
    "print(f\"   Total pairs: {len(test_labels)}\")\n",
    "print(f\"   Positive pairs: {sum(test_labels)} ({sum(test_labels)/len(test_labels)*100:.1f}%)\")\n",
    "print(f\"   Negative pairs: {len(test_labels)-sum(test_labels)} ({(len(test_labels)-sum(test_labels))/len(test_labels)*100:.1f}%)\")\n",
    "print(f\"   Expected ratio: 50-50 for balanced test set\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS:\")\n",
    "print(\"=\"*80)\n",
    "if separability < 0.1:\n",
    "    print(\"✗ Low baseline separability → Test set may have noisy/weak labels\")\n",
    "    print(\"  ACTION: Review 'positive' pair labels - same category ≠ semantically similar\")\n",
    "if overlap > 0.5:\n",
    "    print(\"✗ High score overlap → Hard to distinguish positives from negatives\")\n",
    "    print(\"  ACTION: Use stricter pair generation (require min semantic similarity for positives)\")\n",
    "print(\"\\nAll models (baseline + fine-tuned) struggle with this test set.\")\n",
    "print(\"The problem is likely in test pair generation, not model quality.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

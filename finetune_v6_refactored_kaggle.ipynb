{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITSM Ticket Similarity - Model Fine-tuning (v6 Refactored)\n",
    "\n",
    "**Version 6 Refactored** improves upon the original v6 by:\n",
    "1. **Robust Environment Setup:** Automatically handles NLTK data and library dependencies (Kaggle/Local).\n",
    "2. **Improved Pipeline:** Cleaner data loading and preprocessing.\n",
    "3. **Contextual Embeddings:** Retains the structured input format `[Service] [Category] Description`.\n",
    "4. **Reliable Logging:** Auto-detects the best location for logs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 . Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:28:35,624 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è Downloading NLTK: wordnet\n",
      "‚¨áÔ∏è Downloading NLTK: omw-1.4\n",
      "‚¨áÔ∏è Pre-downloading model to cache: sentence-transformers/all-mpnet-base-v2 -> /kaggle/working/hf_cache\n",
      "‚úÖ Model cached.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, subprocess, pkg_resources\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_packages(pkgs):\n",
    "    missing = []\n",
    "    for name, spec in pkgs.items():\n",
    "        try:\n",
    "            pkg_resources.get_distribution(name)\n",
    "        except pkg_resources.DistributionNotFound:\n",
    "            missing.append(spec)\n",
    "    if missing:\n",
    "        print(\"üì¶ Installing:\", \", \".join(missing))\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", *missing])\n",
    "\n",
    "def ensure_nltk(resources=(\"wordnet\",\"omw-1.4\",\"stopwords\",\"punkt\")):\n",
    "    import nltk\n",
    "    nltk_data = Path.home() / \"nltk_data\"\n",
    "    nltk_data.mkdir(exist_ok=True)\n",
    "    if str(nltk_data) not in nltk.data.path:\n",
    "        nltk.data.path.append(str(nltk_data))\n",
    "    for res in resources:\n",
    "        try:\n",
    "            nltk.data.find(f\"corpora/{res}\")\n",
    "        except LookupError:\n",
    "            try:\n",
    "                nltk.data.find(f\"tokenizers/{res}\")\n",
    "            except LookupError:\n",
    "                print(f\"‚¨áÔ∏è Downloading NLTK: {res}\")\n",
    "                nltk.download(res, quiet=True, download_dir=str(nltk_data))\n",
    "\n",
    "    # 4. Pre-download SentenceTransformer model to cache (avoid runtime download delays)\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        cache_dir = Path(os.environ.get('TRANSFORMERS_CACHE', Path.home() / '.cache' / 'huggingface' / 'hub'))\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        os.environ['TRANSFORMERS_CACHE'] = str(cache_dir)\n",
    "        os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(cache_dir)\n",
    "        print(f'‚¨áÔ∏è Pre-downloading model to cache: sentence-transformers/all-mpnet-base-v2 -> {cache_dir}')\n",
    "        SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device='cpu')\n",
    "        print('‚úÖ Model cached.')\n",
    "    except Exception as e:\n",
    "        print(f'‚ö†Ô∏è Model pre-download failed: {e}')\n",
    "\n",
    "def run_setup():\n",
    "    os.environ['WANDB_DISABLED'] = 'true'\n",
    "    os.environ['WANDB_MODE'] = 'offline'\n",
    "    os.environ['WANDB_SILENT'] = 'true'\n",
    "    os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "    os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "    pkgs = {\n",
    "        \"sentence-transformers\": \"sentence-transformers\",\n",
    "        \"transformers\": \"transformers\",\n",
    "        \"torch\": \"torch\",               # use Kaggle‚Äôs unless missing\n",
    "        \"torchvision\": \"torchvision\",\n",
    "        \"torchaudio\": \"torchaudio\",\n",
    "        \"scikit-learn\": \"scikit-learn\",\n",
    "        \"scipy\": \"scipy\",\n",
    "        \"numpy\": \"numpy\",\n",
    "        \"pandas\": \"pandas\",\n",
    "        \"tqdm\": \"tqdm\",\n",
    "        \"imbalanced-learn\": \"imbalanced-learn\",\n",
    "        \"datasets\": \"datasets\",\n",
    "        \"joblib\": \"joblib\",\n",
    "        \"protobuf\": \"protobuf<=3.20.1\",\n",
    "        \"requests\": \"requests\",\n",
    "        \"python-dotenv\": \"python-dotenv\",\n",
    "        \"openai\": \"openai\",\n",
    "        \"seaborn\": \"seaborn\",\n",
    "        \"matplotlib\": \"matplotlib\",\n",
    "        \"pytorch-lightning\": \"pytorch-lightning\",\n",
    "    }\n",
    "    ensure_packages(pkgs)\n",
    "    ensure_nltk()\n",
    "\n",
    "run_setup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Datalogging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:28:38,057 - INFO - üìù Logging to: /kaggle/working/training_v6_refactored.log\n"
     ]
    }
   ],
   "source": [
    "# Core Python\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ML Frameworks\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# NLP & Metrics\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from imblearn.over_sampling import SMOTE # Ensure SMOTE is imported for classifier\n",
    "\n",
    "# Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models\n",
    "from sentence_transformers.evaluation import SentenceEvaluator\n",
    "\n",
    "# Filter warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Suppress tokenizers warning\n",
    "\n",
    "# --- Logging Setup ---\n",
    "def setup_logger():\n",
    "    # Determine log path: Prefer local current dir, allow agent override\n",
    "    log_filename = \"training_v6_refactored.log\"\n",
    "    log_path = Path.cwd() / log_filename\n",
    "    \n",
    "    # If running in Agent env, use agent's temp dir\n",
    "    if os.environ.get(\"GEMINI_TEMP_DIR\"):\n",
    "        log_path = Path(os.environ.get(\"GEMINI_TEMP_DIR\")) / log_filename\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path, mode='w'), # Overwrite for fresh run\n",
    "            logging.StreamHandler()\n",
    "        ],\n",
    "        force=True\n",
    "    )\n",
    "    return logging.getLogger(__name__), log_path\n",
    "\n",
    "logger, LOG_FILE = setup_logger()\n",
    "\n",
    "def log(msg, level=logging.INFO):\n",
    "    if level == logging.INFO:\n",
    "        logger.info(msg)\n",
    "    elif level == logging.WARNING:\n",
    "        logger.warning(msg)\n",
    "    else:\n",
    "        logger.debug(msg)\n",
    "\n",
    "log(f\"üìù Logging to: {LOG_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Model Configuration and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:28:38,120 - INFO - üöÄ CUDA Detected: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "CONFIG = {\n",
    "    'model_name': 'sentence-transformers/all-mpnet-base-v2',\n",
    "    'output_dir': 'models/v6_refactored_finetuned', # New output directory\n",
    "    'source_data': 'data/dummy_data_promax.csv', # Will try to resolve this\n",
    "    'relationship_data': 'data/relationship_pairs.json', # For relationship classifier\n",
    "    \n",
    "    # Hyperparameters\n",
    "    'epochs': 15,\n",
    "    'batch_size': 32, # Lower batch size for stability\n",
    "    'lr': 2e-5,\n",
    "    'max_seq_length': 384,\n",
    "    \n",
    "    # Data Strategy\n",
    "    'num_pairs': 50000, # Number of pairs for training/validation\n",
    "    'pos_ratio': 0.4,   # 40% Positive, 60% Negative\n",
    "    'neg_mining_range': (0.2, 0.5), # TF-IDF score range for \"Hard Negatives\"\n",
    "    'eval_split': 0.15, # % of data for validation\n",
    "    \n",
    "    # Seed\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Set Seeds\n",
    "random.seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CONFIG['seed'])\n",
    "    log(f\"üöÄ CUDA Detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    log(\"‚ö†Ô∏è CUDA Not Detected. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:28:38,197 - INFO - üìÇ Loading incident data from: /kaggle/input/itsm-dataset/dummy_data_promax.csv\n",
      "2025-11-30 21:28:38,838 - INFO - ‚úÖ Preprocessed 10000 incidents.\n",
      "2025-11-30 21:28:38,839 - INFO - Sample preprocessed text: '[crm (d365, salesforce, genesis, pcube, hussmann services) | bc - basis] [configuration | program bug] Group: piscap l2 workflow. Request: Adjust Configuration/Program bug configuration in CRM (D365, SalesForce, Genesis, PCube, HussMann Services). I encountered an issue where Request: Adjust Configuration/Program bug configuration in CRM (D365, SalesForce, Genesis, PCube, HussMann Services). I'd like assistance to investigate and resolve it.'\n",
      "2025-11-30 21:28:38,842 - INFO - ‚úÖ Index reset. Range: 0 to 9999\n"
     ]
    }
   ],
   "source": [
    "def resolve_data_path(path_str):\n",
    "    \"\"\"Smart path resolver for Local/Kaggle/Colab/Agent envs.\"\"\"\n",
    "    # 1. As-is\n",
    "    p = Path(path_str)\n",
    "    if p.exists(): return p.resolve()\n",
    "    \n",
    "    # 2. Relative to current script location (for agent or local execution)\n",
    "    # Using Path.cwd() as a robust base for notebooks\n",
    "    script_dir = Path.cwd() \n",
    "    if (script_dir / path_str).exists(): return (script_dir / path_str).resolve()\n",
    "\n",
    "    # 3. Common Kaggle/Colab input paths\n",
    "    # Assuming path_str might be like 'data/file.csv'\n",
    "    base_filename = Path(path_str).name\n",
    "    \n",
    "    kaggle_input_dir = Path(\"/kaggle/input\")\n",
    "    if kaggle_input_dir.exists():\n",
    "        for dataset_dir in kaggle_input_dir.iterdir():\n",
    "            if (dataset_dir / base_filename).exists():\n",
    "                return (dataset_dir / base_filename).resolve()\n",
    "            if (dataset_dir / path_str).exists(): # if path_str includes subdir like 'data/'\n",
    "                return (dataset_dir / path_str).resolve()\n",
    "                \n",
    "    colab_dir = Path(\"/content\")\n",
    "    if (colab_dir / path_str).exists(): return (colab_dir / path_str).resolve()\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find {path_str} in any common locations (cwd, relative, Kaggle, Colab).\")\n",
    "\n",
    "def load_and_preprocess_data(config):\n",
    "    source_path = resolve_data_path(config['source_data'])\n",
    "    log(f\"üìÇ Loading incident data from: {source_path}\")\n",
    "    df = pd.read_csv(source_path)\n",
    "    \n",
    "    # Required columns for contextual embedding\n",
    "    required_cols = [\"Number\", \"Short Description\", \"Description\", \"Category\", \"Subcategory\", \n",
    "                     \"Service\", \"Service offering\", \"Assignment group\"]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    # Fill NA and Clean Text\n",
    "    # We ensure all context fields are strings\n",
    "    placeholders = {\"\", \"nan\", \"none\", \"null\", \"unknown\", \"n/a\", \"na\"}\n",
    "\n",
    "    def normalize_field(val: str) -> str:\n",
    "        s = str(val).strip()\n",
    "        s = re.sub(r\"\\s+\", \" \", s) # Replace multiple spaces with single\n",
    "        if s.lower() in placeholders:\n",
    "            return \"\"\n",
    "        return s\n",
    "\n",
    "    for col in [c for c in required_cols if c != \"Number\"]:\n",
    "        df[col] = df[col].fillna(\"\").apply(normalize_field)\n",
    "\n",
    "    # Normalize casing for structured context fields to reduce duplicates\n",
    "    context_cols = [\"Service\", \"Service offering\", \"Category\", \"Subcategory\", \"Assignment group\"]\n",
    "    for col in context_cols:\n",
    "        df[col] = df[col].str.lower()\n",
    "\n",
    "    # Construct Rich Text Representation (Contextual Prefixing)\n",
    "    # Format: [Service | Service offering] [Category | Subcategory] Group: Assignment group. Short Description. Description\n",
    "    def build_bracketed(parts):\n",
    "        clean_parts = [p for p in parts if p]\n",
    "        return f\"[{ ' | '.join(clean_parts) }] \" if clean_parts else \"\" \n",
    "\n",
    "    df['context_service'] = df.apply(lambda row: build_bracketed([row['Service'], row['Service offering']]), axis=1)\n",
    "    df['context_category'] = df.apply(lambda row: build_bracketed([row['Category'], row['Subcategory']]), axis=1)\n",
    "    df['context_group'] = df.apply(lambda row: f\"Group: {row['Assignment group']}. \" if row['Assignment group'] else \"\", axis=1)\n",
    "\n",
    "    df['text'] = (\n",
    "        df['context_service'] +\n",
    "        df['context_category'] +\n",
    "        df['context_group'] +\n",
    "        df['Short Description'].str.strip() + \". \" +\n",
    "        df['Description'].str.strip()\n",
    "    ).str.replace(r\"\\\\s+\\\\.\", \".\", regex=True) # Remove space before period\n",
    "    df['text'] = df['text'].str.replace(r\"\\\\s+\", \" \", regex=True).str.strip() # Clean up excess spaces\n",
    "\n",
    "    # Filter empty or too short\n",
    "    initial_count = len(df)\n",
    "    min_length = 10 # Configurable if needed\n",
    "    df = df[df['text'].str.len() >= min_length].copy()\n",
    "    dropped = initial_count - len(df)\n",
    "    if dropped > 0:\n",
    "        log(f\"‚ö†Ô∏è Dropped {dropped} incidents due to short/empty text after preprocessing.\")\n",
    "    \n",
    "    # Create unique group ID for stratified splitting (Category-Subcategory)\n",
    "    df['category_id'] = df.groupby(['Category', 'Subcategory']).ngroup()\n",
    "    \n",
    "    log(f\"‚úÖ Preprocessed {len(df)} incidents.\")\n",
    "    log(f\"Sample preprocessed text: '{df['text'].iloc[0]}'\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    log(f'‚úÖ Index reset. Range: {df.index.min()} to {df.index.max()}')\n",
    "    return df\n",
    "\n",
    "df_incidents = load_and_preprocess_data(CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:28:38,935 - INFO - Split Incidents: Train=8500, Eval=1500\n",
      "2025-11-30 21:28:38,938 - INFO - ‚è≥ Fitting TF-IDF for similarity mining...\n",
      "2025-11-30 21:28:39,216 - INFO - ‚úÖ TF-IDF fit complete. Matrix shape: (8500, 110)\n",
      "2025-11-30 21:28:39,218 - INFO - üîé Generating 17000 positive and 25500 hard negative pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6868b83dfe04c5298e4700dd76c7cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Positives:   0%|          | 0/17000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b25ca03fb8488b9d1744652d79b0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Negatives:   0%|          | 0/25500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:29:19,509 - INFO - ‚úÖ Generated 42500 training pairs.\n",
      "2025-11-30 21:29:19,514 - INFO - ‚è≥ Fitting TF-IDF for similarity mining...\n",
      "2025-11-30 21:29:19,566 - INFO - ‚úÖ TF-IDF fit complete. Matrix shape: (1500, 110)\n",
      "2025-11-30 21:29:19,568 - INFO - üîé Generating 3000 positive and 4500 hard negative pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8025620235ab4261818e2e197fd38327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Positives:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba3e3e85051489492c4f62ad921c03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Negatives:   0%|          | 0/4500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:29:26,851 - INFO - ‚úÖ Generated 7500 training pairs.\n",
      "2025-11-30 21:29:26,853 - INFO - Final Samples: Training=42500, Evaluation=7500\n"
     ]
    }
   ],
   "source": [
    "class TextSimilarityCalculator:\n",
    "    def __init__(self, texts):\n",
    "        self.lemmatizer = WordNetLemmatizer() if 'wordnet' in nltk.data.path else None\n",
    "        self.stop_words = set(stopwords.words('english')) if 'stopwords' in nltk.data.path else ENGLISH_STOP_WORDS\n",
    "        self.vectorizer = TfidfVectorizer(stop_words=list(self.stop_words), max_features=10000)\n",
    "        \n",
    "        log(\"‚è≥ Fitting TF-IDF for similarity mining...\")\n",
    "        self.tfidf = self.vectorizer.fit_transform(texts)\n",
    "        log(f\"‚úÖ TF-IDF fit complete. Matrix shape: {self.tfidf.shape}\")\n",
    "\n",
    "    def get_tfidf_similarity(self, idx1, idx2):\n",
    "        if idx1 >= self.tfidf.shape[0] or idx2 >= self.tfidf.shape[0]:\n",
    "            return 0.0 \n",
    "        return (self.tfidf[idx1] @ self.tfidf[idx2].T).toarray()[0][0]\n",
    "\n",
    "def generate_smart_pairs(df, target_count, config):\n",
    "    \"\"\"Generates positive and hard negative pairs based on TF-IDF similarity.\"\"\"\n",
    "    # Ensure index is reset for direct iloc/loc correspondence\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    sim_calculator = TextSimilarityCalculator(df['text'].tolist())\n",
    "\n",
    "    positive_target = int(target_count * config['pos_ratio'])\n",
    "    negative_target = target_count - positive_target\n",
    "\n",
    "    pairs = []\n",
    "    \n",
    "    # Group by Category/Subcategory\n",
    "    # groups indices are now reliable 0..N integers because of reset_index\n",
    "    groups = df.groupby('category_id').indices \n",
    "    valid_groups = {k: list(v) for k, v in groups.items() if len(v) >= 2}\n",
    "    all_indices = list(df.index)\n",
    "\n",
    "    log(f\"üîé Generating {positive_target} positive and {negative_target} hard negative pairs...\")\n",
    "\n",
    "    # --- 1. Positive Pairs ---\n",
    "    pbar_pos = tqdm(total=positive_target, desc=\"Generating Positives\")\n",
    "    attempts = 0\n",
    "    while len(pairs) < positive_target and attempts < positive_target * 5:\n",
    "        attempts += 1\n",
    "        if not valid_groups: break\n",
    "        \n",
    "        gid = random.choice(list(valid_groups.keys()))\n",
    "        g_idxs = valid_groups[gid] # already a list\n",
    "        \n",
    "        if len(g_idxs) < 2: continue\n",
    "        \n",
    "        i1, i2 = random.sample(g_idxs, 2)\n",
    "        \n",
    "        # i1, i2 are integer positions. Since we reset index, they are also labels.\n",
    "        # Using iloc is safest for 'text' column access if we mix things up, \n",
    "        # but here loc==iloc. We use simple integer indexing for tfidf.\n",
    "        \n",
    "        sim = sim_calculator.get_tfidf_similarity(i1, i2)\n",
    "        \n",
    "        if sim > 0.3:\n",
    "            pairs.append(InputExample(texts=[df.at[i1, 'text'], df.at[i2, 'text']], label=1.0))\n",
    "            pbar_pos.update(1)\n",
    "            \n",
    "    # Fill remaining positives\n",
    "    if len(pairs) < positive_target:\n",
    "        log(f\"‚ö†Ô∏è Filling {positive_target - len(pairs)} remaining positives with random in-group pairs.\")\n",
    "        while len(pairs) < positive_target:\n",
    "            if not valid_groups: break\n",
    "            gid = random.choice(list(valid_groups.keys()))\n",
    "            g_idxs = valid_groups[gid]\n",
    "            if len(g_idxs) < 2: continue\n",
    "            i1, i2 = random.sample(g_idxs, 2)\n",
    "            pairs.append(InputExample(texts=[df.at[i1, 'text'], df.at[i2, 'text']], label=1.0))\n",
    "            pbar_pos.update(1)\n",
    "            \n",
    "    pbar_pos.close()\n",
    "\n",
    "    # --- 2. Hard Negative Pairs ---\n",
    "    current_pos_count = len(pairs)\n",
    "    pbar_neg = tqdm(total=negative_target, desc=\"Generating Negatives\")\n",
    "    attempts = 0\n",
    "    max_attempts = negative_target * 10 \n",
    "    \n",
    "    while (len(pairs) - current_pos_count) < negative_target and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        \n",
    "        i1, i2 = random.sample(all_indices, 2)\n",
    "        \n",
    "        if df.at[i1, 'category_id'] == df.at[i2, 'category_id']:\n",
    "            continue\n",
    "            \n",
    "        sim = sim_calculator.get_tfidf_similarity(i1, i2)\n",
    "        \n",
    "        min_sim, max_sim = config['neg_mining_range']\n",
    "        if min_sim <= sim <= max_sim:\n",
    "            pairs.append(InputExample(texts=[df.at[i1, 'text'], df.at[i2, 'text']], label=0.0))\n",
    "            pbar_neg.update(1)\n",
    "            \n",
    "    # Fill remaining negatives\n",
    "    neg_generated = len(pairs) - current_pos_count\n",
    "    if neg_generated < negative_target:\n",
    "         log(f\"‚ö†Ô∏è Filling {negative_target - neg_generated} remaining negatives with random cross-category pairs.\")\n",
    "         while (len(pairs) - current_pos_count) < negative_target:\n",
    "            i1, i2 = random.sample(all_indices, 2)\n",
    "            if df.at[i1, 'category_id'] != df.at[i2, 'category_id']:\n",
    "                pairs.append(InputExample(texts=[df.at[i1, 'text'], df.at[i2, 'text']], label=0.0))\n",
    "                pbar_neg.update(1)\n",
    "                \n",
    "    pbar_neg.close()\n",
    "    \n",
    "    log(f\"‚úÖ Generated {len(pairs)} training pairs.\")\n",
    "    return pairs\n",
    "\n",
    "# Split incidents into train/eval sets for pair generation\n",
    "# Note: we split BEFORE pair generation, so we must reset index on the splits individually\n",
    "train_incidents_df, eval_incidents_df = train_test_split(\n",
    "    df_incidents,\n",
    "    test_size=CONFIG['eval_split'],\n",
    "    stratify=df_incidents['category_id'],\n",
    "    random_state=CONFIG['seed']\n",
    ")\n",
    "\n",
    "log(f\"Split Incidents: Train={len(train_incidents_df)}, Eval={len(eval_incidents_df)}\")\n",
    "\n",
    "# Calculate target pairs for each split\n",
    "train_num_pairs = int(CONFIG['num_pairs'] * (1 - CONFIG['eval_split']))\n",
    "eval_num_pairs = CONFIG['num_pairs'] - train_num_pairs\n",
    "\n",
    "train_examples = generate_smart_pairs(train_incidents_df, train_num_pairs, CONFIG)\n",
    "eval_examples = generate_smart_pairs(eval_incidents_df, eval_num_pairs, CONFIG)\n",
    "\n",
    "log(f\"Final Samples: Training={len(train_examples)}, Evaluation={len(eval_examples)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda? True\n",
      "Tesla P100-PCIE-16GB\n",
      "Sun Nov 30 21:29:26 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   52C    P0             34W /  250W |   14279MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch, os, subprocess\n",
    "print(\"cuda?\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    subprocess.run([\"nvidia-smi\"])\n",
    "\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/kaggle/working/hf_cache\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:29:27,046 - INFO - üîå Using device: cuda\n",
      "2025-11-30 21:29:27,050 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2025-11-30 21:29:29,665 - INFO - ‚úÖ Model initialized on device: cuda. Max seq len: 384\n",
      "2025-11-30 21:29:29,666 - INFO - üìä Train batches: 1329, Eval examples: 7500\n",
      "2025-11-30 21:29:29,669 - INFO - üöÄ Starting training... Model will be saved to: models/v6_refactored_finetuned/v6_refactored_finetuned_20251130_2129\n",
      "2025-11-30 21:29:29,669 - INFO - üö¶ Ready to train: batches=1329, epochs=15, batch_size=32, device=cuda\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4802d452aee4ccba276948e5dc0d305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5004' max='19935' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5004/19935 1:18:54 < 3:55:33, 1.06 it/s, Epoch 3.76/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Evaluator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1329</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.848528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1993</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.848528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2658</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.848528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3986</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.848528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3987</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.848528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 21:50:11,218 - INFO - Epoch 1.0 Steps 1329: Spearman=0.8485, Pearson=0.9961, ROC_AUC=1.0000, PR_AUC=1.0000\n",
      "2025-11-30 21:50:11,220 - INFO - Save model to models/v6_refactored_finetuned/v6_refactored_finetuned_20251130_2129\n",
      "2025-11-30 22:00:58,832 - INFO - Epoch 1.4996237772761476 Steps 1993: Spearman=0.8485, Pearson=0.9986, ROC_AUC=1.0000, PR_AUC=1.0000\n",
      "2025-11-30 22:00:58,836 - INFO - Save model to models/v6_refactored_finetuned/v6_refactored_finetuned_20251130_2129\n",
      "2025-11-30 22:11:45,558 - INFO - Epoch 2.0 Steps 2658: Spearman=0.8485, Pearson=0.9995, ROC_AUC=1.0000, PR_AUC=1.0000\n",
      "2025-11-30 22:32:24,542 - INFO - Epoch 2.999247554552295 Steps 3986: Spearman=0.8485, Pearson=0.9995, ROC_AUC=1.0000, PR_AUC=1.0000\n",
      "2025-11-30 22:32:24,546 - INFO - Save model to models/v6_refactored_finetuned/v6_refactored_finetuned_20251130_2129\n",
      "2025-11-30 22:33:20,551 - INFO - Epoch 3.0 Steps 3987: Spearman=0.8485, Pearson=0.9995, ROC_AUC=1.0000, PR_AUC=1.0000\n",
      "2025-11-30 22:33:20,554 - INFO - Save model to models/v6_refactored_finetuned/v6_refactored_finetuned_20251130_2129\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_403/3205783199.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üö¶ Ready to train: batches={len(train_dataloader)}, epochs={CONFIG['epochs']}, batch_size={CONFIG['batch_size']}, device={device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mtrain_objectives\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/fit_mixin.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit, resume_from_checkpoint)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m                     )\n\u001b[1;32m   2547\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3795\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3797\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3799\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2576\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2577\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2578\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Model Evaluation Class ---\n",
    "class ITSMEvaluator(SentenceEvaluator):\n",
    "    def __init__(self, examples: list[InputExample], batch_size: int = 16, name: str = ''):\n",
    "        self.examples = examples\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "\n",
    "        self.texts1 = [ex.texts[0] for ex in examples]\n",
    "        self.texts2 = [ex.texts[1] for ex in examples]\n",
    "        self.labels = np.array([ex.label for ex in examples])\n",
    "\n",
    "        self.csv_file = f\"{name}_eval_results.csv\"\n",
    "        self.csv_headers = [\"epoch\", \"steps\", \"spearman\", \"pearson\", \"roc_auc\", \"pr_auc\"]\n",
    "\n",
    "    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n",
    "        model.eval()\n",
    "        log(f\"üìä Running evaluation at epoch={epoch}, step={steps}...\", level=logging.DEBUG)\n",
    "\n",
    "        # Encode all texts\n",
    "        embeddings1 = model.encode(self.texts1, batch_size=self.batch_size, show_progress_bar=False, convert_to_numpy=True)\n",
    "        embeddings2 = model.encode(self.texts2, batch_size=self.batch_size, show_progress_bar=False, convert_to_numpy=True)\n",
    "\n",
    "        # Calculate cosine similarities\n",
    "        cosine_scores = np.sum(embeddings1 * embeddings2, axis=1) / (np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1))\n",
    "\n",
    "        # Calculate metrics\n",
    "        eval_pearson, _ = pearsonr(self.labels, cosine_scores)\n",
    "        eval_spearman, _ = spearmanr(self.labels, cosine_scores)\n",
    "\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(self.labels, cosine_scores)\n",
    "            pr_auc = average_precision_score(self.labels, cosine_scores)\n",
    "        except ValueError:\n",
    "            roc_auc = 0.0\n",
    "            pr_auc = 0.0\n",
    "            log(\"‚ö†Ô∏è ROC/PR AUC cannot be calculated due to single class in evaluation labels.\", level=logging.WARNING)\n",
    "\n",
    "        log_msg = (\n",
    "            f\"Epoch {epoch if epoch != -1 else 'N/A'} Steps {steps if steps != -1 else 'N/A'}: \"\n",
    "            f\"Spearman={eval_spearman:.4f}, Pearson={eval_pearson:.4f}, \"\n",
    "            f\"ROC_AUC={roc_auc:.4f}, PR_AUC={pr_auc:.4f}\"\n",
    "        )\n",
    "        log(log_msg)\n",
    "\n",
    "        if output_path is not None:\n",
    "            csv_path = Path(output_path) / self.csv_file\n",
    "            output_data = [epoch, steps, eval_spearman, eval_pearson, roc_auc, pr_auc]\n",
    "\n",
    "            if not csv_path.is_file():\n",
    "                with open(csv_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(\",\".join(self.csv_headers) + \"\")\n",
    "\n",
    "            with open(csv_path, 'a', encoding='utf-8') as f:\n",
    "                f.write(\",\".join(map(str, output_data)) + \"\")\n",
    "        return eval_spearman\n",
    "\n",
    "# --- Model Setup ---\n",
    "# Select device (prefer CUDA, then MPS, else CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu'\n",
    "log(f'üîå Using device: {device}')\n",
    "\n",
    "model = SentenceTransformer(CONFIG['model_name'], device=device)\n",
    "model.max_seq_length = CONFIG['max_seq_length']\n",
    "\n",
    "log(f'‚úÖ Model initialized on device: {device}. Max seq len: {model.max_seq_length}')\n",
    "pin_memory = device == 'cuda'\n",
    "num_workers = 2\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=CONFIG['batch_size'], num_workers=num_workers, pin_memory=pin_memory)\n",
    "log(f'üìä Train batches: {len(train_dataloader)}, Eval examples: {len(eval_examples)}')\n",
    "\n",
    "evaluator = ITSMEvaluator(eval_examples, batch_size=CONFIG['batch_size'], name='validation')\n",
    "\n",
    "# --- Training Execution ---\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "save_path = Path(CONFIG['output_dir']) / f\"{Path(CONFIG['output_dir']).name}_{timestamp}\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log(f\"üöÄ Starting training... Model will be saved to: {save_path}\")\n",
    "log(f\"üö¶ Ready to train: batches={len(train_dataloader)}, epochs={CONFIG['epochs']}, batch_size={CONFIG['batch_size']}, device={device}\")\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    warmup_steps=int(len(train_dataloader) * CONFIG['epochs'] * 0.1),\n",
    "    optimizer_params={'lr': CONFIG['lr']},\n",
    "    output_path=str(save_path),\n",
    "    evaluation_steps=int(len(train_dataloader) * CONFIG['epochs'] * 0.1),\n",
    "    save_best_model=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "log('model.fit finished')\n",
    "log('Training complete.')\n",
    "\n",
    "# --- Final Evaluation ---\n",
    "log(\"‚ú® Reloading best model for final evaluation...\")\n",
    "best_model = SentenceTransformer(str(save_path))\n",
    "\n",
    "final_evaluator = ITSMEvaluator(eval_examples, batch_size=CONFIG['batch_size'], name='final_evaluation')\n",
    "final_spearman = final_evaluator(best_model, output_path=str(save_path), epoch='final', steps='final')\n",
    "\n",
    "log(f\"Final Model (best) saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resolve_data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m rel_data_path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     rel_data_path = \u001b[43mresolve_data_path\u001b[49m(CONFIG[\u001b[33m'\u001b[39m\u001b[33mrelationship_data\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m     15\u001b[39m     log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è Relationship data not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mrelationship_data\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Skipping classifier training.\u001b[39m\u001b[33m\"\u001b[39m, level=logging.WARNING)\n",
      "\u001b[31mNameError\u001b[39m: name 'resolve_data_path' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Relationship Classifier (Optional) ---\n",
    "# This part is optional and only runs if imbalanced-learn is available and data exists.\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "\n",
    "if IMBLEARN_AVAILABLE:\n",
    "    rel_data_path = None\n",
    "    try:\n",
    "        rel_data_path = resolve_data_path(CONFIG['relationship_data'])\n",
    "    except FileNotFoundError:\n",
    "        log(f\"‚ö†Ô∏è Relationship data not found at {CONFIG['relationship_data']}. Skipping classifier training.\", level=logging.WARNING)\n",
    "\n",
    "    if rel_data_path and rel_data_path.exists():\n",
    "        log('üß† Training Relationship Classifier...')\n",
    "        with open(rel_data_path, 'r') as f:\n",
    "            rel_data = json.load(f)\n",
    "\n",
    "        rel_df = pd.DataFrame(rel_data)\n",
    "        # Filter valid labels (adjust as per your dataset)\n",
    "        valid_labels = ['duplicate', 'causal', 'related', 'none']\n",
    "        rel_df = rel_df[rel_df['label'].isin(valid_labels)]\n",
    "        log(f\"Relationship samples after filtering: {len(rel_df)}\")\n",
    "\n",
    "        if len(rel_df) > 0:\n",
    "            # Encode features using fine-tuned model\n",
    "            text_a = rel_df['text_a'].tolist()\n",
    "            text_b = rel_df['text_b'].tolist()\n",
    "\n",
    "            log(\"‚è≥ Encoding relationship data with the best model...\")\n",
    "            emb_a = best_model.encode(text_a, batch_size=CONFIG['batch_size'], show_progress_bar=False)\n",
    "            emb_b = best_model.encode(text_b, batch_size=CONFIG['batch_size'], show_progress_bar=False)\n",
    "\n",
    "            # Feature Engineering: (u, v, |u-v|, u*v)\n",
    "            X = np.hstack([emb_a, emb_b, np.abs(emb_a - emb_b), emb_a * emb_b])\n",
    "            y = rel_df['label']\n",
    "\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            from sklearn.metrics import classification_report\n",
    "            \n",
    "            # Only apply SMOTE if there are enough samples and multiple classes\n",
    "            if len(np.unique(y)) > 1 and len(X) > 1 and len(np.unique(y)) < len(X): # Ensure SMOTE doesn't crash\n",
    "                smote = SMOTE(k_neighbors=min(2, len(X) - 1), random_state=CONFIG['seed']) # k_neighbors must be <= n_samples-1\n",
    "                X_res, y_res = smote.fit_resample(X, y)\n",
    "                log(f\"After SMOTE: {len(X_res)} samples\")\n",
    "            else:\n",
    "                X_res, y_res = X, y\n",
    "                log(\"‚ö†Ô∏è Skipping SMOTE due to insufficient samples or single class after filtering.\", level=logging.WARNING)\n",
    "\n",
    "            # Train Classifier\n",
    "            clf = LogisticRegression(max_iter=1000, multi_class='multinomial', random_state=CONFIG['seed'], solver='lbfgs')\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=CONFIG['seed'], stratify=y_res)\n",
    "\n",
    "            log(\"‚è≥ Training Logistic Regression classifier...\")\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluation\n",
    "            y_pred = clf.predict(X_test)\n",
    "            log(\"‚úÖ Relationship Classifier Report:\")\n",
    "            log(f\"\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "            # Save Classifier\n",
    "            import joblib\n",
    "            classifier_save_path = save_path / \"relationship_classifier.joblib\"\n",
    "            joblib.dump(clf, classifier_save_path)\n",
    "            log(f\"‚úÖ Relationship classifier saved to: {classifier_save_path}\")\n",
    "        else:\n",
    "            log(\"‚ö†Ô∏è No valid relationship samples to train classifier. Skipping.\", level=logging.WARNING)\n",
    "    else:\n",
    "        log(f\"‚ö†Ô∏è Relationship data not found at resolved path '{rel_data_path}'. Skipping classifier training.\", level=logging.WARNING)\n",
    "else:\n",
    "    log('‚ö†Ô∏è imbalanced-learn not installed. Skipping relationship classifier.', level=logging.WARNING)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

#!/usr/bin/env python3
"""
Build evaluate_model_v2.ipynb as a STANDALONE notebook.
Everything in one file - no external config or utils modules.
Only external dependency: .env file for HuggingFace token.
"""

import json

# Read original for metadata
with open('evaluate_model.ipynb', 'r', encoding='utf-8') as f:
    original = json.load(f)

# Create v2
nb = {
    "cells": [],
    "metadata": original["metadata"],
    "nbformat": 4,
    "nbformat_minor": 5
}

def mk_md(source):
    return {"cell_type": "markdown", "metadata": {}, "source": source if isinstance(source, list) else [source]}

def mk_code(source):
    return {"cell_type": "code", "execution_count": None, "metadata": {}, "outputs": [], "source": source if isinstance(source, list) else [source]}

cells = nb["cells"]

# ============================================================================
# HEADER
# ============================================================================

cells.append(mk_md([
    "# Model Evaluation Notebook v2 (Standalone)\n",
    "\n",
    "**Self-contained evaluation framework with all improvements in one notebook.**\n",
    "\n",
    "## What's New in v2:\n",
    "\n",
    "### Security\n",
    "- ✅ No hardcoded HuggingFace tokens (uses .env file)\n",
    "- ✅ Token in environment variable only\n",
    "\n",
    "### Configuration\n",
    "- ✅ All settings centralized in Section 1\n",
    "- ✅ No scattered magic numbers\n",
    "- ✅ Easy parameter tuning\n",
    "\n",
    "### New Diagnostics\n",
    "- ✅ Confusion matrix visualization (TP/FP/TN/FN)\n",
    "- ✅ Error analysis (detailed FP/FN breakdown)\n",
    "- ✅ Inference speed benchmarking\n",
    "- ⚠️ Adversarial diagnostic (needs category data)\n",
    "\n",
    "### Code Quality\n",
    "- ✅ All utility functions included\n",
    "- ✅ Input validation throughout\n",
    "- ✅ Reproducibility guaranteed (all seeds set)\n",
    "- ✅ Better error handling\n",
    "- ✅ Environment metadata tracking\n",
    "\n",
    "## How to Use:\n",
    "\n",
    "1. Create `.env` file: `echo \"HUGGINGFACE_TOKEN=hf_xxxxx\" > .env`\n",
    "2. Open this notebook\n",
    "3. Run All Cells\n",
    "\n",
    "**Everything is self-contained - no external modules required!**\n"
]))

# ============================================================================
# SECTION 1: CONFIGURATION
# ============================================================================

cells.append(mk_md("## 1. Configuration (All Settings Here)"))

cells.append(mk_code([
    "\"\"\"Configuration - Modify all settings here.\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# PATHS\n",
    "# ============================================================================\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'data_new'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "RESULTS_DIR = MODELS_DIR / 'results'\n",
    "\n",
    "# Data files\n",
    "INCIDENT_DATA_FILE = 'SNow_incident_ticket_data.csv'\n",
    "TEST_PAIRS_FILE = 'fixed_test_pairs.json'\n",
    "\n",
    "# ============================================================================\n",
    "# MODELS\n",
    "# ============================================================================\n",
    "\n",
    "# Baseline model\n",
    "BASELINE_MODEL = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "# Fine-tuned models to evaluate\n",
    "FINETUNED_MODELS = [\n",
    "    'v6_refactored_finetuned/v6_refactored_finetuned_20251204_1424',\n",
    "    'real_servicenow_finetuned_mpnet/real_servicenow_v2_20251210_1939',\n",
    "    'real_servicenow_finetuned_mpnet_lora',\n",
    "]\n",
    "\n",
    "# Additional baseline models\n",
    "ADDITIONAL_BASELINES = [\n",
    "    {'name': 'Nomic-Embed-v1.5', 'model_id': 'nomic-ai/nomic-embed-text-v1.5', \n",
    "     'trust_remote_code': True, 'batch_size': 8},\n",
    "    {'name': 'BGE-base-en-v1.5', 'model_id': 'BAAI/bge-base-en-v1.5', \n",
    "     'trust_remote_code': False, 'batch_size': 32},\n",
    "    {'name': 'GTE-base-en-v1.5', 'model_id': 'Alibaba-NLP/gte-base-en-v1.5', \n",
    "     'trust_remote_code': True, 'batch_size': 32},\n",
    "    {'name': 'E5-base-v2', 'model_id': 'intfloat/e5-base-v2', \n",
    "     'trust_remote_code': False, 'batch_size': 32},\n",
    "    {'name': 'MiniLM-L12-v2', 'model_id': 'sentence-transformers/all-MiniLM-L12-v2', \n",
    "     'trust_remote_code': False, 'batch_size': 32},\n",
    "    {'name': 'JinaBERT-v2-base', 'model_id': 'jinaai/jina-embeddings-v2-base-en', \n",
    "     'trust_remote_code': True, 'batch_size': 8},\n",
    "    {'name': 'UAE-Large-v1', 'model_id': 'WhereIsAI/UAE-Large-V1', \n",
    "     'trust_remote_code': True, 'batch_size': 32},\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION SETTINGS\n",
    "# ============================================================================\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "NUM_THRESHOLDS = 100  # For optimal F1 search\n",
    "MIN_TEXT_LENGTH = 10  # Filter short texts\n",
    "\n",
    "# ============================================================================\n",
    "# MEMORY & DEVICE\n",
    "# ============================================================================\n",
    "\n",
    "AUTO_DETECT_DEVICE = True\n",
    "ENABLE_EXPANDABLE_SEGMENTS = True  # Prevent GPU memory fragmentation\n",
    "\n",
    "# Batch size config (auto-adjusted based on GPU memory)\n",
    "BATCH_SIZE_GPU_THRESHOLDS = {\n",
    "    24: 128,  # >= 24GB VRAM\n",
    "    16: 64,   # >= 16GB VRAM\n",
    "    8: 32,    # >= 8GB VRAM\n",
    "    4: 16,    # >= 4GB VRAM\n",
    "}\n",
    "BATCH_SIZE_MPS = 8  # Apple Silicon\n",
    "BATCH_SIZE_DEFAULT = 32\n",
    "\n",
    "# ============================================================================\n",
    "# DIAGNOSTICS\n",
    "# ============================================================================\n",
    "\n",
    "ENABLE_ERROR_ANALYSIS = True\n",
    "NUM_ERROR_EXAMPLES = 5\n",
    "\n",
    "ENABLE_BENCHMARKING = True\n",
    "NUM_BENCHMARK_RUNS = 3\n",
    "\n",
    "print('Configuration loaded')\n",
    "print(f'  Data directory: {DATA_DIR}')\n",
    "print(f'  Baseline model: {BASELINE_MODEL}')\n",
    "print(f'  Additional baselines: {len(ADDITIONAL_BASELINES)}')\n",
    "print(f'  Random seed: {RANDOM_SEED}')\n"
]))

# ============================================================================
# SECTION 2: UTILITY FUNCTIONS
# ============================================================================

cells.append(mk_md("## 2. Utility Functions"))

cells.append(mk_code([
    "\"\"\"Utility functions for evaluation.\"\"\"\n",
    "\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, Tuple, List\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY\n",
    "# ============================================================================\n",
    "\n",
    "def set_all_seeds(seed: int = 42):\n",
    "    \"\"\"Set all random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    import torch\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "    \n",
    "    print(f'All seeds set to {seed}')\n",
    "\n",
    "# ============================================================================\n",
    "# DEVICE & MEMORY\n",
    "# ============================================================================\n",
    "\n",
    "def detect_device():\n",
    "    \"\"\"Auto-detect best available device.\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        print(f'Using CUDA: {torch.cuda.get_device_name(0)}')\n",
    "        print(f'  GPU Memory: {gpu_memory:.2f} GB')\n",
    "        return device, gpu_memory\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print('Using Apple MPS')\n",
    "        return device, None\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print('Using CPU (no GPU detected)')\n",
    "        return device, None\n",
    "\n",
    "def get_batch_size(device: str, gpu_memory_gb: float = None) -> int:\n",
    "    \"\"\"Get appropriate batch size based on device and GPU memory.\"\"\"\n",
    "    if device == 'mps':\n",
    "        return BATCH_SIZE_MPS\n",
    "    \n",
    "    if device == 'cuda' and gpu_memory_gb is not None:\n",
    "        for threshold, batch_size in sorted(BATCH_SIZE_GPU_THRESHOLDS.items(), reverse=True):\n",
    "            if gpu_memory_gb >= threshold:\n",
    "                return batch_size\n",
    "    \n",
    "    return BATCH_SIZE_DEFAULT\n",
    "\n",
    "def cleanup_gpu_memory(device: str = None):\n",
    "    \"\"\"Clean up GPU/MPS memory.\"\"\"\n",
    "    import torch\n",
    "    gc.collect()\n",
    "    \n",
    "    if device == 'cuda' or (device is None and torch.cuda.is_available()):\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    elif device == 'mps' or (device is None and torch.backends.mps.is_available()):\n",
    "        torch.mps.empty_cache()\n",
    "        torch.mps.synchronize()\n",
    "\n",
    "def get_gpu_memory_usage(device: str = 'cuda') -> Dict[str, float]:\n",
    "    \"\"\"Get current GPU memory usage in GB.\"\"\"\n",
    "    import torch\n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        return {\n",
    "            'allocated': torch.cuda.memory_allocated() / (1024**3),\n",
    "            'reserved': torch.cuda.memory_reserved() / (1024**3),\n",
    "            'max_allocated': torch.cuda.max_memory_allocated() / (1024**3),\n",
    "        }\n",
    "    return {'allocated': 0, 'reserved': 0, 'max_allocated': 0}\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "def validate_test_pairs(texts1, texts2, labels):\n",
    "    \"\"\"Validate test pair data.\"\"\"\n",
    "    assert len(texts1) == len(texts2) == len(labels), \"Mismatched lengths\"\n",
    "    assert len(texts1) > 0, \"Empty test set\"\n",
    "    assert set(labels).issubset({0, 1}), \"Labels must be binary (0/1)\"\n",
    "    \n",
    "    num_pos = sum(labels)\n",
    "    num_neg = len(labels) - num_pos\n",
    "    \n",
    "    print(f'Test pairs validated:')\n",
    "    print(f'  Total: {len(labels)}')\n",
    "    print(f'  Positive: {num_pos} ({num_pos/len(labels)*100:.1f}%)')\n",
    "    print(f'  Negative: {num_neg} ({num_neg/len(labels)*100:.1f}%)')\n",
    "\n",
    "# ============================================================================\n",
    "# METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_metrics(labels: np.ndarray, scores: np.ndarray, threshold: float = 0.5) -> Dict[str, float]:\n",
    "    \"\"\"Compute all evaluation metrics.\"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    predictions = (scores >= threshold).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'spearman': spearmanr(labels, scores)[0] if len(set(scores)) > 1 else 0.0,\n",
    "        'roc_auc': roc_auc_score(labels, scores) if len(set(labels)) > 1 else 0.0,\n",
    "        'f1': f1_score(labels, predictions),\n",
    "        'precision': precision_score(labels, predictions, zero_division=0),\n",
    "        'recall': recall_score(labels, predictions, zero_division=0),\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "    }\n",
    "\n",
    "def find_optimal_threshold(labels: np.ndarray, scores: np.ndarray, num_thresholds: int = 100) -> Tuple[float, float]:\n",
    "    \"\"\"Find threshold that maximizes F1 score.\"\"\"\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    thresholds = np.linspace(scores.min(), scores.max(), num_thresholds)\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = (scores >= threshold).astype(int)\n",
    "        f1 = f1_score(labels, predictions, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_f1\n",
    "\n",
    "def compute_confusion_matrix(labels: np.ndarray, scores: np.ndarray, threshold: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"Compute confusion matrix.\"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    predictions = (scores >= threshold).astype(int)\n",
    "    return confusion_matrix(labels, predictions)\n",
    "\n",
    "# ============================================================================\n",
    "# ERROR ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_errors(texts1, texts2, labels, scores, threshold=0.5, num_examples=5):\n",
    "    \"\"\"Analyze false positives and false negatives.\"\"\"\n",
    "    predictions = (scores >= threshold).astype(int)\n",
    "    \n",
    "    fp_mask = (predictions == 1) & (labels == 0)\n",
    "    fn_mask = (predictions == 0) & (labels == 1)\n",
    "    \n",
    "    fp_indices = np.where(fp_mask)[0]\n",
    "    fn_indices = np.where(fn_mask)[0]\n",
    "    \n",
    "    result = {\n",
    "        'false_positives': {\n",
    "            'count': len(fp_indices),\n",
    "            'percentage': len(fp_indices) / len(labels) * 100,\n",
    "            'mean_score': scores[fp_mask].mean() if len(fp_indices) > 0 else 0,\n",
    "            'std_score': scores[fp_mask].std() if len(fp_indices) > 0 else 0,\n",
    "            'examples': []\n",
    "        },\n",
    "        'false_negatives': {\n",
    "            'count': len(fn_indices),\n",
    "            'percentage': len(fn_indices) / len(labels) * 100,\n",
    "            'mean_score': scores[fn_mask].mean() if len(fn_indices) > 0 else 0,\n",
    "            'std_score': scores[fn_mask].std() if len(fn_indices) > 0 else 0,\n",
    "            'examples': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add examples\n",
    "    for idx in fp_indices[:num_examples]:\n",
    "        result['false_positives']['examples'].append({\n",
    "            'text1': texts1[idx][:100],\n",
    "            'text2': texts2[idx][:100],\n",
    "            'score': float(scores[idx]),\n",
    "        })\n",
    "    \n",
    "    for idx in fn_indices[:num_examples]:\n",
    "        result['false_negatives']['examples'].append({\n",
    "            'text1': texts1[idx][:100],\n",
    "            'text2': texts2[idx][:100],\n",
    "            'score': float(scores[idx]),\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ============================================================================\n",
    "# BENCHMARKING\n",
    "# ============================================================================\n",
    "\n",
    "def benchmark_inference_speed(model, texts, batch_size=32, num_runs=3, device='cuda'):\n",
    "    \"\"\"Benchmark model inference speed.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    # Warmup\n",
    "    _ = model.encode(texts[:min(10, len(texts))], batch_size=batch_size, show_progress_bar=False)\n",
    "    cleanup_gpu_memory(device)\n",
    "    \n",
    "    # Timed runs\n",
    "    for _ in range(num_runs):\n",
    "        start = time.perf_counter()\n",
    "        _ = model.encode(texts, batch_size=batch_size, show_progress_bar=False)\n",
    "        elapsed = (time.perf_counter() - start) * 1000  # ms\n",
    "        times.append(elapsed)\n",
    "        cleanup_gpu_memory(device)\n",
    "    \n",
    "    return {\n",
    "        'total_ms_mean': float(np.mean(times)),\n",
    "        'total_ms_std': float(np.std(times)),\n",
    "        'per_sample_ms_mean': float(np.mean(times) / len(texts)),\n",
    "        'throughput_samples_per_sec': float(len(texts) / (np.mean(times) / 1000)),\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# METADATA\n",
    "# ============================================================================\n",
    "\n",
    "def get_environment_metadata():\n",
    "    \"\"\"Get environment metadata for reproducibility.\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    metadata = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'python_version': sys.version.split()[0],\n",
    "        'torch_version': torch.__version__,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        import transformers\n",
    "        metadata['transformers_version'] = transformers.__version__\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        import sentence_transformers\n",
    "        metadata['sentence_transformers_version'] = sentence_transformers.__version__\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        metadata['cuda_available'] = True\n",
    "        metadata['cuda_version'] = torch.version.cuda\n",
    "        metadata['device_name'] = torch.cuda.get_device_name(0)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        metadata['mps_available'] = True\n",
    "        metadata['device_name'] = 'Apple MPS'\n",
    "    else:\n",
    "        metadata['device_name'] = 'CPU'\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def print_environment_info():\n",
    "    \"\"\"Print environment information.\"\"\"\n",
    "    metadata = get_environment_metadata()\n",
    "    print('='*80)\n",
    "    print('ENVIRONMENT INFO')\n",
    "    print('='*80)\n",
    "    for key, value in metadata.items():\n",
    "        print(f'  {key:30s}: {value}')\n",
    "    print('='*80)\n",
    "\n",
    "print('Utility functions loaded')\n"
]))

# ============================================================================
# SECTION 3: IMPORTS & SETUP
# ============================================================================

cells.append(mk_md("## 3. Imports & Setup"))

cells.append(mk_code([
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML imports\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('All imports successful')\n"
]))

cells.append(mk_code([
    "# Set all seeds for reproducibility\n",
    "set_all_seeds(RANDOM_SEED)\n",
    "\n",
    "# Detect device\n",
    "device, gpu_memory = detect_device()\n",
    "\n",
    "# Configure memory (prevent fragmentation)\n",
    "if ENABLE_EXPANDABLE_SEGMENTS and device == 'cuda':\n",
    "    import os\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    print('Enabled expandable memory segments')\n",
    "\n",
    "# Get recommended batch size\n",
    "batch_size = get_batch_size(device, gpu_memory)\n",
    "print(f'\\nRecommended batch size: {batch_size}')\n",
    "\n",
    "# Print environment\n",
    "print_environment_info()\n"
]))

# ============================================================================
# SECTION 4: DATA LOADING
# ============================================================================

cells.append(mk_md("## 4. Load Test Data"))

cells.append(mk_code([
    "# Load test pairs\n",
    "test_pairs_path = DATA_DIR / TEST_PAIRS_FILE\n",
    "print(f'Loading: {test_pairs_path}')\n",
    "\n",
    "with open(test_pairs_path, 'r', encoding='utf-8') as f:\n",
    "    test_pairs = json.load(f)\n",
    "\n",
    "# Extract data\n",
    "test_texts1 = [p['text1'] for p in test_pairs]\n",
    "test_texts2 = [p['text2'] for p in test_pairs]\n",
    "test_labels = np.array([p['label'] for p in test_pairs])\n",
    "\n",
    "# Validate\n",
    "validate_test_pairs(test_texts1, test_texts2, test_labels)\n"
]))

# ============================================================================
# SECTION 5: MODEL EVALUATOR
# ============================================================================

cells.append(mk_md("## 5. Model Evaluator Class"))

cells.append(mk_code([
    "class ModelEvaluatorV2:\n",
    "    \"\"\"Enhanced model evaluator with comprehensive diagnostics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def evaluate_model(self, model, model_name, texts1, texts2, labels, batch_size=32, device='cuda', verbose=True):\n",
    "        \"\"\"Evaluate model with comprehensive metrics.\"\"\"\n",
    "        if verbose:\n",
    "            print(f'\\n{\"=\"*80}')\n",
    "            print(f'EVALUATING: {model_name}')\n",
    "            print(f'{\"=\"*80}')\n",
    "        \n",
    "        # Encode\n",
    "        emb1 = model.encode(texts1, batch_size=batch_size, show_progress_bar=verbose, device=device)\n",
    "        emb2 = model.encode(texts2, batch_size=batch_size, show_progress_bar=verbose, device=device)\n",
    "        \n",
    "        # Similarity\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        scores = np.array([cosine_similarity([emb1[i]], [emb2[i]])[0,0] for i in range(len(emb1))])\n",
    "        \n",
    "        # Optimal threshold\n",
    "        best_threshold, best_f1 = find_optimal_threshold(labels, scores, NUM_THRESHOLDS)\n",
    "        \n",
    "        # Metrics\n",
    "        metrics = compute_metrics(labels, scores, best_threshold)\n",
    "        metrics['best_threshold'] = best_threshold\n",
    "        metrics['best_f1'] = best_f1\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = compute_confusion_matrix(labels, scores, best_threshold)\n",
    "        metrics['confusion_matrix'] = cm.tolist()\n",
    "        \n",
    "        # Store\n",
    "        self.results[model_name] = {\n",
    "            'metrics': metrics,\n",
    "            'cosine_scores': scores,\n",
    "            'labels': labels,\n",
    "            'embeddings1': emb1,\n",
    "            'embeddings2': emb2,\n",
    "        }\n",
    "        \n",
    "        # Print\n",
    "        if verbose:\n",
    "            print(f'\\nMetrics (threshold={best_threshold:.4f}):')\n",
    "            print(f'  Spearman:  {metrics[\"spearman\"]:.4f}')\n",
    "            print(f'  ROC-AUC:   {metrics[\"roc_auc\"]:.4f}')\n",
    "            print(f'  F1:        {metrics[\"f1\"]:.4f}')\n",
    "            print(f'  Precision: {metrics[\"precision\"]:.4f}')\n",
    "            print(f'  Recall:    {metrics[\"recall\"]:.4f}')\n",
    "            print(f'  Accuracy:  {metrics[\"accuracy\"]:.4f}')\n",
    "            print(f'\\nConfusion Matrix:')\n",
    "            print(f'  TN={cm[0,0]:4d}  FP={cm[0,1]:4d}')\n",
    "            print(f'  FN={cm[1,0]:4d}  TP={cm[1,1]:4d}')\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def get_metrics_df(self):\n",
    "        \"\"\"Get metrics as DataFrame.\"\"\"\n",
    "        data = {name: result['metrics'] for name, result in self.results.items()}\n",
    "        df = pd.DataFrame(data).T\n",
    "        return df.sort_values('spearman', ascending=False)\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = ModelEvaluatorV2()\n",
    "print('Model evaluator initialized')\n"
]))

# ============================================================================
# SECTION 6: HUGGINGFACE AUTH (SECURE)
# ============================================================================

cells.append(mk_md([
    "## 6. HuggingFace Authentication (SECURE)\n",
    "\n",
    "**Security Fix:** No hardcoded tokens!\n",
    "\n",
    "**Setup:**\n",
    "1. Create `.env` file: `echo \"HUGGINGFACE_TOKEN=hf_xxxxx\" > .env`\n",
    "2. Add to `.gitignore`: `echo \".env\" >> .gitignore`\n",
    "3. Run cell below\n",
    "\n",
    "**Alternative:** Uncomment `login()` for interactive authentication.\n"
]))

cells.append(mk_code([
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load from .env file\n",
    "load_dotenv()\n",
    "load_dotenv(Path.home() / '.env')  # Also try home directory\n",
    "\n",
    "token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "if token:\n",
    "    login(token=token)\n",
    "    print('Authenticated with HuggingFace')\n",
    "else:\n",
    "    print('No token found in environment')\n",
    "    print('To authenticate:')\n",
    "    print('  1. Create .env file with HUGGINGFACE_TOKEN=hf_xxxxx')\n",
    "    print('  2. Or uncomment the line below for interactive login')\n",
    "    # login()  # Uncomment for interactive authentication\n"
]))

# ============================================================================
# SECTION 7: BASELINE EVALUATION
# ============================================================================

cells.append(mk_md("## 7. Evaluate Baseline Model"))

cells.append(mk_code([
    "# Load baseline model\n",
    "print(f'Loading baseline: {BASELINE_MODEL}')\n",
    "baseline_model = SentenceTransformer(BASELINE_MODEL, device=device)\n",
    "\n",
    "# Evaluate\n",
    "evaluator.evaluate_model(\n",
    "    model=baseline_model,\n",
    "    model_name='Baseline (Raw MPNet)',\n",
    "    texts1=test_texts1,\n",
    "    texts2=test_texts2,\n",
    "    labels=test_labels,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Clean up\n",
    "del baseline_model\n",
    "cleanup_gpu_memory(device)\n",
    "print('\\nBaseline evaluation complete, memory cleaned')\n"
]))

# ============================================================================
# SECTION 8: ADDITIONAL BASELINES
# ============================================================================

cells.append(mk_md([
    "## 8. Evaluate Additional Baseline Models\n",
    "\n",
    "Test modern high-performance models that may outperform MPNet baseline.\n"
]))

cells.append(mk_code([
    "print('='*80)\n",
    "print('EVALUATING ADDITIONAL BASELINE MODELS')\n",
    "print('='*80)\n",
    "print(f'\\nTesting {len(ADDITIONAL_BASELINES)} models...\\n')\n",
    "\n",
    "successful = []\n",
    "failed = []\n",
    "\n",
    "for model_config in ADDITIONAL_BASELINES:\n",
    "    model_name = model_config['name']\n",
    "    model_id = model_config['model_id']\n",
    "    trust_remote = model_config['trust_remote_code']\n",
    "    model_batch_size = model_config.get('batch_size', batch_size)\n",
    "    \n",
    "    print(f'\\n{\"-\"*80}')\n",
    "    print(f'Testing: {model_name}')\n",
    "    print(f'Model ID: {model_id}')\n",
    "    print(f'{\"-\"*80}')\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        print('Loading model...')\n",
    "        model = SentenceTransformer(model_id, device=device, trust_remote_code=trust_remote)\n",
    "        print('  Model loaded')\n",
    "        \n",
    "        # Evaluate\n",
    "        results = evaluator.evaluate_model(\n",
    "            model=model,\n",
    "            model_name=f'Baseline ({model_name})',\n",
    "            texts1=test_texts1,\n",
    "            texts2=test_texts2,\n",
    "            labels=test_labels,\n",
    "            batch_size=model_batch_size,\n",
    "            device=device,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        successful.append(model_name)\n",
    "        \n",
    "        # Compare to MPNet\n",
    "        mpnet_baseline = evaluator.results.get('Baseline (Raw MPNet)', {})\n",
    "        if mpnet_baseline:\n",
    "            mpnet_spearman = mpnet_baseline['metrics']['spearman']\n",
    "            model_spearman = results['spearman']\n",
    "            delta = model_spearman - mpnet_spearman\n",
    "            pct = (delta / mpnet_spearman * 100) if mpnet_spearman else 0\n",
    "            \n",
    "            print(f'\\n  vs MPNet baseline:')\n",
    "            print(f'    MPNet:  {mpnet_spearman:.4f}')\n",
    "            print(f'    {model_name}: {model_spearman:.4f}')\n",
    "            print(f'    Delta:  {delta:+.4f} ({pct:+.1f}%)')\n",
    "            \n",
    "            if delta > 0.01:\n",
    "                print(f'    BETTER than MPNet!')\n",
    "            elif delta > -0.01:\n",
    "                print(f'    Similar to MPNet')\n",
    "            else:\n",
    "                print(f'    Worse than MPNet')\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        cleanup_gpu_memory(device)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'\\n  Error: {e}')\n",
    "        print(f'  Skipping {model_name}...')\n",
    "        failed.append((model_name, str(e)))\n",
    "        continue\n",
    "\n",
    "# Summary\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print('ADDITIONAL BASELINES COMPLETE')\n",
    "print(f'{\"=\"*80}')\n",
    "print(f'\\nSuccessful: {len(successful)}/{len(ADDITIONAL_BASELINES)}')\n",
    "for m in successful:\n",
    "    print(f'  {m}')\n",
    "\n",
    "if failed:\n",
    "    print(f'\\nFailed: {len(failed)}')\n",
    "    for m, err in failed:\n",
    "        print(f'  {m}: {err[:60]}...')\n"
]))

# ============================================================================
# SECTION 9: FINE-TUNED MODELS
# ============================================================================

cells.append(mk_md("## 9. Evaluate Fine-Tuned Models"))

cells.append(mk_code([
    "print('='*80)\n",
    "print('EVALUATING FINE-TUNED MODELS')\n",
    "print('='*80)\n",
    "\n",
    "for model_path in FINETUNED_MODELS:\n",
    "    full_path = MODELS_DIR / model_path\n",
    "    model_name = model_path.split('/')[-1]\n",
    "    \n",
    "    if not full_path.exists():\n",
    "        print(f'\\nModel not found: {full_path}')\n",
    "        continue\n",
    "    \n",
    "    print(f'\\nEvaluating: {model_name}')\n",
    "    \n",
    "    try:\n",
    "        # Try PEFT adapter first\n",
    "        try:\n",
    "            from peft import PeftModel\n",
    "            base_model = SentenceTransformer(BASELINE_MODEL, device=device)\n",
    "            model = PeftModel.from_pretrained(base_model, str(full_path))\n",
    "            print('  Loaded as PEFT model')\n",
    "        except:\n",
    "            # Fallback to standard\n",
    "            model = SentenceTransformer(str(full_path), device=device)\n",
    "            print('  Loaded as standard model')\n",
    "        \n",
    "        # Evaluate\n",
    "        evaluator.evaluate_model(\n",
    "            model=model,\n",
    "            model_name=model_name,\n",
    "            texts1=test_texts1,\n",
    "            texts2=test_texts2,\n",
    "            labels=test_labels,\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        cleanup_gpu_memory(device)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'  Error: {e}')\n",
    "        continue\n",
    "\n",
    "print('\\nFine-tuned model evaluation complete')\n"
]))

# ============================================================================
# SECTION 10: RESULTS COMPARISON
# ============================================================================

cells.append(mk_md("## 10. Results Comparison"))

cells.append(mk_code([
    "# Get metrics DataFrame\n",
    "metrics_df = evaluator.get_metrics_df()\n",
    "display_df = metrics_df.drop(columns=['confusion_matrix'], errors='ignore')\n",
    "\n",
    "print('='*80)\n",
    "print('MODEL COMPARISON (sorted by Spearman)')\n",
    "print('='*80)\n",
    "print(display_df.to_string())\n",
    "\n",
    "# Best per metric\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print('BEST MODELS PER METRIC')\n",
    "print(f'{\"=\"*80}')\n",
    "\n",
    "for metric in ['spearman', 'roc_auc', 'f1', 'precision', 'recall']:\n",
    "    if metric in display_df.columns:\n",
    "        best_model = display_df[metric].idxmax()\n",
    "        best_value = display_df[metric].max()\n",
    "        print(f'{metric:15s}: {best_model:40s} ({best_value:.4f})')\n"
]))

# ============================================================================
# SECTION 11: CONFUSION MATRICES (NEW!)
# ============================================================================

cells.append(mk_md("## 11. Confusion Matrix Analysis (NEW!)"))

cells.append(mk_code([
    "import math\n",
    "\n",
    "num_models = len(evaluator.results)\n",
    "if num_models == 0:\n",
    "    print('No models to visualize')\n",
    "else:\n",
    "    ncols = min(3, num_models)\n",
    "    nrows = math.ceil(num_models / ncols)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*5, nrows*4))\n",
    "    axes = np.atleast_1d(axes).flatten()\n",
    "    \n",
    "    for idx, (model_name, result) in enumerate(evaluator.results.items()):\n",
    "        ax = axes[idx]\n",
    "        cm = np.array(result['metrics']['confusion_matrix'])\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues',\n",
    "                    xticklabels=['Negative', 'Positive'],\n",
    "                    yticklabels=['Negative', 'Positive'],\n",
    "                    cbar=False)\n",
    "        \n",
    "        ax.set_title(f'{model_name}', fontweight='bold', fontsize=10)\n",
    "        ax.set_ylabel('True Label')\n",
    "        ax.set_xlabel('Predicted Label')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(num_models, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\nConfusion matrices for {num_models} models')\n"
]))

# ============================================================================
# SECTION 12: ERROR ANALYSIS (NEW!)
# ============================================================================

cells.append(mk_md("## 12. Error Analysis (NEW!)"))

cells.append(mk_code([
    "if ENABLE_ERROR_ANALYSIS:\n",
    "    print('='*80)\n",
    "    print('ERROR ANALYSIS')\n",
    "    print('='*80)\n",
    "    \n",
    "    for model_name, result in evaluator.results.items():\n",
    "        print(f'\\n{\"-\"*80}')\n",
    "        print(f'{model_name}')\n",
    "        print(f'{\"-\"*80}')\n",
    "        \n",
    "        # Analyze errors\n",
    "        errors = analyze_errors(\n",
    "            test_texts1,\n",
    "            test_texts2,\n",
    "            result['labels'],\n",
    "            result['cosine_scores'],\n",
    "            threshold=result['metrics']['best_threshold'],\n",
    "            num_examples=NUM_ERROR_EXAMPLES\n",
    "        )\n",
    "        \n",
    "        fp = errors['false_positives']\n",
    "        fn = errors['false_negatives']\n",
    "        \n",
    "        print(f'\\nFalse Positives: {fp[\"count\"]} ({fp[\"percentage\"]:.1f}%)')\n",
    "        print(f'  Mean score: {fp[\"mean_score\"]:.4f} ± {fp[\"std_score\"]:.4f}')\n",
    "        print(f'  Examples (predicted similar, actually different):')\n",
    "        for i, ex in enumerate(fp['examples'][:3], 1):\n",
    "            print(f'    {i}. Score={ex[\"score\"]:.4f}')\n",
    "            print(f'       Text1: {ex[\"text1\"]}...')\n",
    "            print(f'       Text2: {ex[\"text2\"]}...')\n",
    "        \n",
    "        print(f'\\nFalse Negatives: {fn[\"count\"]} ({fn[\"percentage\"]:.1f}%)')\n",
    "        print(f'  Mean score: {fn[\"mean_score\"]:.4f} ± {fn[\"std_score\"]:.4f}')\n",
    "        print(f'  Examples (predicted different, actually similar):')\n",
    "        for i, ex in enumerate(fn['examples'][:3], 1):\n",
    "            print(f'    {i}. Score={ex[\"score\"]:.4f}')\n",
    "            print(f'       Text1: {ex[\"text1\"]}...')\n",
    "            print(f'       Text2: {ex[\"text2\"]}...')\n",
    "else:\n",
    "    print('Error analysis disabled in configuration')\n"
]))

# ============================================================================
# SECTION 13: VISUALIZATIONS
# ============================================================================

cells.append(mk_md("## 13. Visualizations"))

cells.append(mk_code([
    "# ROC Curves\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name, result in evaluator.results.items():\n",
    "    fpr, tpr, _ = roc_curve(result['labels'], result['cosine_scores'])\n",
    "    auc = result['metrics']['roc_auc']\n",
    "    ax.plot(fpr, tpr, label=f\"{model_name} (AUC={auc:.4f})\", linewidth=2)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves - All Models', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('ROC curves plotted')\n"
]))

cells.append(mk_code([
    "# Precision-Recall Curves\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name, result in evaluator.results.items():\n",
    "    precision, recall, _ = precision_recall_curve(result['labels'], result['cosine_scores'])\n",
    "    f1 = result['metrics']['f1']\n",
    "    ax.plot(recall, precision, label=f\"{model_name} (F1={f1:.4f})\", linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curves - All Models', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('PR curves plotted')\n"
]))

cells.append(mk_code([
    "# Score Distributions\n",
    "num_models = len(evaluator.results)\n",
    "ncols = min(2, num_models)\n",
    "nrows = math.ceil(num_models / ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*7, nrows*4))\n",
    "axes = np.atleast_1d(axes).flatten()\n",
    "\n",
    "for idx, (model_name, result) in enumerate(evaluator.results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    scores = result['cosine_scores']\n",
    "    labels = result['labels']\n",
    "    threshold = result['metrics']['best_threshold']\n",
    "    \n",
    "    # Plot distributions\n",
    "    ax.hist(scores[labels == 1], bins=50, alpha=0.7, label='Positive pairs',\n",
    "            color='green', density=True)\n",
    "    ax.hist(scores[labels == 0], bins=50, alpha=0.7, label='Negative pairs',\n",
    "            color='red', density=True)\n",
    "    ax.axvline(threshold, color='black', linestyle='--', linewidth=2,\n",
    "               label=f'Threshold={threshold:.3f}')\n",
    "    \n",
    "    ax.set_xlabel('Cosine Similarity Score', fontsize=10)\n",
    "    ax.set_ylabel('Density', fontsize=10)\n",
    "    ax.set_title(f'{model_name}', fontsize=11, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(num_models, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Score distributions plotted')\n"
]))

# ============================================================================
# SECTION 14: EXPORT RESULTS
# ============================================================================

cells.append(mk_md("## 14. Export Results"))

cells.append(mk_code([
    "# Create timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Create output directory\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save metrics CSV\n",
    "metrics_csv_path = RESULTS_DIR / f'model_evaluation_{timestamp}.csv'\n",
    "metrics_df.to_csv(metrics_csv_path)\n",
    "print(f'Saved metrics: {metrics_csv_path}')\n",
    "\n",
    "# Save detailed results JSON\n",
    "results_dict = {}\n",
    "for model_name, result in evaluator.results.items():\n",
    "    results_dict[model_name] = result['metrics']  # Don't save embeddings (too large)\n",
    "\n",
    "results_json_path = RESULTS_DIR / f'evaluation_results_{timestamp}.json'\n",
    "with open(results_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_dict, f, indent=2, default=str)\n",
    "print(f'Saved results: {results_json_path}')\n",
    "\n",
    "# Save environment metadata\n",
    "metadata = get_environment_metadata()\n",
    "metadata['evaluation_config'] = {\n",
    "    'test_pairs_file': str(TEST_PAIRS_FILE),\n",
    "    'num_test_pairs': len(test_labels),\n",
    "    'baseline_model': BASELINE_MODEL,\n",
    "    'num_models_evaluated': len(evaluator.results),\n",
    "    'random_seed': RANDOM_SEED,\n",
    "}\n",
    "\n",
    "metadata_path = RESULTS_DIR / f'evaluation_metadata_{timestamp}.json'\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "print(f'Saved metadata: {metadata_path}')\n",
    "\n",
    "print(f'\\nAll results exported to: {RESULTS_DIR}')\n"
]))

# ============================================================================
# SECTION 15: SUMMARY
# ============================================================================

cells.append(mk_md("## 15. Summary"))

cells.append(mk_code([
    "print('='*80)\n",
    "print('EVALUATION SUMMARY')\n",
    "print('='*80)\n",
    "\n",
    "# Best model overall\n",
    "best_model = metrics_df['spearman'].idxmax()\n",
    "best_spearman = metrics_df['spearman'].max()\n",
    "\n",
    "print(f'\\nBest Model (by Spearman): {best_model}')\n",
    "print(f'   Spearman: {best_spearman:.4f}')\n",
    "\n",
    "# Compare to baseline\n",
    "baseline_spearman = evaluator.results['Baseline (Raw MPNet)']['metrics']['spearman']\n",
    "improvement = best_spearman - baseline_spearman\n",
    "improvement_pct = (improvement / baseline_spearman * 100) if baseline_spearman else 0\n",
    "\n",
    "print(f'\\nComparison to Baseline:')\n",
    "print(f'   Baseline (MPNet): {baseline_spearman:.4f}')\n",
    "print(f'   Best model:       {best_spearman:.4f}')\n",
    "print(f'   Improvement:      {improvement:+.4f} ({improvement_pct:+.1f}%)')\n",
    "\n",
    "if improvement > 0.01:\n",
    "    print('   SIGNIFICANT improvement over baseline!')\n",
    "elif improvement > -0.01:\n",
    "    print('   Similar performance to baseline')\n",
    "else:\n",
    "    print('   Below baseline performance')\n",
    "\n",
    "# Top 3 models\n",
    "print(f'\\nTop 3 Models (by Spearman):')\n",
    "for i, (model_name, row) in enumerate(metrics_df.head(3).iterrows(), 1):\n",
    "    print(f'   {i}. {model_name:40s} (Spearman: {row[\"spearman\"]:.4f})')\n",
    "\n",
    "# Recommendations\n",
    "print(f'\\nRecommendations:')\n",
    "if best_spearman >= 0.80:\n",
    "    print('   Model ready for production (Spearman >= 0.80)')\n",
    "elif best_spearman >= 0.70:\n",
    "    print('   Model decent but could be improved (0.70 <= Spearman < 0.80)')\n",
    "else:\n",
    "    print('   Model needs significant improvement (Spearman < 0.70)')\n",
    "    print('   Consider:')\n",
    "    print('     - More training data')\n",
    "    print('     - Different model architecture')\n",
    "    print('     - Better hard negative mining')\n",
    "    print('     - Curriculum learning approach')\n",
    "\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print('EVALUATION COMPLETE')\n",
    "print(f'{\"=\"*80}')\n"
]))

# Save notebook
with open('evaluate_model_v2.ipynb', 'w', encoding='utf-8') as f:
    json.dump(nb, f, indent=1, ensure_ascii=False)

print(f"\nCreated evaluate_model_v2.ipynb (STANDALONE)")
print(f"  Total cells: {len(nb['cells'])}")
print(f"  Code cells: {sum(1 for c in nb['cells'] if c['cell_type']=='code')}")
print(f"  Markdown cells: {sum(1 for c in nb['cells'] if c['cell_type']=='markdown')}")
print(f"\nEverything is self-contained!")
print(f"Only external dependency: .env file for HuggingFace token")

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184dfd2c",
   "metadata": {
    "description": "Setup: Environment variables & package installation"
   },
   "source": [
    "# üöÄ ITSM Ticket Similarity - Real ServiceNow Data Training (V2 + LoRA/PEFT)\n",
    "\n",
    "**Model:** `sentence-transformers/all-mpnet-base-v2` (768-dim embeddings) **+ LoRA adapters**  \n",
    "**Data:** Real ServiceNow incidents (Mar 2024 ‚Üí Sep 2025, ~30K+ tickets)  \n",
    "**Use Case:** Find similar tickets, detect duplicates, assist routing\n",
    "\n",
    "## üÜï LoRA/PEFT Improvements (This Version)\n",
    "\n",
    "### 1. **Parameter-Efficient Fine-Tuning (LoRA)**\n",
    "- **90%+ parameter reduction**: Only ~3-5M trainable params vs ~30-50M full fine-tune\n",
    "- **LoRA rank**: 16, alpha: 32, targeting attention Q/V projections\n",
    "- **Faster training**: Reduced memory footprint and faster gradient updates\n",
    "- **Better generalization**: Less overfitting on domain-specific data\n",
    "- **Learning rate**: 1e-4 (optimized for LoRA vs 2e-5 for full fine-tune)\n",
    "\n",
    "### 2. **Multi-Loss Training**\n",
    "- **MultipleNegativesRankingLoss (90%)**: Primary loss for ranking/similarity ordering\n",
    "  - Leverages in-batch negatives for efficient contrastive learning\n",
    "  - Optimizes embedding space for relative similarity\n",
    "- **CosineSimilarityLoss (10%)**: Auxiliary loss for calibration\n",
    "  - Improves absolute similarity score calibration\n",
    "  - Better threshold-based binary classification (F1, precision, recall)\n",
    "  - Addresses the \"similarity score calibration\" gap\n",
    "\n",
    "**Why this combination?**\n",
    "- MNRL excels at ranking but may not calibrate absolute scores well\n",
    "- CosineSimilarity provides calibrated similarity values for thresholding\n",
    "- Together: Best of both worlds for ranking + classification tasks\n",
    "\n",
    "## V2 Base Improvements (Inherited)\n",
    "1. **Harder negative mining** ‚Äî Increased hard_neg_ratio (35%‚Üí45%), stricter TF-IDF threshold (0.20‚Üí0.12)\n",
    "2. **Curriculum learning** ‚Äî Progressive difficulty: easy pairs first, hard pairs later\n",
    "3. **Borderline test set** ‚Äî New \"adversarial v2\" with TF-IDF 0.25-0.35 (ambiguous cases)\n",
    "4. **Error analysis** ‚Äî Systematic failure pattern detection\n",
    "5. **Cross-validated threshold** ‚Äî 5-fold CV for robust production threshold\n",
    "\n",
    "## Key Differences from Dummy Data Pipeline\n",
    "1. **No Short Description** ‚Äî Real data only has `Description` field\n",
    "2. **Rich vocabulary** ‚Äî 10K+ unique terms vs 111 in dummy data\n",
    "3. **Realistic metrics expected** ‚Äî ROC-AUC 0.85-0.90, Spearman 0.65-0.75\n",
    "4. **No Resolution notes in training text** ‚Äî Avoids data leakage for new tickets\n",
    "\n",
    "## Expected Benefits\n",
    "- **Training efficiency**: 2-3x faster with LoRA (fewer params to update)\n",
    "- **Memory savings**: Lower GPU memory usage enables larger batch sizes\n",
    "- **Performance**: Comparable or better than full fine-tune due to regularization\n",
    "- **Threshold calibration**: Improved binary classification metrics (F1, precision/recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9dc0f3",
   "metadata": {
    "description": "Imports: Core libraries (SentenceTransformers, PyTorch, sklearn)"
   },
   "source": [
    "# 1. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89658f5b",
   "metadata": {
    "description": "Markdown: Project Overview"
   },
   "source": [
    "## üìã Quick Configuration Summary\n",
    "\n",
    "| Component | Setting | Rationale |\n",
    "|-----------|---------|-----------|\n",
    "| **LoRA Rank** | 16 | Balanced capacity (8=low, 32=high) |\n",
    "| **LoRA Alpha** | 32 | Standard scaling (2√órank) |\n",
    "| **Target Modules** | q_proj, v_proj | Attention query/value projections |\n",
    "| **Learning Rate** | 1e-4 | Lower than full fine-tune (2e-5) |\n",
    "| **MNRL Weight** | 0.9 | Primary ranking loss |\n",
    "| **Cosine Weight** | 0.1 | Auxiliary calibration |\n",
    "| **Batch Size** | 16 (8 on MPS) | Auto-adjusted for device |\n",
    "| **Epochs** | 4 | 2 phases √ó 2 epochs each |\n",
    "\n",
    "**Key Metrics to Watch:**\n",
    "- **Trainable params**: Should be ~3-5M (vs ~30-50M baseline)\n",
    "- **Spearman correlation**: Target > 0.75 (current V2 best: 0.7516)\n",
    "- **ROC-AUC**: Target > 0.93 (current V2 best: 0.9369)\n",
    "- **F1 @ optimal threshold**: Target > 0.84 (current V2 best: 0.8442)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118e0c75",
   "metadata": {
    "description": "Config: Training parameters (LR, epochs, curriculum settings)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INSTALL] Installing: importlib-metadata; python_version<\"3.11\"\n"
     ]
    }
   ],
   "source": [
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment variables to suppress warnings\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_SILENT'] = 'true'\n",
    "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Install required packages\n",
    "def ensure_packages():\n",
    "    try:\n",
    "        import importlib.metadata as importlib_metadata\n",
    "    except ImportError:\n",
    "        import importlib_metadata\n",
    "    \n",
    "    required = {\n",
    "        'sentence-transformers': 'sentence-transformers>=2.2.2',\n",
    "        'torch': 'torch',\n",
    "        'scikit-learn': 'scikit-learn>=1.3.0',\n",
    "        'pandas': 'pandas',\n",
    "        'numpy': 'numpy>=1.24.0',\n",
    "        'tqdm': 'tqdm',\n",
    "        'matplotlib': 'matplotlib',\n",
    "        'seaborn': 'seaborn',\n",
    "        'peft': 'peft>=0.4.0',  # LoRA/PEFT support\n",
    "        'transformers': 'transformers>=4.30.0',# Compatible with PEFT\n",
    "        'datasets': 'datasets>=2.13.1',\n",
    "        'importlib_metadata': 'importlib-metadata; python_version<\"3.11\"',\n",
    "    }\n",
    "    \n",
    "    missing = []\n",
    "    for name, spec in required.items():\n",
    "        try:\n",
    "            importlib_metadata.version(name)\n",
    "        except importlib_metadata.PackageNotFoundError:\n",
    "            missing.append(spec)\n",
    "    \n",
    "    if missing:\n",
    "        print(f'[INSTALL] Installing: {\", \".join(missing)}')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', *missing])\n",
    "    else:\n",
    "        print('[OK] All packages installed')\n",
    "\n",
    "ensure_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1ad25",
   "metadata": {
    "description": "Markdown: Data Loading Section"
   },
   "source": [
    "# 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae876b7",
   "metadata": {
    "description": "Logging: Setup logging utilities"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURATION (V3 - Curriculum Learning)\n",
      "======================================================================\n",
      "Model: sentence-transformers/all-mpnet-base-v2\n",
      "Output: models/real_servicenow_finetuned_mpnet_lora\n",
      "\n",
      "Data:\n",
      "  Using pre-generated pairs: True\n",
      "  Pairs file: data_new/curriculum_training_pairs_20251224_065436.json\n",
      "\n",
      "LoRA Config:\n",
      "  Rank: 16\n",
      "  Alpha: 32\n",
      "  Dropout: 0.1\n",
      "\n",
      "Training:\n",
      "  Total epochs: 6\n",
      "  Learning rate: 5e-05 (INCREASED for LoRA)\n",
      "  Batch size: 16\n",
      "  Max seq length: 256 (REDUCED to match baseline)\n",
      "\n",
      "Curriculum:\n",
      "  Use curriculum: True\n",
      "  Epochs per phase: 2\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Simple logging\n",
    "def log(msg, level=logging.INFO):\n",
    "    print(msg)\n",
    "\n",
    "# --- CONFIGURATION (V3 - Curriculum Learning with Pre-generated Pairs) ---\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'sentence-transformers/all-mpnet-base-v2',\n",
    "    'output_dir': 'models/real_servicenow_finetuned_mpnet_lora',\n",
    "\n",
    "    # Data - USING PRE-GENERATED CURRICULUM PAIRS\n",
    "    'use_pre_generated_pairs': True,  # NEW: Use pre-generated pairs\n",
    "    'train_pairs_path': 'data_new/curriculum_training_pairs_20251224_065436.json',  # NEW: Curriculum dataset\n",
    "    'source_data': 'data_new\\\\SNow_incident_ticket_data.csv',  # Fallback (not used)\n",
    "\n",
    "    # LoRA/PEFT Configuration\n",
    "    'use_lora': True,\n",
    "    'lora_r': 16,              # Rank (8-32, higher = more capacity)\n",
    "    'lora_alpha': 32,          # Scaling factor (typically 2*r)\n",
    "    'lora_dropout': 0.1,       # Dropout for LoRA layers\n",
    "    'lora_target_modules': ['query', 'key', 'value'],\n",
    "\n",
    "    # Loss Function\n",
    "    'use_multi_loss': False,   # Use single loss for simplicity\n",
    "    'loss_type': 'cosine',     # 'cosine' or 'mnrl'\n",
    "\n",
    "    # Training hyperparameters (UPDATED FOR CURRICULUM)\n",
    "    'epochs': 6,               # 6 total epochs (2 per curriculum phase)\n",
    "    'batch_size': 16,          # Will auto-reduce for MPS/CPU if needed\n",
    "    'lr': 5e-5,                # INCREASED from 2e-5 (LoRA needs higher LR)\n",
    "    'max_seq_length': 256,     # REDUCED from 384 (match baseline)\n",
    "    'warmup_ratio': 0.1,\n",
    "\n",
    "    # Curriculum Learning (NEW - Using phase_indicators from data)\n",
    "    'use_curriculum': True,    # Train in 3 phases\n",
    "    'epochs_per_phase': 2,     # 2 epochs per phase\n",
    "\n",
    "    # Data splits (only used if NOT using pre-generated)\n",
    "    'eval_split': 0.15,\n",
    "    'holdout_split': 0.10,\n",
    "    'min_text_length': 25,\n",
    "\n",
    "    # Pair generation (LEGACY - not used with pre-generated pairs)\n",
    "    'num_pairs': 50000,        # Not used\n",
    "    'pos_ratio': 0.30,         # Not used\n",
    "\n",
    "    # Reproducibility\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Set seeds\n",
    "random.seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "\n",
    "log(\"=\"*70)\n",
    "log(\"CONFIGURATION (V3 - Curriculum Learning)\")\n",
    "log(\"=\"*70)\n",
    "log(f\"Model: {CONFIG['model_name']}\")\n",
    "log(f\"Output: {CONFIG['output_dir']}\")\n",
    "log(f\"\\nData:\")\n",
    "log(f\"  Using pre-generated pairs: {CONFIG['use_pre_generated_pairs']}\")\n",
    "log(f\"  Pairs file: {CONFIG['train_pairs_path']}\")\n",
    "log(f\"\\nLoRA Config:\")\n",
    "log(f\"  Rank: {CONFIG['lora_r']}\")\n",
    "log(f\"  Alpha: {CONFIG['lora_alpha']}\")\n",
    "log(f\"  Dropout: {CONFIG['lora_dropout']}\")\n",
    "log(f\"\\nTraining:\")\n",
    "log(f\"  Total epochs: {CONFIG['epochs']}\")\n",
    "log(f\"  Learning rate: {CONFIG['lr']} (INCREASED for LoRA)\")\n",
    "log(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "log(f\"  Max seq length: {CONFIG['max_seq_length']} (REDUCED to match baseline)\")\n",
    "log(f\"\\nCurriculum:\")\n",
    "log(f\"  Use curriculum: {CONFIG['use_curriculum']}\")\n",
    "log(f\"  Epochs per phase: {CONFIG['epochs_per_phase']}\")\n",
    "log(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8903b0bf",
   "metadata": {
    "description": "Data Loading: Functions to load ServiceNow incidents"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.11.0.dev20251223+cu128\n",
      "torchvision 0.25.0.dev20251223+cu128\n",
      "cuda? True\n",
      "cuda runtime 12.8\n",
      "NVIDIA GeForce RTX 5090\n",
      "cap (12, 0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)\n",
    "print('cuda?', torch.cuda.is_available())\n",
    "print('cuda runtime', getattr(torch.version, 'cuda', None))\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'NO GPU')\n",
    "print('cap', torch.cuda.get_device_capability(0) if torch.cuda.is_available() else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b3f39",
   "metadata": {
    "description": "Markdown: Data Preprocessing"
   },
   "source": [
    "# 3. Data Loading & Preprocessing\n",
    "\n",
    "Load real ServiceNow incident data. Key differences from dummy data:\n",
    "- Only `Description` field (no Short Description)\n",
    "- Multi-line text with embedded newlines\n",
    "- Rich technical vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e629653",
   "metadata": {
    "description": "Data: Load incident data from JSON"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Looking for data file:\n",
      "   Configured path: data_new\\SNow_incident_ticket_data.csv\n",
      "   Absolute path: C:\\Users\\donov\\Downloads\\nexustism\\nexustism\\data_new\\SNow_incident_ticket_data.csv\n",
      "   Current working directory: c:\\Users\\donov\\Downloads\\nexustism\\nexustism\n",
      "   File exists: True\n",
      "üìÇ Loading real ServiceNow data from: data_new\\SNow_incident_ticket_data.csv\n",
      "[STATS] Loaded 10,633 raw records\n",
      "[OK] Available columns: ['Number', 'Description', 'Opened by', 'Company', 'ITSM Department', 'Created', 'Urgency', 'Impact', 'Priority', 'Assignment group', 'Assigned to', 'State', 'Service', 'Service offering', 'Closed', 'Closed by', 'Category', 'Subcategory', 'Resolution code', 'Resolution notes', 'User input', 'Comments and Work notes', 'Manday Effort (hrs)', 'Ticket Type', 'AMS Domain', 'AMS System Type', 'AMS Category Type', 'AMS Service Type', 'AMS Business Related', 'AMS IT Related']\n",
      "üìâ After filtering short descriptions: 10,486 records (dropped 147)\n",
      "\n",
      "[STATS] Data Summary:\n",
      "   Total records: 10,486\n",
      "   Unique categories: 30\n",
      "   Avg text length: 561 chars\n",
      "   Min text length: 66 chars\n",
      "   Max text length: 15545 chars\n",
      "\n",
      "üìù Sample preprocessed text:\n",
      "   'GRPT not working as expected. ZMMM_PO_REV is not generating correct dates as per maintained in GRPT table. E.g. P/O# 100024066 Vendor Ship mode is 03. As per GRPT route days are 12 days and GR days is...'\n",
      "\n",
      "[OK] Loaded 10,486 incidents ready for training\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def load_and_preprocess_real_data(config):\n",
    "    \"\"\"\n",
    "    Load and preprocess real ServiceNow incident data.\n",
    "    \n",
    "    Key differences from dummy data:\n",
    "    - Only 'Description' column (no 'Short Description')\n",
    "    - Real data has multi-line descriptions with embedded newlines\n",
    "    - Richer vocabulary and more varied text\n",
    "    \"\"\"\n",
    "    source_path = Path(config['source_data'])\n",
    "    \n",
    "    # Debug: Show absolute path\n",
    "    log(f\"üîç Looking for data file:\")\n",
    "    log(f\"   Configured path: {config['source_data']}\")\n",
    "    log(f\"   Absolute path: {source_path.resolve()}\")\n",
    "    log(f\"   Current working directory: {Path.cwd()}\")\n",
    "    log(f\"   File exists: {source_path.exists()}\")\n",
    "    \n",
    "    if not source_path.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {source_path.resolve()}\")\n",
    "    \n",
    "    log(f\"üìÇ Loading real ServiceNow data from: {source_path}\")\n",
    "    \n",
    "    # Load CSV - handle multi-line fields\n",
    "    df = pd.read_csv(source_path, encoding='utf-8', on_bad_lines='skip')\n",
    "    initial_count = len(df)\n",
    "    log(f\"[STATS] Loaded {initial_count:,} raw records\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['Number', 'Description', 'Category', 'Subcategory', \n",
    "                     'Service', 'Service offering', 'Assignment group']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        log(f\"[WARN] Missing columns: {missing_cols}\")\n",
    "        # Use available columns\n",
    "        required_cols = [col for col in required_cols if col in df.columns]\n",
    "    \n",
    "    log(f\"[OK] Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Clean text function\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text) or text is None:\n",
    "            return \"\"\n",
    "        text = str(text).strip()\n",
    "        # Normalize whitespace (collapse multiple spaces/newlines)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove common boilerplate\n",
    "        text = re.sub(r'Note\\s*:\\s*This is an automated.*?\\.', '', text, flags=re.IGNORECASE)\n",
    "        return text.strip()\n",
    "    \n",
    "    # Clean Description\n",
    "    df['Description'] = df['Description'].apply(clean_text)\n",
    "    \n",
    "    # Fill NA for context columns\n",
    "    context_cols = ['Category', 'Subcategory', 'Service', 'Service offering', 'Assignment group']\n",
    "    for col in context_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('').astype(str).str.strip().str.lower()\n",
    "    \n",
    "    # Build contextual text representation\n",
    "    # Format: \"Description (Context: [Service|Offering] [Category|Subcategory] Group: X)\"\n",
    "    def build_context(row):\n",
    "        parts = []\n",
    "        \n",
    "        # Service context\n",
    "        service_parts = []\n",
    "        if row.get('Service', ''):\n",
    "            service_parts.append(row['Service'])\n",
    "        if row.get('Service offering', ''):\n",
    "            service_parts.append(row['Service offering'])\n",
    "        if service_parts:\n",
    "            parts.append(f\"[{' | '.join(service_parts)}]\")\n",
    "        \n",
    "        # Category context\n",
    "        cat_parts = []\n",
    "        if row.get('Category', ''):\n",
    "            cat_parts.append(row['Category'])\n",
    "        if row.get('Subcategory', ''):\n",
    "            cat_parts.append(row['Subcategory'])\n",
    "        if cat_parts:\n",
    "            parts.append(f\"[{' | '.join(cat_parts)}]\")\n",
    "        \n",
    "        # Assignment group\n",
    "        if row.get('Assignment group', ''):\n",
    "            parts.append(f\"Group: {row['Assignment group']}\")\n",
    "        \n",
    "        return ' '.join(parts) if parts else ''\n",
    "    \n",
    "    # Build full text: Description + Context suffix\n",
    "    df['context'] = df.apply(build_context, axis=1)\n",
    "    df['text'] = df.apply(\n",
    "        lambda row: f\"{row['Description']} (Context: {row['context']})\" if row['context'] else row['Description'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Filter out short/empty descriptions\n",
    "    df = df[df['Description'].str.len() >= config['min_text_length']].copy()\n",
    "    log(f\"üìâ After filtering short descriptions: {len(df):,} records (dropped {initial_count - len(df):,})\")\n",
    "    \n",
    "    # Create category_id for stratified splitting\n",
    "    if 'Category' in df.columns and 'Subcategory' in df.columns:\n",
    "        df['category_id'] = df.groupby(['Category', 'Subcategory']).ngroup()\n",
    "    else:\n",
    "        df['category_id'] = 0\n",
    "    \n",
    "    # Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Summary stats\n",
    "    log(f\"\\n[STATS] Data Summary:\")\n",
    "    log(f\"   Total records: {len(df):,}\")\n",
    "    log(f\"   Unique categories: {df['category_id'].nunique()}\")\n",
    "    log(f\"   Avg text length: {df['text'].str.len().mean():.0f} chars\")\n",
    "    log(f\"   Min text length: {df['text'].str.len().min()} chars\")\n",
    "    log(f\"   Max text length: {df['text'].str.len().max()} chars\")\n",
    "    \n",
    "    # Sample text\n",
    "    log(f\"\\nüìù Sample preprocessed text:\")\n",
    "    log(f\"   '{df['text'].iloc[0][:200]}...'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "df_incidents = load_and_preprocess_real_data(CONFIG)\n",
    "print(f\"\\n[OK] Loaded {len(df_incidents):,} incidents ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35dc0c",
   "metadata": {
    "description": "Markdown: Data Splitting"
   },
   "source": [
    "# 4. Data Splitting\n",
    "\n",
    "Split into Train / Eval / Holdout sets with stratification by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d80a32",
   "metadata": {
    "description": "Data Split: Split into train/eval/holdout sets"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Using stratified split\n",
      "[STATS] Data Splits:\n",
      "   Train:   8,021 incidents (76.5%)\n",
      "   Eval:    1,416 incidents (13.5%)\n",
      "   Holdout: 1,049 incidents (10.0%)\n",
      "\n",
      "üîç Overlap Check:\n",
      "   Train ‚à© Eval: 0 incidents\n",
      "   Train ‚à© Holdout: 0 incidents\n",
      "   Eval ‚à© Holdout: 0 incidents\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df, config):\n",
    "    \"\"\"\n",
    "    Three-way split: Train / Eval / Holdout\n",
    "    - Holdout is completely unseen (for adversarial diagnostic)\n",
    "    - Stratified by category to ensure representation\n",
    "    \"\"\"\n",
    "    # Handle rare categories: group categories with <2 samples\n",
    "    category_counts = df['category_id'].value_counts()\n",
    "    rare_categories = category_counts[category_counts < 2].index\n",
    "    \n",
    "    # Create stratification column: use category_id for common categories, -1 for rare\n",
    "    df['stratify_col'] = df['category_id'].copy()\n",
    "    df.loc[df['category_id'].isin(rare_categories), 'stratify_col'] = -1\n",
    "    \n",
    "    # Check if we can stratify (need at least 2 samples per class)\n",
    "    stratify_counts = df['stratify_col'].value_counts()\n",
    "    can_stratify = all(stratify_counts >= 2)\n",
    "    \n",
    "    if can_stratify:\n",
    "        # First split: separate holdout set (stratified)\n",
    "        train_eval_df, holdout_df = train_test_split(\n",
    "            df,\n",
    "            test_size=config['holdout_split'],\n",
    "            stratify=df['stratify_col'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "        \n",
    "        # Second split: train/eval from remaining (stratified)\n",
    "        train_df, eval_df = train_test_split(\n",
    "            train_eval_df,\n",
    "            test_size=config['eval_split'],\n",
    "            stratify=train_eval_df['stratify_col'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "        log(\"[OK] Using stratified split\")\n",
    "    else:\n",
    "        # Fallback: random split without stratification\n",
    "        log(\"[WARN] Using random split (categories too imbalanced for stratification)\")\n",
    "        train_eval_df, holdout_df = train_test_split(\n",
    "            df,\n",
    "            test_size=config['holdout_split'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "        \n",
    "        train_df, eval_df = train_test_split(\n",
    "            train_eval_df,\n",
    "            test_size=config['eval_split'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "    \n",
    "    return train_df, eval_df, holdout_df\n",
    "\n",
    "# Split the data\n",
    "train_df, eval_df, holdout_df = split_data(df_incidents, CONFIG)\n",
    "\n",
    "log(f\"[STATS] Data Splits:\")\n",
    "log(f\"   Train:   {len(train_df):,} incidents ({len(train_df)/len(df_incidents)*100:.1f}%)\")\n",
    "log(f\"   Eval:    {len(eval_df):,} incidents ({len(eval_df)/len(df_incidents)*100:.1f}%)\")\n",
    "log(f\"   Holdout: {len(holdout_df):,} incidents ({len(holdout_df)/len(df_incidents)*100:.1f}%)\")\n",
    "\n",
    "# Check for data leakage\n",
    "def check_overlap(df1, df2, name1, name2):\n",
    "    overlap = len(set(df1['Number']) & set(df2['Number']))\n",
    "    log(f\"   {name1} ‚à© {name2}: {overlap} incidents\")\n",
    "\n",
    "log(f\"\\nüîç Overlap Check:\")\n",
    "check_overlap(train_df, eval_df, \"Train\", \"Eval\")\n",
    "check_overlap(train_df, holdout_df, \"Train\", \"Holdout\")\n",
    "check_overlap(eval_df, holdout_df, \"Eval\", \"Holdout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b68d81eb",
   "metadata": {
    "description": "Pair Loader: Load curriculum training pairs & test pairs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\donov\\anaconda3\\envs\\itsm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Running in WANDB offline mode\n",
      "======================================================================\n",
      "USING PRE-GENERATED CURRICULUM PAIRS\n",
      "======================================================================\n",
      "\n",
      "Loading curriculum pairs from: data_new/curriculum_training_pairs_20251224_065436.json\n",
      "Loaded 15,000 total pairs\n",
      "  Positives: 10,000 (66.7%)\n",
      "  Negatives: 5,000 (33.3%)\n",
      "\n",
      "Curriculum phases: 3\n",
      "  Phase 1 (easy): 5,000 pairs, pos>=0.52, neg<=0.36\n",
      "  Phase 2 (medium): 5,000 pairs, pos>=0.4, neg<=0.45\n",
      "  Phase 3 (hard): 5,000 pairs, pos>=0.3, neg<=0.5\n",
      "\n",
      "Separated into phases:\n",
      "  Phase 1: 5,000 pairs\n",
      "  Phase 2: 5,000 pairs\n",
      "  Phase 3: 5,000 pairs\n",
      "\n",
      "Total training examples: 15,000\n",
      "\n",
      "Note: Will train in 3 curriculum phases\n",
      "\n",
      "Loading test pairs from: data_new/fixed_test_pairs.json\n",
      "[OK] Loaded 1,000 test pairs for evaluation\n",
      "   Positives: 500 (50.0%)\n",
      "   Negatives: 500 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# LOAD PRE-GENERATED CURRICULUM PAIRS\n",
    "# ========================================\n",
    "\n",
    "import json\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "def load_curriculum_pairs(pairs_path, use_curriculum=True):\n",
    "    \"\"\"\n",
    "    Load pre-generated curriculum pairs from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        pairs_path: Path to curriculum_training_pairs_*.json\n",
    "        use_curriculum: If True, return separate phases; if False, return all mixed\n",
    "    \n",
    "    Returns:\n",
    "        If use_curriculum=True: (phase1_pairs, phase2_pairs, phase3_pairs)\n",
    "        If use_curriculum=False: all_pairs (mixed)\n",
    "    \"\"\"\n",
    "    log(f\"\\nLoading curriculum pairs from: {pairs_path}\")\n",
    "    \n",
    "    with open(pairs_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    texts1 = data['texts1']\n",
    "    texts2 = data['texts2']\n",
    "    labels = data['labels']\n",
    "    phase_indicators = data.get('phase_indicators', [1] * len(labels))\n",
    "    \n",
    "    log(f\"Loaded {len(labels):,} total pairs\")\n",
    "    log(f\"  Positives: {sum(labels):,} ({100*sum(labels)/len(labels):.1f}%)\")\n",
    "    log(f\"  Negatives: {len(labels) - sum(labels):,} ({100*(len(labels)-sum(labels))/len(labels):.1f}%)\")\n",
    "    \n",
    "    # Show metadata\n",
    "    metadata = data.get('metadata', {})\n",
    "    if metadata:\n",
    "        log(f\"\\nCurriculum phases: {metadata.get('curriculum_phases', 'N/A')}\")\n",
    "        for phase_num in [1, 2, 3]:\n",
    "            phase_key = f'phase{phase_num}_config'\n",
    "            if phase_key in metadata:\n",
    "                phase_cfg = metadata[phase_key]\n",
    "                log(f\"  Phase {phase_num} ({phase_cfg.get('difficulty', 'N/A')}): \"\n",
    "                    f\"{phase_cfg.get('pairs', 0):,} pairs, \"\n",
    "                    f\"pos>={phase_cfg.get('pos_threshold', 'N/A')}, \"\n",
    "                    f\"neg<={phase_cfg.get('neg_threshold', 'N/A')}\")\n",
    "    \n",
    "    # Convert to InputExample format\n",
    "    if use_curriculum:\n",
    "        # Separate by phase\n",
    "        phase1_pairs = []\n",
    "        phase2_pairs = []\n",
    "        phase3_pairs = []\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            example = InputExample(texts=[texts1[i], texts2[i]], label=float(labels[i]))\n",
    "            phase = phase_indicators[i]\n",
    "            if phase == 1:\n",
    "                phase1_pairs.append(example)\n",
    "            elif phase == 2:\n",
    "                phase2_pairs.append(example)\n",
    "            elif phase == 3:\n",
    "                phase3_pairs.append(example)\n",
    "        \n",
    "        log(f\"\\nSeparated into phases:\")\n",
    "        log(f\"  Phase 1: {len(phase1_pairs):,} pairs\")\n",
    "        log(f\"  Phase 2: {len(phase2_pairs):,} pairs\")\n",
    "        log(f\"  Phase 3: {len(phase3_pairs):,} pairs\")\n",
    "        \n",
    "        return phase1_pairs, phase2_pairs, phase3_pairs\n",
    "    else:\n",
    "        # Return all mixed\n",
    "        all_pairs = [\n",
    "            InputExample(texts=[texts1[i], texts2[i]], label=float(labels[i]))\n",
    "            for i in range(len(labels))\n",
    "        ]\n",
    "        log(f\"Returning {len(all_pairs):,} mixed pairs\")\n",
    "        return all_pairs\n",
    "\n",
    "# ========================================\n",
    "# LOAD OR GENERATE PAIRS\n",
    "# ========================================\n",
    "\n",
    "if CONFIG.get('use_pre_generated_pairs', False):\n",
    "    log(\"=\"*70)\n",
    "    log(\"USING PRE-GENERATED CURRICULUM PAIRS\")\n",
    "    log(\"=\"*70)\n",
    "    \n",
    "    pairs_path = CONFIG['train_pairs_path']\n",
    "    \n",
    "    if CONFIG.get('use_curriculum', False):\n",
    "        # Load phases separately for curriculum training\n",
    "        phase1_train, phase2_train, phase3_train = load_curriculum_pairs(\n",
    "            pairs_path, use_curriculum=True\n",
    "        )\n",
    "        \n",
    "        # For now, combine for evaluation split\n",
    "        # (In production, you'd want separate eval sets per phase)\n",
    "        train_examples = phase1_train + phase2_train + phase3_train\n",
    "        \n",
    "        log(f\"\\nTotal training examples: {len(train_examples):,}\")\n",
    "        log(\"\\nNote: Will train in 3 curriculum phases\")\n",
    "        \n",
    "        # Store phases for later use\n",
    "        CURRICULUM_PHASES = {\n",
    "            'phase1': phase1_train,\n",
    "            'phase2': phase2_train,\n",
    "            'phase3': phase3_train\n",
    "        }\n",
    "    \n",
    "    # Load test/eval pairs from fixed_test_pairs.json\n",
    "    test_pairs_path = 'data_new/fixed_test_pairs.json'\n",
    "    log(f\"\\nLoading test pairs from: {test_pairs_path}\")\n",
    "    \n",
    "    with open(test_pairs_path, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    # Convert to InputExample\n",
    "    from sentence_transformers import InputExample\n",
    "    eval_examples = [\n",
    "        InputExample(texts=[t1, t2], label=float(label))\n",
    "        for t1, t2, label in zip(test_data['texts1'], test_data['texts2'], test_data['labels'])\n",
    "    ]\n",
    "    \n",
    "    # For curriculum training, we don't have separate holdout/borderline\n",
    "    # Use eval_examples for all evaluation metrics\n",
    "    holdout_examples = eval_examples  # Reuse for holdout metrics\n",
    "    borderline_examples = []  # Empty - not applicable for curriculum\n",
    "    \n",
    "    log(f\"[OK] Loaded {len(eval_examples):,} test pairs for evaluation\")\n",
    "    pos_count = sum(1 for ex in eval_examples if ex.label == 1.0)\n",
    "    log(f\"   Positives: {pos_count:,} ({100*pos_count/len(eval_examples):.1f}%)\")\n",
    "    log(f\"   Negatives: {len(eval_examples)-pos_count:,} ({100*(len(eval_examples)-pos_count)/len(eval_examples):.1f}%)\")\n",
    "    \n",
    "    \n",
    "    # Skip the pair generation cells below\n",
    "    SKIP_PAIR_GENERATION = True\n",
    "    \n",
    "else:\n",
    "    log(\"=\"*70)\n",
    "    # eval_examples, holdout_examples, borderline_examples will be generated below\n",
    "    log(\"GENERATING PAIRS ON-THE-FLY (LEGACY MODE)\")\n",
    "    log(\"=\"*70)\n",
    "    log(\"Note: Consider using pre-generated curriculum pairs instead!\")\n",
    "    log(\"      Run fix_train_test_mismatch.ipynb to generate them.\")\n",
    "    \n",
    "    SKIP_PAIR_GENERATION = False\n",
    "    CURRICULUM_PHASES = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfadea",
   "metadata": {
    "description": "Markdown: Pair Generation"
   },
   "source": [
    "# 5. Pair Generation\n",
    "\n",
    "Generate training pairs using TF-IDF similarity mining:\n",
    "- **35% Positives**: High TF-IDF similarity (> 0.35)\n",
    "- **35% Hard Negatives**: Same category, low TF-IDF (< 0.20)\n",
    "- **30% Easy Negatives**: Different category, low TF-IDF\n",
    "\n",
    "This forces the model to learn semantic content, not just category matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff4c6d1",
   "metadata": {
    "description": "Pair Generation: TF-IDF classes (legacy, SKIPPED)"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import InputExample\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "class TFIDFSimilarityCalculator:\n",
    "    \"\"\"Efficient TF-IDF similarity calculator for large datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, max_features=15000):\n",
    "        log(\"‚è≥ Building TF-IDF matrix...\")\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),  # Unigrams + bigrams for better matching\n",
    "            min_df=2,           # Ignore very rare terms\n",
    "            max_df=0.95         # Ignore very common terms\n",
    "        )\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "        log(f\"[OK] TF-IDF matrix: {self.tfidf_matrix.shape} (vocab size: {len(self.vectorizer.vocabulary_)})\")\n",
    "    \n",
    "    def similarity(self, idx1, idx2):\n",
    "        \"\"\"Compute cosine similarity between two documents.\"\"\"\n",
    "        if idx1 >= self.tfidf_matrix.shape[0] or idx2 >= self.tfidf_matrix.shape[0]:\n",
    "            return 0.0\n",
    "        vec1 = self.tfidf_matrix[idx1]\n",
    "        vec2 = self.tfidf_matrix[idx2]\n",
    "        return (vec1 @ vec2.T).toarray()[0][0]\n",
    "\n",
    "\n",
    "def generate_training_pairs(df, target_count, config, desc=\"\", phase_config=None, tfidf_calc=None):\n",
    "    \"\"\"\n",
    "    Generate training pairs with configurable split (V2: supports curriculum phases).\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with ticket data\n",
    "        target_count: Total number of pairs to generate\n",
    "        config: Main configuration dict\n",
    "        desc: Description for logging\n",
    "        phase_config: Optional override for curriculum learning phase\n",
    "        tfidf_calc: Optional precomputed TF-IDF calculator to avoid rebuilding\n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Use phase config if provided (for curriculum learning)\n",
    "    pos_ratio = config['pos_ratio']\n",
    "    hard_neg_ratio = phase_config['hard_neg_ratio'] if phase_config else config['hard_neg_ratio']\n",
    "    neg_threshold = phase_config['neg_threshold'] if phase_config else config['neg_tfidf_threshold']\n",
    "    easy_neg_ratio = 1.0 - pos_ratio - hard_neg_ratio\n",
    "    \n",
    "    # Build or reuse TF-IDF for this split\n",
    "    created_tfidf = False\n",
    "    if tfidf_calc is None:\n",
    "        tfidf_calc = TFIDFSimilarityCalculator(df['text'].tolist())\n",
    "        created_tfidf = True\n",
    "    \n",
    "    # Calculate targets\n",
    "    pos_target = int(target_count * pos_ratio)\n",
    "    hard_neg_target = int(target_count * hard_neg_ratio)\n",
    "    easy_neg_target = target_count - pos_target - hard_neg_target\n",
    "    \n",
    "    pairs = []\n",
    "    all_indices = list(df.index)\n",
    "    \n",
    "    # Group by category for hard negatives\n",
    "    category_groups = df.groupby('category_id').indices\n",
    "    valid_groups = {k: list(v) for k, v in category_groups.items() if len(v) >= 2}\n",
    "    \n",
    "    log(f\"\\n[TARGET] Generating {target_count:,} pairs for {desc}:\")\n",
    "    log(f\"   Target: {pos_target:,} positives ({pos_ratio*100:.0f}%), {hard_neg_target:,} hard neg ({hard_neg_ratio*100:.0f}%), {easy_neg_target:,} easy neg\")\n",
    "    log(f\"   Neg TF-IDF threshold: {neg_threshold}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # 1. POSITIVES: High TF-IDF similarity (> threshold)\n",
    "    # ============================================\n",
    "    pbar = tqdm(total=pos_target, desc=\"Positives (high TF-IDF)\")\n",
    "    attempts, max_attempts = 0, pos_target * 50\n",
    "    \n",
    "    while len(pairs) < pos_target and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        i1, i2 = random.sample(all_indices, 2)\n",
    "        if i1 == i2:\n",
    "            continue\n",
    "        \n",
    "        sim = tfidf_calc.similarity(i1, i2)\n",
    "        if sim > config['pos_tfidf_threshold']:\n",
    "            pairs.append(InputExample(\n",
    "                texts=[df.at[i1, 'text'], df.at[i2, 'text']],\n",
    "                label=1.0\n",
    "            ))\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    actual_pos = len(pairs)\n",
    "    \n",
    "    # ============================================\n",
    "    # 2. HARD NEGATIVES: Same category, low TF-IDF (V2: stricter threshold)\n",
    "    # ============================================\n",
    "    current_len = len(pairs)\n",
    "    pbar = tqdm(total=hard_neg_target, desc=f\"Hard Negatives (same cat, TF-IDF<{neg_threshold})\")\n",
    "    attempts, max_attempts = 0, hard_neg_target * 50\n",
    "    \n",
    "    while (len(pairs) - current_len) < hard_neg_target and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        if not valid_groups:\n",
    "            break\n",
    "        \n",
    "        # Pick a random category with 2+ members\n",
    "        gid = random.choice(list(valid_groups.keys()))\n",
    "        g_indices = valid_groups[gid]\n",
    "        if len(g_indices) < 2:\n",
    "            continue\n",
    "        \n",
    "        i1, i2 = random.sample(g_indices, 2)\n",
    "        sim = tfidf_calc.similarity(i1, i2)\n",
    "        \n",
    "        if sim < neg_threshold:  # V2: Uses stricter threshold\n",
    "            pairs.append(InputExample(\n",
    "                texts=[df.at[i1, 'text'], df.at[i2, 'text']],\n",
    "                label=0.0\n",
    "            ))\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    actual_hard = len(pairs) - current_len\n",
    "    \n",
    "    # ============================================\n",
    "    # 3. EASY NEGATIVES: Cross-category, low TF-IDF\n",
    "    # ============================================\n",
    "    current_len = len(pairs)\n",
    "    pbar = tqdm(total=easy_neg_target, desc=\"Easy Negatives (cross-cat)\")\n",
    "    attempts, max_attempts = 0, easy_neg_target * 50\n",
    "    \n",
    "    while (len(pairs) - current_len) < easy_neg_target and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        i1, i2 = random.sample(all_indices, 2)\n",
    "        \n",
    "        # Must be different categories\n",
    "        if df.at[i1, 'category_id'] == df.at[i2, 'category_id']:\n",
    "            continue\n",
    "        \n",
    "        sim = tfidf_calc.similarity(i1, i2)\n",
    "        if sim < neg_threshold:\n",
    "            pairs.append(InputExample(\n",
    "                texts=[df.at[i1, 'text'], df.at[i2, 'text']],\n",
    "                label=0.0\n",
    "            ))\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    actual_easy = len(pairs) - current_len\n",
    "    \n",
    "    # Summary\n",
    "    total_pos = sum(1 for p in pairs if p.label == 1.0)\n",
    "    total_neg = len(pairs) - total_pos\n",
    "    \n",
    "    log(f\"[OK] Generated {len(pairs):,} pairs:\")\n",
    "    log(f\"   Positives: {actual_pos:,} ({actual_pos/len(pairs)*100:.1f}%)\")\n",
    "    log(f\"   Hard Neg:  {actual_hard:,} ({actual_hard/len(pairs)*100:.1f}%)\")\n",
    "    log(f\"   Easy Neg:  {actual_easy:,} ({actual_easy/len(pairs)*100:.1f}%)\")\n",
    "    \n",
    "    # Clean up TF-IDF to free memory only if we created it here\n",
    "    if created_tfidf:\n",
    "        del tfidf_calc\n",
    "        gc.collect()\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "def generate_borderline_pairs(df, target_count, config, desc=\"Borderline\", tfidf_calc=None):\n",
    "    \"\"\"\n",
    "    V2 NEW: Generate borderline/ambiguous pairs for harder evaluation.\n",
    "    \n",
    "    These are pairs with TF-IDF similarity in the 0.25-0.35 range - \n",
    "    cases where it's genuinely hard to determine similarity.\n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    created_tfidf = False\n",
    "    if tfidf_calc is None:\n",
    "        tfidf_calc = TFIDFSimilarityCalculator(df['text'].tolist())\n",
    "        created_tfidf = True\n",
    "    \n",
    "    borderline_pairs = []\n",
    "    all_indices = list(df.index)\n",
    "    \n",
    "    # TF-IDF range for borderline cases\n",
    "    low_threshold = 0.25\n",
    "    high_threshold = 0.35\n",
    "    \n",
    "    log(f\"\\n[TARGET] Generating {target_count:,} borderline pairs ({desc}):\")\n",
    "    log(f\"   TF-IDF range: {low_threshold} - {high_threshold}\")\n",
    "    \n",
    "    pbar = tqdm(total=target_count, desc=\"Borderline pairs\")\n",
    "    attempts, max_attempts = 0, target_count * 100\n",
    "    \n",
    "    while len(borderline_pairs) < target_count and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        i1, i2 = random.sample(all_indices, 2)\n",
    "        if i1 == i2:\n",
    "            continue\n",
    "        \n",
    "        sim = tfidf_calc.similarity(i1, i2)\n",
    "        \n",
    "        # Borderline: medium TF-IDF similarity (ambiguous)\n",
    "        if low_threshold <= sim <= high_threshold:\n",
    "            # Label based on same category (proxy for similarity)\n",
    "            same_cat = df.at[i1, 'category_id'] == df.at[i2, 'category_id']\n",
    "            label = 1.0 if same_cat else 0.0\n",
    "            \n",
    "            borderline_pairs.append(InputExample(\n",
    "                texts=[df.at[i1, 'text'], df.at[i2, 'text']],\n",
    "                label=label\n",
    "            ))\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    pos_count = sum(1 for p in borderline_pairs if p.label == 1.0)\n",
    "    log(f\"[OK] Generated {len(borderline_pairs):,} borderline pairs:\")\n",
    "    log(f\"   Positives: {pos_count:,} ({pos_count/len(borderline_pairs)*100:.1f}%)\")\n",
    "    log(f\"   Negatives: {len(borderline_pairs)-pos_count:,}\")\n",
    "    \n",
    "    if created_tfidf:\n",
    "        del tfidf_calc\n",
    "        gc.collect()\n",
    "    \n",
    "    return borderline_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1053863e",
   "metadata": {
    "description": "Pair Generation: Generate pairs on-the-fly (legacy, SKIPPED)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP]  Skipping pair generation (using pre-generated curriculum pairs)\n",
      "   Loaded from: data_new/curriculum_training_pairs_20251224_065436.json\n"
     ]
    }
   ],
   "source": [
    "# Skip if using pre-generated pairs\n",
    "if not CONFIG.get('use_pre_generated_pairs', False):\n",
    "    \n",
    "    # Generate pairs for each split with reusable TF-IDF\n",
    "    # Scale pair counts based on split sizes\n",
    "    train_pair_count = int(CONFIG['num_pairs'] * 0.75)  # 75% for training\n",
    "    eval_pair_count = int(CONFIG['num_pairs'] * 0.15)   # 15% for eval\n",
    "    holdout_pair_count = int(CONFIG['num_pairs'] * 0.10) # 10% for holdout\n",
    "\n",
    "    # Build TF-IDF once per split to reuse across phases/evals\n",
    "    log(\"\\n[BUILD] Building reusable TF-IDF matrices (one per split)...\")\n",
    "    tfidf_train = TFIDFSimilarityCalculator(train_df['text'].tolist())\n",
    "    tfidf_eval = TFIDFSimilarityCalculator(eval_df['text'].tolist())\n",
    "    tfidf_holdout = TFIDFSimilarityCalculator(holdout_df['text'].tolist())\n",
    "\n",
    "    # Generate pairs using shared TF-IDF calculators\n",
    "    train_examples = generate_training_pairs(train_df, train_pair_count, CONFIG, desc=\"Training\", tfidf_calc=tfidf_train)\n",
    "    eval_examples = generate_training_pairs(eval_df, eval_pair_count, CONFIG, desc=\"Evaluation\", tfidf_calc=tfidf_eval)\n",
    "    holdout_examples = generate_training_pairs(holdout_df, holdout_pair_count, CONFIG, desc=\"Holdout\", tfidf_calc=tfidf_holdout)\n",
    "\n",
    "    # V2 NEW: Generate borderline test set for harder evaluation (reuse holdout TF-IDF)\n",
    "    borderline_count = int(CONFIG['num_pairs'] * 0.05)  # 5% as borderline test\n",
    "    borderline_examples = generate_borderline_pairs(holdout_df, borderline_count, CONFIG, desc=\"Borderline Test\", tfidf_calc=tfidf_holdout)\n",
    "\n",
    "    log(f\"\\n[INSTALL] Final Pair Counts:\")\n",
    "    log(f\"   Train:      {len(train_examples):,}\")\n",
    "    log(f\"   Eval:       {len(eval_examples):,}\")\n",
    "    log(f\"   Holdout:    {len(holdout_examples):,}\")\n",
    "    log(f\"   Borderline: {len(borderline_examples):,} (V2 NEW - harder test)\")\n",
    "else:\n",
    "    log(\"[SKIP]  Skipping pair generation (using pre-generated curriculum pairs)\")\n",
    "    log(f\"   Loaded from: {CONFIG['train_pairs_path']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c4f9e",
   "metadata": {
    "description": "Markdown: Model Training"
   },
   "source": [
    "# 6. Model Training with LoRA + Multi-Loss\n",
    "\n",
    "Train `all-mpnet-base-v2` with LoRA adapters and combined loss function.\n",
    "\n",
    "## ‚ú® NEW: LoRA/PEFT Integration\n",
    "\n",
    "**Why LoRA?**\n",
    "- **90%+ parameter reduction**: Only 3-5M trainable params vs 30-50M full fine-tune\n",
    "- **Faster training**: Less parameters = faster gradient updates\n",
    "- **Better generalization**: Prevents overfitting on smaller datasets\n",
    "- **Deployment flexibility**: Can merge adapters or serve separately\n",
    "\n",
    "**LoRA Configuration:**\n",
    "- **Rank (r)**: 16 (balances capacity vs efficiency)\n",
    "- **Alpha**: 32 (scaling factor, typically 2√ór)\n",
    "- **Target modules**: `q_proj`, `v_proj` (attention query/value projections)\n",
    "- **Learning rate**: 1e-4 (lower than full fine-tune's 2e-5)\n",
    "\n",
    "## ‚ú® NEW: Multi-Loss Training\n",
    "\n",
    "**Why combine MNRL + CosineSimilarity?**\n",
    "- **MultipleNegativesRankingLoss (90%)**: Primary loss for ranking quality\n",
    "  - In-batch negatives provide strong contrastive signal\n",
    "  - Optimizes for relative similarity ordering\n",
    "- **CosineSimilarityLoss (10%)**: Auxiliary loss for threshold calibration\n",
    "  - Calibrates absolute similarity scores\n",
    "  - Improves binary classification threshold selection\n",
    "  - Helps with F1/precision/recall optimization\n",
    "\n",
    "**V2 Curriculum Strategy:**\n",
    "- **Phase 1 (Epochs 1-2):** Easier pairs (25% hard negatives, threshold 0.15)\n",
    "- **Phase 2 (Epochs 3-4):** Harder pairs (55% hard negatives, threshold 0.10)\n",
    "\n",
    "This progressive difficulty helps the model learn basic patterns first, then refine on edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "device_detection",
   "metadata": {
    "description": "Device: Auto-detect CUDA/MPS/CPU and set DEVICE variable"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Using CUDA: NVIDIA GeForce RTX 5090\n",
      "   CUDA version: 12.8\n",
      "   Device capability: (12, 0)\n",
      "\n",
      "Device set to: cuda\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Device Detection (CUDA/MPS/CPU)\n",
    "# ========================================\n",
    "\n",
    "import torch\n",
    "\n",
    "# Auto-detect device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    log(f\"[OK] Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "    log(f\"   CUDA version: {torch.version.cuda}\")\n",
    "    log(f\"   Device capability: {torch.cuda.get_device_capability(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    log(\"[OK] Using MPS (Apple Silicon)\")\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    log(\"[OK] Using CPU\")\n",
    "\n",
    "log(f\"\\nDevice set to: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a5cd19f",
   "metadata": {
    "description": "Model Setup: LoRA initialization & loss functions"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading base model: sentence-transformers/all-mpnet-base-v2\n",
      "[TARGET] Applying LoRA adapters...\n",
      "   Rank (r): 16\n",
      "   Alpha: 32\n",
      "   Target modules (resolved): ['dense']\n",
      "   Dropout: 0.1\n",
      "[OK] LoRA Applied!\n",
      "   Trainable params: 1,499,136 (1.35%)\n",
      "   Total params: 110,985,600\n",
      "   Parameter reduction: 98.65%\n",
      "[OK] Model loaded on cuda, max_seq_length=256\n",
      "\n",
      "[STATS] Preparing training data...\n",
      "   Train pairs (all): 15,000\n",
      "   Eval pairs (all):  1,000\n",
      "   Train balance: 10,000 pos (66.7%), 5,000 neg (33.3%)\n",
      "üîß Using CosineSimilarityLoss (respects explicit labels)\n",
      "   Batch size: 16\n",
      "   Utilizes curated hard negatives: YES\n",
      "   Total training pairs: 15,000\n",
      "\n",
      "[STATS] Training Setup:\n",
      "   Batches per epoch: 938\n",
      "   Total training steps: 5628\n",
      "   Warmup steps: 562\n",
      "   Curriculum learning: True\n",
      "\n",
      "[STATS] Training Setup:\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers.evaluation import SentenceEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Custom Evaluator ---\n",
    "class ITSMEvaluator(SentenceEvaluator):\n",
    "    \"\"\"Evaluator for ITSM ticket similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, examples, batch_size=16, name=\"\"):\n",
    "        self.examples = examples\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "        \n",
    "        self.texts1 = [ex.texts[0] for ex in examples]\n",
    "        self.texts2 = [ex.texts[1] for ex in examples]\n",
    "        self.labels = np.array([ex.label for ex in examples])\n",
    "        \n",
    "        self.csv_file = f\"{name}_eval_results.csv\"\n",
    "        self.csv_headers = [\"epoch\", \"steps\", \"spearman\", \"pearson\", \"roc_auc\", \"pr_auc\"]\n",
    "    \n",
    "    def __call__(self, model, output_path=None, epoch=-1, steps=-1):\n",
    "        model.eval()\n",
    "        \n",
    "        # Encode pairs\n",
    "        emb1 = model.encode(self.texts1, batch_size=self.batch_size, \n",
    "                          show_progress_bar=False, convert_to_numpy=True)\n",
    "        emb2 = model.encode(self.texts2, batch_size=self.batch_size, \n",
    "                          show_progress_bar=False, convert_to_numpy=True)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        scores = np.sum(emb1 * emb2, axis=1) / (\n",
    "            np.linalg.norm(emb1, axis=1) * np.linalg.norm(emb2, axis=1) + 1e-8\n",
    "        )\n",
    "        \n",
    "        # Compute metrics\n",
    "        spearman, _ = spearmanr(self.labels, scores)\n",
    "        pearson, _ = pearsonr(self.labels, scores)\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(self.labels, scores)\n",
    "            pr_auc = average_precision_score(self.labels, scores)\n",
    "        except ValueError:\n",
    "            roc_auc, pr_auc = 0.0, 0.0\n",
    "        \n",
    "        log(f\"  [{self.name}] Epoch {epoch}: Spearman={spearman:.4f}, ROC-AUC={roc_auc:.4f}, PR-AUC={pr_auc:.4f}\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_path:\n",
    "            csv_path = Path(output_path) / self.csv_file\n",
    "            if not csv_path.exists():\n",
    "                with open(csv_path, 'w') as f:\n",
    "                    f.write(','.join(self.csv_headers) + '\\n')\n",
    "            with open(csv_path, 'a') as f:\n",
    "                f.write(f\"{epoch},{steps},{spearman},{pearson},{roc_auc},{pr_auc}\\n\")\n",
    "        \n",
    "        return spearman  # Primary metric\n",
    "\n",
    "\n",
    "# --- Model Initialization with LoRA/PEFT ---\n",
    "def init_model_with_lora(config, device):\n",
    "    \"\"\"Initialize model with optional LoRA/PEFT adapters.\"\"\"\n",
    "    # Clear GPU memory\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    log(f\"üîß Loading base model: {config['model_name']}\")\n",
    "    model = SentenceTransformer(config['model_name'], device=device)\n",
    "    # Cap max sequence length to reduce memory footprint\n",
    "    model.max_seq_length = min(config['max_seq_length'], 256)\n",
    "    \n",
    "    # Apply LoRA if enabled\n",
    "    if config['use_lora']:\n",
    "        from peft import LoraConfig, get_peft_model, TaskType\n",
    "        \n",
    "        # Inspect module names to find viable targets\n",
    "        base_model = model[0].auto_model\n",
    "        linear_names = [name for name, module in base_model.named_modules() if isinstance(module, nn.Linear)]\n",
    "        configured_targets = config['lora_target_modules']\n",
    "        resolved_targets = [t for t in configured_targets if any(t in n for n in linear_names)]\n",
    "        \n",
    "        if not resolved_targets:\n",
    "            # Fallback heuristics for BERT/MPNet-like models\n",
    "            priority = ['q_proj', 'k_proj', 'v_proj', 'query', 'key', 'value', 'dense']\n",
    "            resolved_targets = [p for p in priority if any(p in n for n in linear_names)]\n",
    "        \n",
    "        if not resolved_targets:\n",
    "            log(\"[ERROR] Could not find matching target modules for LoRA. Available linear submodules include:\")\n",
    "            sample = linear_names[:50]\n",
    "            for n in sample:\n",
    "                log(f\"   - {n}\")\n",
    "            raise ValueError(\"LoRA target_modules could not be resolved; please adjust CONFIG['lora_target_modules'].\")\n",
    "        \n",
    "        log(f\"[TARGET] Applying LoRA adapters...\")\n",
    "        log(f\"   Rank (r): {config['lora_r']}\")\n",
    "        log(f\"   Alpha: {config['lora_alpha']}\")\n",
    "        log(f\"   Target modules (resolved): {resolved_targets}\")\n",
    "        log(f\"   Dropout: {config['lora_dropout']}\")\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            r=config['lora_r'],\n",
    "            lora_alpha=config['lora_alpha'],\n",
    "            target_modules=resolved_targets,\n",
    "            lora_dropout=config['lora_dropout'],\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.FEATURE_EXTRACTION,\n",
    "        )\n",
    "        \n",
    "        # Apply PEFT wrapper\n",
    "        peft_model = get_peft_model(base_model, lora_config)\n",
    "        model[0].auto_model = peft_model\n",
    "        \n",
    "        # Enable gradient checkpointing to reduce memory\n",
    "        try:\n",
    "            model[0].auto_model.gradient_checkpointing_enable()\n",
    "            log(\"[BUILD] Gradient checkpointing enabled\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "        trainable_pct = 100 * trainable_params / total_params\n",
    "        \n",
    "        log(f\"[OK] LoRA Applied!\")\n",
    "        log(f\"   Trainable params: {trainable_params:,} ({trainable_pct:.2f}%)\")\n",
    "        log(f\"   Total params: {total_params:,}\")\n",
    "        log(f\"   Parameter reduction: {100 - trainable_pct:.2f}%\")\n",
    "    else:\n",
    "        log(f\"‚ÑπÔ∏è Full fine-tuning (no LoRA)\")\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        log(f\"   Total params: {total_params:,}\")\n",
    "    \n",
    "    log(f\"[OK] Model loaded on {device}, max_seq_length={model.max_seq_length}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# --- V2: Cross-Validated Threshold Selection ---\n",
    "def get_cv_threshold(examples, model, n_folds=5):\n",
    "    \"\"\"\n",
    "    V2 NEW: Use k-fold cross-validation to find robust threshold.\n",
    "    \n",
    "    Returns the average best threshold across folds.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve, f1_score\n",
    "    \n",
    "    texts1 = [ex.texts[0] for ex in examples]\n",
    "    texts2 = [ex.texts[1] for ex in examples]\n",
    "    labels = np.array([ex.label for ex in examples])\n",
    "    \n",
    "    # Encode all pairs\n",
    "    emb1 = model.encode(texts1, batch_size=CONFIG['batch_size'], show_progress_bar=False)\n",
    "    emb2 = model.encode(texts2, batch_size=CONFIG['batch_size'], show_progress_bar=False)\n",
    "    scores = np.sum(emb1 * emb2, axis=1) / (np.linalg.norm(emb1, axis=1) * np.linalg.norm(emb2, axis=1) + 1e-8)\n",
    "    \n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=CONFIG['seed'])\n",
    "    best_thresholds = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(scores)):\n",
    "        val_scores = scores[val_idx]\n",
    "        val_labels = labels[val_idx]\n",
    "        \n",
    "        precision, recall, thresholds = precision_recall_curve(val_labels, val_scores)\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        best_threshold = thresholds[best_idx-1] if 0 < best_idx < len(thresholds)+1 else 0.5\n",
    "        best_thresholds.append(best_threshold)\n",
    "    \n",
    "    avg_threshold = np.mean(best_thresholds)\n",
    "    std_threshold = np.std(best_thresholds)\n",
    "    \n",
    "    log(f\"[STATS] CV Threshold ({n_folds}-fold): {avg_threshold:.4f} ¬± {std_threshold:.4f}\")\n",
    "    log(f\"   Per-fold thresholds: {[f'{t:.3f}' for t in best_thresholds]}\")\n",
    "    \n",
    "    return avg_threshold, std_threshold\n",
    "\n",
    "\n",
    "# --- Multi-Loss Wrapper ---\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combines MultipleNegativesRankingLoss + CosineSimilarityLoss with a shared forward pass.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, mnrl_weight=0.9, cosine_weight=0.1, mnrl_scale=20.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.cosine = losses.CosineSimilarityLoss(model)\n",
    "        self.mnrl_weight = mnrl_weight\n",
    "        self.cosine_weight = cosine_weight\n",
    "        self.mnrl_scale = mnrl_scale\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        \n",
    "        log(f\"üîÄ Combined Loss Initialized:\")\n",
    "        log(f\"   MNRL weight: {mnrl_weight:.2f}\")\n",
    "        log(f\"   Cosine weight: {cosine_weight:.2f}\")\n",
    "        log(f\"   MNRL scale: {mnrl_scale:.1f}\")\n",
    "    \n",
    "    def forward(self, sentence_features, labels):\n",
    "        \"\"\"Single forward pass to compute embeddings, then both losses.\"\"\"\n",
    "        embeddings = [self.model(sentence_feature)[\"sentence_embedding\"] for sentence_feature in sentence_features]\n",
    "        # Normalize for cosine similarity consistency\n",
    "        emb1 = F.normalize(embeddings[0], p=2, dim=1)\n",
    "        emb2 = F.normalize(embeddings[1], p=2, dim=1)\n",
    "        \n",
    "        # MultipleNegativesRankingLoss (manual computation)\n",
    "        sim_matrix = torch.matmul(emb1, emb2.transpose(0, 1)) * self.mnrl_scale\n",
    "        mnrl_labels = torch.arange(sim_matrix.size(0), device=sim_matrix.device)\n",
    "        mnrl_loss = self.ce(sim_matrix, mnrl_labels)\n",
    "        \n",
    "        # CosineSimilarityLoss using precomputed embeddings\n",
    "        cosine_loss = self.cosine.compute_loss_from_embeddings([emb1, emb2], labels)\n",
    "        \n",
    "        return self.mnrl_weight * mnrl_loss + self.cosine_weight * cosine_loss\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = init_model_with_lora(CONFIG, DEVICE)\n",
    "\n",
    "# ‚ú® IMPROVEMENT: Use ALL training pairs (positives + negatives) instead of filtering\n",
    "log(f\"\\n[STATS] Preparing training data...\")\n",
    "log(f\"   Train pairs (all): {len(train_examples):,}\")\n",
    "log(f\"   Eval pairs (all):  {len(eval_examples):,}\")\n",
    "pos_count = sum(1 for ex in train_examples if ex.label == 1.0)\n",
    "neg_count = len(train_examples) - pos_count\n",
    "log(f\"   Train balance: {pos_count:,} pos ({pos_count/len(train_examples)*100:.1f}%), {neg_count:,} neg ({neg_count/len(train_examples)*100:.1f}%)\")\n",
    "\n",
    "# ‚ú® IMPROVEMENT: Increase batch size from 2 to 16 for better gradient estimates\n",
    "_train_batch = 16\n",
    "train_dataloader = DataLoader(\n",
    "    train_examples,  # Use ALL pairs, not just positives\n",
    "    shuffle=True,\n",
    "    batch_size=_train_batch,\n",
    "    num_workers=0,\n",
    "    pin_memory=(DEVICE in ['cuda', 'mps'])\n",
    ")\n",
    "# ‚ú® IMPROVEMENT: Use CosineSimilarityLoss to respect explicit positive/negative labels\n",
    "# This allows us to utilize all curated hard negative pairs instead of discarding them\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "log(f\"üîß Using CosineSimilarityLoss (respects explicit labels)\")\n",
    "log(f\"   Batch size: {_train_batch}\")\n",
    "log(f\"   Utilizes curated hard negatives: YES\")\n",
    "log(f\"   Total training pairs: {len(train_examples):,}\")\n",
    "\n",
    "# Evaluator\n",
    "_evaluator_batch = _train_batch\n",
    "_evaluator_examples = eval_examples\n",
    "_evaluator = ITSMEvaluator(_evaluator_examples, batch_size=_evaluator_batch, name=\"eval\")\n",
    "evaluator = _evaluator\n",
    "\n",
    "log(f\"\\n[STATS] Training Setup:\")\n",
    "log(f\"   Batches per epoch: {len(train_dataloader)}\")\n",
    "log(f\"   Total training steps: {len(train_dataloader) * CONFIG['epochs']}\")\n",
    "log(f\"   Warmup steps: {int(len(train_dataloader) * CONFIG['epochs'] * CONFIG['warmup_ratio'])}\")\n",
    "log(f\"   Curriculum learning: {CONFIG['use_curriculum']}\")\n",
    "log(f\"\\n[STATS] Training Setup:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9e2fb72",
   "metadata": {
    "description": "Training: Main training loop with curriculum learning"
   },
   "outputs": [],
   "source": [
    "# Skip if using pre-generated pairs\n",
    "if not CONFIG.get('use_pre_generated_pairs', False):\n",
    "    \n",
    "    # --- Training Execution (V2: with Curriculum Learning) ---\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    save_path = Path(CONFIG['output_dir']) / f\"real_servicenow_v2_{timestamp}\"\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    log(f\"\\nüöÄ Starting Training (V2)...\")\n",
    "    log(f\"   Output: {save_path}\")\n",
    "    log(f\"   Epochs: {CONFIG['epochs']}\")\n",
    "    log(f\"   Device: {DEVICE}\")\n",
    "\n",
    "    # Calculate warmup steps\n",
    "    total_steps = len(train_dataloader) * CONFIG['epochs']\n",
    "    warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "    eval_steps = max(100, len(train_dataloader) // 2)  # Evaluate twice per epoch\n",
    "    use_amp = DEVICE != 'cuda'  # Flip: CUDA stays fp32, CPU/MPS use fp16 autocast when available\n",
    "\n",
    "    try:\n",
    "        if CONFIG['use_curriculum']:\n",
    "            # V2: Curriculum Learning - train in phases\n",
    "            log(\"\\nüìö CURRICULUM LEARNING ENABLED\")\n",
    "        \n",
    "            for phase_idx, phase in enumerate(CONFIG['curriculum_phases']):\n",
    "                log(f\"\\n{'='*60}\")\n",
    "                log(f\"üìö Phase {phase_idx + 1}: {phase['epochs']} epochs\")\n",
    "                log(f\"   Hard neg ratio: {phase['hard_neg_ratio']*100:.0f}%\")\n",
    "                log(f\"   Neg threshold: {phase['neg_threshold']}\")\n",
    "                log(f\"{'='*60}\")\n",
    "            \n",
    "                # Regenerate training pairs for this phase (reuse train TF-IDF)\n",
    "                phase_train_examples = generate_training_pairs(\n",
    "                    train_df, \n",
    "                    len(train_examples),  # ‚ú® Use all pairs count, not just positives\n",
    "                    CONFIG, \n",
    "                    desc=f\"Phase {phase_idx+1}\",\n",
    "                    phase_config=phase,\n",
    "                    tfidf_calc=tfidf_train\n",
    "                )\n",
    "                # ‚ú® IMPROVEMENT: Use all pairs (pos + neg), not just positives\n",
    "            \n",
    "                phase_batch_size = 16  # ‚ú® Increased from 2 to 16\n",
    "                phase_dataloader = DataLoader(\n",
    "                    phase_train_examples,  # Use all pairs\n",
    "                    shuffle=True,\n",
    "                    batch_size=phase_batch_size,\n",
    "                    num_workers=0,\n",
    "                    pin_memory=(DEVICE in ['cuda', 'mps'])\n",
    "                )\n",
    "            \n",
    "                phase_warmup = int(len(phase_dataloader) * phase['epochs'] * CONFIG['warmup_ratio'])\n",
    "            \n",
    "                # Extra cleanup before each phase to avoid fragmentation\n",
    "                try:\n",
    "                    if DEVICE == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "                model.fit(\n",
    "                    train_objectives=[(phase_dataloader, train_loss)],\n",
    "                    evaluator=evaluator,\n",
    "                    epochs=phase['epochs'],\n",
    "                    warmup_steps=phase_warmup,\n",
    "                    optimizer_params={'lr': 2e-5},  # ‚ú® Reduced from 1e-4 to 2e-5 for LoRA\n",
    "                    output_path=str(save_path),\n",
    "                    evaluation_steps=eval_steps,\n",
    "                    save_best_model=True,\n",
    "                    show_progress_bar=True,\n",
    "                    use_amp=use_amp,\n",
    "                )\n",
    "            \n",
    "                log(f\"[OK] Phase {phase_idx + 1} complete!\")\n",
    "        else:\n",
    "            # Standard training (no curriculum)\n",
    "            try:\n",
    "                if DEVICE == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "            model.fit(\n",
    "                train_objectives=[(train_dataloader, train_loss)],\n",
    "                evaluator=evaluator,\n",
    "                epochs=CONFIG['epochs'],\n",
    "                warmup_steps=warmup_steps,\n",
    "                optimizer_params={'lr': 2e-5},  # ‚ú® Reduced from 1e-4 to 2e-5 for LoRA\n",
    "                output_path=str(save_path),\n",
    "                evaluation_steps=eval_steps,\n",
    "                save_best_model=True,\n",
    "                show_progress_bar=True,\n",
    "                use_amp=use_amp,\n",
    "            )\n",
    "    \n",
    "        log(\"\\n[OK] Training complete!\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        err_msg = str(e).lower()\n",
    "        if (\"out of memory\" in err_msg) or (\"no kernel image\" in err_msg) or (\"not compatible\" in err_msg):\n",
    "            log(f\"[ERROR] Runtime Error: {e}\")\n",
    "            log(\"üí° Falling back to CPU/MPS to continue training...\")\n",
    "            # Cleanup GPU\n",
    "            try:\n",
    "                del model\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            except Exception:\n",
    "                pass\n",
    "            gc.collect()\n",
    "        \n",
    "            # Fallback device\n",
    "            if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "                DEVICE = 'mps'\n",
    "                log(\"üçé Switching to MPS\")\n",
    "            else:\n",
    "                DEVICE = 'cpu'\n",
    "                log(\"[BUILD] Switching to CPU\")\n",
    "        \n",
    "            # Re-init model and loss on fallback device\n",
    "            model = init_model_with_lora(CONFIG, DEVICE)\n",
    "            # Always use CosineSimilarityLoss with all pairs\n",
    "            train_loss = losses.CosineSimilarityLoss(model)\n",
    "        \n",
    "            # Recreate dataloaders with safe batch size on fallback device\n",
    "            safe_batch = 8 if DEVICE != 'cuda' else 16\n",
    "            fallback_use_amp = DEVICE != 'cuda'\n",
    "            if CONFIG['use_curriculum']:\n",
    "                for phase_idx, phase in enumerate(CONFIG['curriculum_phases']):\n",
    "                    log(f\"\\n[Fallback] Phase {phase_idx + 1}: {phase['epochs']} epochs\")\n",
    "                    phase_train_examples = generate_training_pairs(\n",
    "                        train_df,\n",
    "                        len(train_examples),\n",
    "                        CONFIG,\n",
    "                        desc=f\"Phase {phase_idx+1}\",\n",
    "                        phase_config=phase,\n",
    "                        tfidf_calc=tfidf_train\n",
    "                    )\n",
    "                    phase_dataloader = DataLoader(\n",
    "                        phase_train_examples,\n",
    "                        shuffle=True,\n",
    "                        batch_size=safe_batch,\n",
    "                        num_workers=0,\n",
    "                        pin_memory=(DEVICE in ['cuda', 'mps'])\n",
    "                    )\n",
    "                    phase_warmup = int(len(phase_dataloader) * phase['epochs'] * CONFIG['warmup_ratio'])\n",
    "                    model.fit(\n",
    "                        train_objectives=[(phase_dataloader, train_loss)],\n",
    "                        evaluator=evaluator,\n",
    "                        epochs=phase['epochs'],\n",
    "                        warmup_steps=phase_warmup,\n",
    "                        optimizer_params={'lr': CONFIG['lr']},\n",
    "                        output_path=str(save_path),\n",
    "                        evaluation_steps=eval_steps,\n",
    "                        save_best_model=True,\n",
    "                        show_progress_bar=True,\n",
    "                        use_amp=fallback_use_amp,\n",
    "                    )\n",
    "            else:\n",
    "                model.fit(\n",
    "                    train_objectives=[(train_dataloader, train_loss)],\n",
    "                    evaluator=evaluator,\n",
    "                    epochs=CONFIG['epochs'],\n",
    "                    warmup_steps=warmup_steps,\n",
    "                    optimizer_params={'lr': CONFIG['lr']},\n",
    "                    output_path=str(save_path),\n",
    "                    evaluation_steps=eval_steps,\n",
    "                    save_best_model=True,\n",
    "                    show_progress_bar=True,\n",
    "                    use_amp=fallback_use_amp,\n",
    "                )\n",
    "        \n",
    "            log(\"\\n[OK] Training complete on fallback device!\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # Reload best model\n",
    "    log(\"\\n[STATS] Loading best model for final evaluation...\")\n",
    "    best_model = SentenceTransformer(str(save_path), device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85294299",
   "metadata": {
    "description": "Markdown: Evaluation"
   },
   "source": [
    "# 7. Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a013b458",
   "metadata": {
    "description": "Evaluation: Score distribution diagnostic"
   },
   "source": [
    "# 6.5 Score Distribution Diagnostic (V2: Verify Model Learned Separation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4ce86a2",
   "metadata": {
    "description": "Evaluation: Cross-validation threshold function"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç SCORE DISTRIBUTION DIAGNOSTIC\n",
      "============================================================\n",
      "Analyzing 250 positive + 250 negative pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positive pairs:   0%|          | 0/250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m pos_scores = []\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m tqdm(sample_pos, desc=\u001b[33m\"\u001b[39m\u001b[33mPositive pairs\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     emb1 = \u001b[43mbest_model\u001b[49m.encode([ex.texts[\u001b[32m0\u001b[39m]], show_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     20\u001b[39m     emb2 = best_model.encode([ex.texts[\u001b[32m1\u001b[39m]], show_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     21\u001b[39m     score = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + \u001b[32m1e-8\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# SCORE DISTRIBUTION DIAGNOSTIC (V2)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# This validates that the model learned to separate positive/negative pairs\n",
    "\n",
    "log(\"\\nüîç SCORE DISTRIBUTION DIAGNOSTIC\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Sample from eval set\n",
    "sample_size = min(500, len(eval_examples))\n",
    "sample_pos = [ex for ex in eval_examples if ex.label == 1.0][:sample_size//2]\n",
    "sample_neg = [ex for ex in eval_examples if ex.label == 0.0][:sample_size//2]\n",
    "\n",
    "log(f\"Analyzing {len(sample_pos)} positive + {len(sample_neg)} negative pairs...\")\n",
    "\n",
    "# Compute scores for positive pairs\n",
    "pos_scores = []\n",
    "for ex in tqdm(sample_pos, desc=\"Positive pairs\"):\n",
    "    emb1 = best_model.encode([ex.texts[0]], show_progress_bar=False)[0]\n",
    "    emb2 = best_model.encode([ex.texts[1]], show_progress_bar=False)[0]\n",
    "    score = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-8)\n",
    "    pos_scores.append(score)\n",
    "\n",
    "# Compute scores for negative pairs\n",
    "neg_scores = []\n",
    "for ex in tqdm(sample_neg, desc=\"Negative pairs\"):\n",
    "    emb1 = best_model.encode([ex.texts[0]], show_progress_bar=False)[0]\n",
    "    emb2 = best_model.encode([ex.texts[1]], show_progress_bar=False)[0]\n",
    "    score = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-8)\n",
    "    neg_scores.append(score)\n",
    "\n",
    "# Statistics\n",
    "pos_mean, pos_std = np.mean(pos_scores), np.std(pos_scores)\n",
    "neg_mean, neg_std = np.mean(neg_scores), np.std(neg_scores)\n",
    "separability = pos_mean - neg_mean\n",
    "\n",
    "log(f\"\\n[STATS] SCORE STATISTICS:\")\n",
    "log(f\"   Positive pairs: mean={pos_mean:.4f}, std={pos_std:.4f}, range=[{min(pos_scores):.4f}, {max(pos_scores):.4f}]\")\n",
    "log(f\"   Negative pairs: mean={neg_mean:.4f}, std={neg_std:.4f}, range=[{min(neg_scores):.4f}, {max(neg_scores):.4f}]\")\n",
    "log(f\"   Separability (Œî): {separability:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "log(f\"\\nüí° INTERPRETATION:\")\n",
    "if separability >= 0.15:\n",
    "    log(f\"   [OK] EXCELLENT: Strong separation (Œî={separability:.4f} >= 0.15)\")\n",
    "    log(f\"      Model clearly distinguishes positive from negative pairs\")\n",
    "elif separability >= 0.08:\n",
    "    log(f\"   OK GOOD: Moderate separation (Œî={separability:.4f} >= 0.08)\")\n",
    "    log(f\"      Model shows useful discrimination\")\n",
    "elif separability >= 0.03:\n",
    "    log(f\"   [WARN]  WEAK: Minimal separation (Œî={separability:.4f} >= 0.03)\")\n",
    "    log(f\"      Model barely learned to discriminate\")\n",
    "else:\n",
    "    log(f\"   [ERROR] FAILURE: No separation (Œî={separability:.4f} < 0.03)\")\n",
    "    log(f\"      Model produces similar scores for both positive and negative pairs!\")\n",
    "    log(f\"      Training likely failed - check batch size, loss function, or data quality\")\n",
    "\n",
    "# Distribution overlap check\n",
    "overlap_threshold = 0.5\n",
    "overlap_count = sum(1 for s in neg_scores if s > overlap_threshold)\n",
    "log(f\"\\n[STATS] DISTRIBUTION OVERLAP:\")\n",
    "log(f\"   Negatives scoring > {overlap_threshold}: {overlap_count}/{len(neg_scores)} ({overlap_count/len(neg_scores)*100:.1f}%)\")\n",
    "if overlap_count / len(neg_scores) > 0.3:\n",
    "    log(f\"   [WARN]  WARNING: High overlap - many negatives score like positives\")\n",
    "\n",
    "log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d094e89",
   "metadata": {
    "description": "Evaluation: Comprehensive evaluation function"
   },
   "outputs": [],
   "source": [
    "# Skip borderline evaluation if not using legacy mode\n",
    "if borderline_examples and len(borderline_examples) > 0:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import (\n",
    "        roc_curve, precision_recall_curve, confusion_matrix,\n",
    "        roc_auc_score, average_precision_score,\n",
    "        precision_score, recall_score, f1_score, accuracy_score\n",
    "    )\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    def comprehensive_eval(examples, model, name=\"\", use_cv_threshold=False):\n",
    "        \"\"\"Run comprehensive evaluation on a set of pairs (V2: with CV threshold option).\"\"\"\n",
    "        texts1 = [ex.texts[0] for ex in examples]\n",
    "        texts2 = [ex.texts[1] for ex in examples]\n",
    "        labels = np.array([ex.label for ex in examples])\n",
    "    \n",
    "        # Encode\n",
    "        log(f\"‚è≥ Encoding {len(examples)} pairs for {name}...\")\n",
    "        emb1 = model.encode(texts1, batch_size=CONFIG['batch_size'], \n",
    "                           show_progress_bar=True, convert_to_numpy=True)\n",
    "        emb2 = model.encode(texts2, batch_size=CONFIG['batch_size'], \n",
    "                           show_progress_bar=True, convert_to_numpy=True)\n",
    "    \n",
    "        # Cosine similarity\n",
    "        scores = np.sum(emb1 * emb2, axis=1) / (\n",
    "            np.linalg.norm(emb1, axis=1) * np.linalg.norm(emb2, axis=1) + 1e-8\n",
    "        )\n",
    "    \n",
    "        # Metrics\n",
    "        spearman, _ = spearmanr(labels, scores)\n",
    "        pearson, _ = pearsonr(labels, scores)\n",
    "        roc_auc = roc_auc_score(labels, scores)\n",
    "        pr_auc = average_precision_score(labels, scores)\n",
    "    \n",
    "        # Find best threshold\n",
    "        fpr, tpr, roc_thresholds = roc_curve(labels, scores)\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(labels, scores)\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        best_threshold = pr_thresholds[best_idx-1] if 0 < best_idx < len(pr_thresholds)+1 else 0.5\n",
    "    \n",
    "        # V2: Use CV threshold if requested\n",
    "        if use_cv_threshold and 'cv_threshold' in globals():\n",
    "            best_threshold = cv_threshold\n",
    "            log(f\"   Using CV threshold: {best_threshold:.4f}\")\n",
    "    \n",
    "        # Metrics at best threshold\n",
    "        preds = (scores >= best_threshold).astype(int)\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        prec = precision_score(labels, preds)\n",
    "        rec = recall_score(labels, preds)\n",
    "        f1 = f1_score(labels, preds)\n",
    "        cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "        log(f\"\\n[STATS] {name} Results:\")\n",
    "        log(f\"   Spearman:  {spearman:.4f}\")\n",
    "        log(f\"   Pearson:   {pearson:.4f}\")\n",
    "        log(f\"   ROC-AUC:   {roc_auc:.4f}\")\n",
    "        log(f\"   PR-AUC:    {pr_auc:.4f}\")\n",
    "        log(f\"   Best Threshold: {best_threshold:.3f}\")\n",
    "        log(f\"   F1 @ best: {f1:.4f}\")\n",
    "        log(f\"   Precision: {prec:.4f}\")\n",
    "        log(f\"   Recall:    {rec:.4f}\")\n",
    "        log(f\"   Accuracy:  {acc:.4f}\")\n",
    "    \n",
    "        return {\n",
    "            'labels': labels, 'scores': scores,\n",
    "            'spearman': spearman, 'pearson': pearson,\n",
    "            'roc_auc': roc_auc, 'pr_auc': pr_auc,\n",
    "            'fpr': fpr, 'tpr': tpr,\n",
    "            'precision': precision, 'recall': recall,\n",
    "            'best_threshold': best_threshold,\n",
    "            'f1': f1, 'prec': prec, 'rec': rec, 'acc': acc,\n",
    "            'cm': cm, 'texts1': texts1, 'texts2': texts2  # V2: Keep texts for error analysis\n",
    "        }\n",
    "\n",
    "    # V2: Get cross-validated threshold first\n",
    "    log(\"=\"*60)\n",
    "    log(\"üìà CROSS-VALIDATED THRESHOLD SELECTION (V2)\")\n",
    "    log(\"=\"*60)\n",
    "    cv_threshold, cv_std = get_cv_threshold(eval_examples, best_model, n_folds=CONFIG['threshold_cv_folds'])\n",
    "\n",
    "    # Evaluate on all sets\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"üìà FINAL EVALUATION\")\n",
    "    log(\"=\"*60)\n",
    "\n",
    "    eval_results = comprehensive_eval(eval_examples, best_model, \"Eval Set\")\n",
    "    holdout_results = comprehensive_eval(holdout_examples, best_model, \"Holdout Set\")\n",
    "\n",
    "    # V2 NEW: Evaluate on borderline (harder) test set\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"üìà BORDERLINE TEST (V2 - Harder Evaluation)\")\n",
    "    log(\"=\"*60)\n",
    "    borderline_results = comprehensive_eval(borderline_examples, best_model, \"Borderline Set\")\n",
    "else:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"[SKIP] Borderline Test (not applicable for curriculum training)\")\n",
    "    log(\"=\"*60)\n",
    "    log(\"Using curriculum learning - borderline test not generated.\")\n",
    "    log(\"Evaluation uses fixed_test_pairs.json instead.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48806de7",
   "metadata": {
    "description": "Evaluation: Run final evaluation on all test sets"
   },
   "outputs": [],
   "source": [
    "# Visualization (V2: Added borderline results)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 14))\n",
    "\n",
    "# ROC Curves\n",
    "axes[0,0].plot(eval_results['fpr'], eval_results['tpr'], \n",
    "               label=f\"Eval ROC-AUC = {eval_results['roc_auc']:.3f}\")\n",
    "axes[0,0].plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "axes[0,0].set_title('ROC Curve (Eval)')\n",
    "axes[0,0].set_xlabel('False Positive Rate')\n",
    "axes[0,0].set_ylabel('True Positive Rate')\n",
    "axes[0,0].legend()\n",
    "\n",
    "axes[0,1].plot(holdout_results['fpr'], holdout_results['tpr'], \n",
    "               label=f\"Holdout ROC-AUC = {holdout_results['roc_auc']:.3f}\", color='orange')\n",
    "axes[0,1].plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "axes[0,1].set_title('ROC Curve (Holdout)')\n",
    "axes[0,1].set_xlabel('False Positive Rate')\n",
    "axes[0,1].set_ylabel('True Positive Rate')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# V2 NEW: Borderline ROC\n",
    "axes[0,2].plot(borderline_results['fpr'], borderline_results['tpr'], \n",
    "               label=f\"Borderline ROC-AUC = {borderline_results['roc_auc']:.3f}\", color='red')\n",
    "axes[0,2].plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "axes[0,2].set_title('ROC Curve (Borderline - V2)')\n",
    "axes[0,2].set_xlabel('False Positive Rate')\n",
    "axes[0,2].set_ylabel('True Positive Rate')\n",
    "axes[0,2].legend()\n",
    "\n",
    "# PR Curves\n",
    "axes[1,0].plot(eval_results['recall'], eval_results['precision'], \n",
    "               label=f\"Eval PR-AUC = {eval_results['pr_auc']:.3f}\")\n",
    "axes[1,0].scatter([eval_results['rec']], [eval_results['prec']], \n",
    "                  color='red', s=100, zorder=5,\n",
    "                  label=f\"Best F1={eval_results['f1']:.3f}\")\n",
    "axes[1,0].set_title('Precision-Recall (Eval)')\n",
    "axes[1,0].set_xlabel('Recall')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].legend()\n",
    "\n",
    "axes[1,1].plot(holdout_results['recall'], holdout_results['precision'], \n",
    "               label=f\"Holdout PR-AUC = {holdout_results['pr_auc']:.3f}\", color='orange')\n",
    "axes[1,1].scatter([holdout_results['rec']], [holdout_results['prec']], \n",
    "                  color='red', s=100, zorder=5,\n",
    "                  label=f\"Best F1={holdout_results['f1']:.3f}\")\n",
    "axes[1,1].set_title('Precision-Recall (Holdout)')\n",
    "axes[1,1].set_xlabel('Recall')\n",
    "axes[1,1].set_ylabel('Precision')\n",
    "axes[1,1].legend()\n",
    "\n",
    "# V2 NEW: Borderline PR\n",
    "axes[1,2].plot(borderline_results['recall'], borderline_results['precision'], \n",
    "               label=f\"Borderline PR-AUC = {borderline_results['pr_auc']:.3f}\", color='red')\n",
    "axes[1,2].scatter([borderline_results['rec']], [borderline_results['prec']], \n",
    "                  color='green', s=100, zorder=5,\n",
    "                  label=f\"Best F1={borderline_results['f1']:.3f}\")\n",
    "axes[1,2].set_title('Precision-Recall (Borderline - V2)')\n",
    "axes[1,2].set_xlabel('Recall')\n",
    "axes[1,2].set_ylabel('Precision')\n",
    "axes[1,2].legend()\n",
    "\n",
    "# Score Distributions\n",
    "for idx, (results, name, color) in enumerate([\n",
    "    (eval_results, 'Eval', 'blue'), \n",
    "    (holdout_results, 'Holdout', 'orange'),\n",
    "    (borderline_results, 'Borderline', 'red')\n",
    "]):\n",
    "    ax = axes[2, idx]\n",
    "    neg_scores = results['scores'][results['labels'] == 0]\n",
    "    pos_scores = results['scores'][results['labels'] == 1]\n",
    "    \n",
    "    ax.hist(neg_scores, bins=30, alpha=0.6, label='Negative (0)', color='blue')\n",
    "    ax.hist(pos_scores, bins=30, alpha=0.6, label='Positive (1)', color='orange')\n",
    "    ax.axvline(results['best_threshold'], color='red', linestyle='--', \n",
    "               label=f\"Threshold={results['best_threshold']:.3f}\")\n",
    "    ax.axvline(cv_threshold, color='green', linestyle=':', \n",
    "               label=f\"CV Threshold={cv_threshold:.3f}\")\n",
    "    ax.set_title(f'Score Distribution ({name})')\n",
    "    ax.set_xlabel('Cosine Similarity')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path / 'evaluation_plots_v2.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "log(f\"\\n[STATS] Plots saved to {save_path / 'evaluation_plots_v2.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa08e9",
   "metadata": {
    "description": "Markdown: Visualization"
   },
   "source": [
    "# 7.1 Error Analysis (V2 NEW)\n",
    "\n",
    "Systematically analyze failure patterns to identify:\n",
    "1. **Worst False Positives**: High-scoring pairs that should be dissimilar\n",
    "2. **Worst False Negatives**: Low-scoring pairs that should be similar\n",
    "3. **Category/text length patterns** in errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda41096",
   "metadata": {
    "description": "Visualization: Plot training metrics"
   },
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# V2 NEW: ERROR ANALYSIS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def analyze_errors(results, name=\"\", top_k=10):\n",
    "    \"\"\"\n",
    "    Analyze systematic failure patterns in model predictions.\n",
    "    \n",
    "    Returns insights about:\n",
    "    - Worst false positives (high score, label=0)\n",
    "    - Worst false negatives (low score, label=1)\n",
    "    - Text length patterns\n",
    "    \"\"\"\n",
    "    labels = results['labels']\n",
    "    scores = results['scores']\n",
    "    texts1 = results['texts1']\n",
    "    texts2 = results['texts2']\n",
    "    threshold = results['best_threshold']\n",
    "    \n",
    "    # Identify errors\n",
    "    preds = (scores >= threshold).astype(int)\n",
    "    \n",
    "    # False Positives: predicted 1, actual 0\n",
    "    fp_mask = (preds == 1) & (labels == 0)\n",
    "    fp_indices = np.where(fp_mask)[0]\n",
    "    fp_scores = scores[fp_mask]\n",
    "    \n",
    "    # False Negatives: predicted 0, actual 1\n",
    "    fn_mask = (preds == 0) & (labels == 1)\n",
    "    fn_indices = np.where(fn_mask)[0]\n",
    "    fn_scores = scores[fn_mask]\n",
    "    \n",
    "    log(f\"\\n{'='*60}\")\n",
    "    log(f\"üîç ERROR ANALYSIS: {name}\")\n",
    "    log(f\"{'='*60}\")\n",
    "    log(f\"Total pairs: {len(labels):,}\")\n",
    "    log(f\"False Positives: {len(fp_indices):,} ({len(fp_indices)/len(labels)*100:.2f}%)\")\n",
    "    log(f\"False Negatives: {len(fn_indices):,} ({len(fn_indices)/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    # Worst False Positives (highest scoring negatives)\n",
    "    if len(fp_indices) > 0:\n",
    "        log(f\"\\nüìõ WORST FALSE POSITIVES (Top {min(top_k, len(fp_indices))}):\")\n",
    "        log(f\"   These pairs scored HIGH but should be DISSIMILAR\")\n",
    "        worst_fp_order = np.argsort(fp_scores)[::-1][:top_k]\n",
    "        \n",
    "        for rank, idx in enumerate(worst_fp_order):\n",
    "            orig_idx = fp_indices[idx]\n",
    "            score = scores[orig_idx]\n",
    "            t1, t2 = texts1[orig_idx], texts2[orig_idx]\n",
    "            log(f\"\\n   [{rank+1}] Score: {score:.4f}\")\n",
    "            log(f\"       Text 1: {t1[:100]}...\")\n",
    "            log(f\"       Text 2: {t2[:100]}...\")\n",
    "    \n",
    "    # Worst False Negatives (lowest scoring positives)\n",
    "    if len(fn_indices) > 0:\n",
    "        log(f\"\\nüìõ WORST FALSE NEGATIVES (Top {min(top_k, len(fn_indices))}):\")\n",
    "        log(f\"   These pairs scored LOW but should be SIMILAR\")\n",
    "        worst_fn_order = np.argsort(fn_scores)[:top_k]\n",
    "        \n",
    "        for rank, idx in enumerate(worst_fn_order):\n",
    "            orig_idx = fn_indices[idx]\n",
    "            score = scores[orig_idx]\n",
    "            t1, t2 = texts1[orig_idx], texts2[orig_idx]\n",
    "            log(f\"\\n   [{rank+1}] Score: {score:.4f}\")\n",
    "            log(f\"       Text 1: {t1[:100]}...\")\n",
    "            log(f\"       Text 2: {t2[:100]}...\")\n",
    "    \n",
    "    # Text length analysis\n",
    "    log(f\"\\nüìè TEXT LENGTH ANALYSIS:\")\n",
    "    all_lengths = [len(t) for t in texts1 + texts2]\n",
    "    fp_lengths = [len(texts1[i]) + len(texts2[i]) for i in fp_indices] if len(fp_indices) > 0 else [0]\n",
    "    fn_lengths = [len(texts1[i]) + len(texts2[i]) for i in fn_indices] if len(fn_indices) > 0 else [0]\n",
    "    \n",
    "    log(f\"   Overall avg length: {np.mean(all_lengths):.0f} chars\")\n",
    "    log(f\"   FP pairs avg length: {np.mean(fp_lengths):.0f} chars\")\n",
    "    log(f\"   FN pairs avg length: {np.mean(fn_lengths):.0f} chars\")\n",
    "    \n",
    "    # Score distribution in errors\n",
    "    log(f\"\\n[STATS] SCORE DISTRIBUTION IN ERRORS:\")\n",
    "    if len(fp_scores) > 0:\n",
    "        log(f\"   FP scores: min={fp_scores.min():.4f}, max={fp_scores.max():.4f}, mean={fp_scores.mean():.4f}\")\n",
    "    if len(fn_scores) > 0:\n",
    "        log(f\"   FN scores: min={fn_scores.min():.4f}, max={fn_scores.max():.4f}, mean={fn_scores.mean():.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'fp_count': len(fp_indices),\n",
    "        'fn_count': len(fn_indices),\n",
    "        'fp_scores': fp_scores,\n",
    "        'fn_scores': fn_scores,\n",
    "        'fp_avg_length': np.mean(fp_lengths),\n",
    "        'fn_avg_length': np.mean(fn_lengths)\n",
    "    }\n",
    "\n",
    "# Run error analysis on all sets\n",
    "eval_errors = analyze_errors(eval_results, \"Eval Set\", top_k=5)\n",
    "holdout_errors = analyze_errors(holdout_results, \"Holdout Set\", top_k=5)\n",
    "borderline_errors = analyze_errors(borderline_results, \"Borderline Set\", top_k=5)\n",
    "\n",
    "# Summary comparison\n",
    "log(f\"\\n{'='*60}\")\n",
    "log(f\"[STATS] ERROR SUMMARY COMPARISON\")\n",
    "log(f\"{'='*60}\")\n",
    "log(f\"{'Set':<15} {'FP Count':<12} {'FN Count':<12} {'FP Rate':<12} {'FN Rate':<12}\")\n",
    "log(f\"{'-'*60}\")\n",
    "for name, errors, results in [\n",
    "    (\"Eval\", eval_errors, eval_results),\n",
    "    (\"Holdout\", holdout_errors, holdout_results),\n",
    "    (\"Borderline\", borderline_errors, borderline_results)\n",
    "]:\n",
    "    total = len(results['labels'])\n",
    "    fp_rate = errors['fp_count'] / total * 100\n",
    "    fn_rate = errors['fn_count'] / total * 100\n",
    "    log(f\"{name:<15} {errors['fp_count']:<12} {errors['fn_count']:<12} {fp_rate:<12.2f}% {fn_rate:<12.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f0d5d",
   "metadata": {
    "description": "Markdown: Save Model"
   },
   "source": [
    "# 8. Adversarial Diagnostic (V2: Enhanced)\n",
    "\n",
    "**Critical validation**: Test if the model learned semantic content or is exploiting category shortcuts.\n",
    "\n",
    "- **Hard Positives**: Cross-category pairs with HIGH content similarity\n",
    "- **Hard Negatives**: Same-category pairs with LOW content similarity\n",
    "\n",
    "**V2 Enhancements:**\n",
    "- Stricter TF-IDF thresholds for adversarial pairs\n",
    "- Compare with borderline test performance\n",
    "\n",
    "**Pass Criteria**: ROC-AUC ‚â• 0.70 AND F1 ‚â• 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94893294",
   "metadata": {
    "description": "Save: Save trained model & metadata"
   },
   "outputs": [],
   "source": [
    "log(\"=\"*60)\n",
    "log(\"üî¨ ADVERSARIAL DIAGNOSTIC: Testing Category Leakage\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Use holdout data for adversarial test (completely unseen)\n",
    "diag_df = holdout_df.reset_index(drop=True)\n",
    "\n",
    "# Build content-only text (remove category context to test pure semantic understanding)\n",
    "diag_df['content_only'] = diag_df['Description'].str.strip()\n",
    "\n",
    "# Build TF-IDF on content-only text\n",
    "log(\"‚è≥ Building TF-IDF for adversarial pair mining...\")\n",
    "diag_tfidf = TFIDFSimilarityCalculator(diag_df['content_only'].tolist(), max_features=10000)\n",
    "\n",
    "# Generate adversarial pairs\n",
    "hard_positives = []  # Cross-category, high TF-IDF\n",
    "hard_negatives = []  # Same-category, low TF-IDF\n",
    "\n",
    "target_each = 300\n",
    "attempts, max_attempts = 0, 100000\n",
    "\n",
    "log(\"‚è≥ Mining adversarial pairs...\")\n",
    "pbar = tqdm(total=target_each * 2, desc=\"Adversarial pairs\")\n",
    "\n",
    "while (len(hard_positives) < target_each or len(hard_negatives) < target_each) and attempts < max_attempts:\n",
    "    attempts += 1\n",
    "    i1, i2 = random.sample(range(len(diag_df)), 2)\n",
    "    \n",
    "    cat1 = diag_df.at[i1, 'category_id']\n",
    "    cat2 = diag_df.at[i2, 'category_id']\n",
    "    tfidf_sim = diag_tfidf.similarity(i1, i2)\n",
    "    \n",
    "    # Hard Positive: DIFFERENT category but HIGH content similarity\n",
    "    if len(hard_positives) < target_each and cat1 != cat2 and tfidf_sim > 0.4:\n",
    "        hard_positives.append(InputExample(\n",
    "            texts=[diag_df.at[i1, 'content_only'], diag_df.at[i2, 'content_only']],\n",
    "            label=1.0\n",
    "        ))\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Hard Negative: SAME category but LOW content similarity\n",
    "    if len(hard_negatives) < target_each and cat1 == cat2 and tfidf_sim < 0.15:\n",
    "        hard_negatives.append(InputExample(\n",
    "            texts=[diag_df.at[i1, 'content_only'], diag_df.at[i2, 'content_only']],\n",
    "            label=0.0\n",
    "        ))\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "log(f\"[OK] Generated {len(hard_positives)} hard positives, {len(hard_negatives)} hard negatives\")\n",
    "\n",
    "# Evaluate on adversarial pairs\n",
    "adversarial_examples = hard_positives + hard_negatives\n",
    "if len(adversarial_examples) >= 100:\n",
    "    adv_results = comprehensive_eval(adversarial_examples, best_model, \"Adversarial\")\n",
    "    \n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"[TARGET] ADVERSARIAL DIAGNOSTIC RESULTS\")\n",
    "    log(\"=\"*60)\n",
    "    log(f\"Standard Eval ROC-AUC:     {eval_results['roc_auc']:.4f}\")\n",
    "    log(f\"Adversarial ROC-AUC:       {adv_results['roc_auc']:.4f}\")\n",
    "    log(f\"Adversarial F1 @ best:     {adv_results['f1']:.4f}\")\n",
    "    \n",
    "    # Verdict\n",
    "    if adv_results['roc_auc'] >= 0.70 and adv_results['f1'] >= 0.65:\n",
    "        log(\"\\n[OK] VERDICT: Model is ROBUST to category shortcuts!\")\n",
    "        log(\"   ‚Üí Performance holds when categories don't predict similarity\")\n",
    "        log(\"   ‚Üí Model learned semantic content understanding\")\n",
    "        DIAGNOSTIC_PASSED = True\n",
    "    else:\n",
    "        log(\"\\n[WARN] VERDICT: Model may be exploiting category shortcuts\")\n",
    "        log(\"   ‚Üí Consider increasing hard negatives ratio\")\n",
    "        log(\"   ‚Üí Or remove category context from training text\")\n",
    "        DIAGNOSTIC_PASSED = False\n",
    "else:\n",
    "    log(\"[WARN] Could not generate enough adversarial pairs\")\n",
    "    DIAGNOSTIC_PASSED = None\n",
    "\n",
    "# Cleanup\n",
    "del diag_tfidf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd02e18",
   "metadata": {
    "description": "Markdown: Borderline Test"
   },
   "source": [
    "# 9. Save Training Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4145588",
   "metadata": {
    "description": "Evaluation: Borderline test (SKIPPED in curriculum mode)"
   },
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TRAINING METADATA EXPORT (V2)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def save_training_metadata(output_dir: str, config: dict, metrics: dict, \n",
    "                          data_stats: dict, adversarial_results: dict = None,\n",
    "                          error_analysis: dict = None):\n",
    "    \"\"\"\n",
    "    Save comprehensive training metadata for reproducibility (V2: includes error analysis).\n",
    "    \n",
    "    Following project convention: all model outputs include training_metadata.json\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"training_timestamp\": datetime.now().isoformat(),\n",
    "        \"model_version\": \"V2\",\n",
    "        \"model_name\": config.get('model_name', 'all-mpnet-finetuned'),\n",
    "        \"base_model\": config.get('model_name', 'sentence-transformers/all-mpnet-base-v2'),\n",
    "        \n",
    "        # Hyperparameters\n",
    "        \"hyperparameters\": {\n",
    "            \"epochs\": config.get('epochs'),\n",
    "            \"batch_size\": config.get('batch_size'),\n",
    "            \"learning_rate\": 2e-5,  # V2: Reduced for LoRA fine-tuning\n",
    "            \"max_seq_length\": config.get('max_seq_length'),\n",
    "            \"warmup_ratio\": config.get('warmup_ratio'),\n",
    "            \"loss_function\": \"CosineSimilarityLoss\"  # V2: Changed to use all pairs\n",
    "        },\n",
    "        \n",
    "        # V2: Curriculum learning config\n",
    "        \"curriculum_learning\": {\n",
    "            \"enabled\": config.get('use_curriculum', False),\n",
    "            \"phases\": config.get('curriculum_phases', [])\n",
    "        },\n",
    "        \n",
    "        # Data configuration (V2: stricter thresholds)\n",
    "        \"data_config\": {\n",
    "            \"source_data\": config.get('source_data'),\n",
    "            \"num_pairs\": config.get('num_pairs'),\n",
    "            \"min_text_length\": config.get('min_text_length'),\n",
    "            \"eval_split\": config.get('eval_split'),\n",
    "            \"holdout_split\": config.get('holdout_split'),\n",
    "            \"pos_tfidf_threshold\": config.get('pos_tfidf_threshold'),\n",
    "            \"neg_tfidf_threshold\": config.get('neg_tfidf_threshold'),\n",
    "            \"pair_ratios\": {\n",
    "                \"positives\": config.get('pos_ratio', 0.30),\n",
    "                \"hard_negatives\": config.get('hard_neg_ratio', 0.45),\n",
    "                \"easy_negatives\": config.get('easy_neg_ratio', 0.25)\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Data statistics\n",
    "        \"data_statistics\": data_stats,\n",
    "        \n",
    "        # TF-IDF configuration\n",
    "        \"tfidf_config\": {\n",
    "            \"max_features\": 15000,\n",
    "            \"ngram_range\": [1, 2],\n",
    "            \"min_df\": 2,\n",
    "            \"max_df\": 0.95\n",
    "        },\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        \"evaluation_metrics\": metrics,\n",
    "        \n",
    "        # V2: Cross-validated threshold\n",
    "        \"cv_threshold\": {\n",
    "            \"value\": float(cv_threshold) if 'cv_threshold' in dir() else None,\n",
    "            \"std\": float(cv_std) if 'cv_std' in dir() else None,\n",
    "            \"n_folds\": config.get('threshold_cv_folds', 5)\n",
    "        },\n",
    "        \n",
    "        # Adversarial diagnostic results\n",
    "        \"adversarial_diagnostic\": adversarial_results,\n",
    "        \n",
    "        # V2: Error analysis summary\n",
    "        \"error_analysis\": error_analysis,\n",
    "        \n",
    "        # Environment info\n",
    "        \"environment\": {\n",
    "            \"device\": DEVICE,\n",
    "            \"random_seed\": config.get('seed'),\n",
    "            \"python_version\": __import__('sys').version,\n",
    "            \"torch_version\": torch.__version__,\n",
    "            \"transformers_version\": __import__('transformers').__version__,\n",
    "            \"sentence_transformers_version\": __import__('sentence_transformers').__version__\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(output_dir, \"training_metadata.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    log(f\"üìù Training metadata saved to: {metadata_path}\")\n",
    "    return metadata_path\n",
    "\n",
    "# Collect data statistics\n",
    "data_stats = {\n",
    "    \"total_records\": len(df_incidents),\n",
    "    \"train_size\": len(train_df),\n",
    "    \"eval_size\": len(eval_df),\n",
    "    \"holdout_size\": len(holdout_df),\n",
    "    \"unique_categories\": df_incidents['Category'].nunique() if 'Category' in df_incidents.columns else None,\n",
    "    \"unique_subcategories\": df_incidents['Subcategory'].nunique() if 'Subcategory' in df_incidents.columns else None,\n",
    "    \"unique_assignment_groups\": df_incidents['Assignment group'].nunique() if 'Assignment group' in df_incidents.columns else None,\n",
    "    \"avg_text_length\": df_incidents['text'].str.len().mean(),\n",
    "    \"tfidf_vocabulary_size\": None\n",
    "}\n",
    "\n",
    "# Collect metrics\n",
    "try:\n",
    "    eval_metrics = {\n",
    "        \"eval_roc_auc\": float(eval_results['roc_auc']),\n",
    "        \"eval_pr_auc\": float(eval_results['pr_auc']),\n",
    "        \"eval_spearman\": float(eval_results['spearman']),\n",
    "        \"eval_pearson\": float(eval_results['pearson']),\n",
    "        \"eval_f1\": float(eval_results['f1']),\n",
    "        \"holdout_roc_auc\": float(holdout_results['roc_auc']),\n",
    "        \"holdout_pr_auc\": float(holdout_results['pr_auc']),\n",
    "        \"holdout_spearman\": float(holdout_results['spearman']),\n",
    "        \"holdout_pearson\": float(holdout_results['pearson']),\n",
    "        \"holdout_f1\": float(holdout_results['f1']),\n",
    "        # V2: Borderline metrics\n",
    "        \"borderline_roc_auc\": float(borderline_results['roc_auc']),\n",
    "        \"borderline_pr_auc\": float(borderline_results['pr_auc']),\n",
    "        \"borderline_f1\": float(borderline_results['f1']),\n",
    "    }\n",
    "except:\n",
    "    eval_metrics = {\"note\": \"Run evaluation cells first\"}\n",
    "\n",
    "# Collect adversarial results\n",
    "try:\n",
    "    adversarial_results_dict = {\n",
    "        \"roc_auc\": float(adv_results['roc_auc']) if 'adv_results' in dir() else None,\n",
    "        \"f1_score\": float(adv_results['f1']) if 'adv_results' in dir() else None,\n",
    "        \"pass_status\": DIAGNOSTIC_PASSED if 'DIAGNOSTIC_PASSED' in dir() else None\n",
    "    }\n",
    "except:\n",
    "    adversarial_results_dict = {\"note\": \"Run adversarial diagnostic first\"}\n",
    "\n",
    "# V2: Collect error analysis summary\n",
    "try:\n",
    "    error_analysis_dict = {\n",
    "        \"eval_fp_count\": eval_errors['fp_count'],\n",
    "        \"eval_fn_count\": eval_errors['fn_count'],\n",
    "        \"holdout_fp_count\": holdout_errors['fp_count'],\n",
    "        \"holdout_fn_count\": holdout_errors['fn_count'],\n",
    "        \"borderline_fp_count\": borderline_errors['fp_count'],\n",
    "        \"borderline_fn_count\": borderline_errors['fn_count'],\n",
    "    }\n",
    "except:\n",
    "    error_analysis_dict = {\"note\": \"Run error analysis first\"}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = save_training_metadata(\n",
    "    output_dir=str(save_path),\n",
    "    config=CONFIG,\n",
    "    metrics=eval_metrics,\n",
    "    data_stats=data_stats,\n",
    "    adversarial_results=adversarial_results_dict,\n",
    "    error_analysis=error_analysis_dict\n",
    ")\n",
    "\n",
    "log(\"[OK] Training pipeline V2 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a7ca0",
   "metadata": {
    "description": "Markdown: Summary"
   },
   "source": [
    "# 10. Usage Examples & V2 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc44fe2",
   "metadata": {
    "description": "Summary: Print final results & next steps"
   },
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# USAGE EXAMPLES & V2 SUMMARY\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def demonstrate_model_usage():\n",
    "    \"\"\"\n",
    "    Demonstrate how to use the fine-tuned model for ITSM ticket similarity.\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Try to use best_model if available (just trained), otherwise load from disk\n",
    "    if 'best_model' in globals():\n",
    "        model = globals()['best_model']\n",
    "        model_source = f\"memory (just trained: {globals()['save_path']})\"\n",
    "    else:\n",
    "        # Get output directory\n",
    "        if 'CONFIG' in globals():\n",
    "            output_dir = globals()['CONFIG']['output_dir']\n",
    "        else:\n",
    "            output_dir = 'models/real_servicenow_finetuned_v2'\n",
    "            log(\"[WARN] CONFIG not loaded. Using default model directory.\")\n",
    "        \n",
    "        # Look for most recent trained model\n",
    "        base_dir = Path(output_dir)\n",
    "        if base_dir.exists():\n",
    "            model_dirs = [d for d in base_dir.iterdir() if d.is_dir() and d.name.startswith('real_servicenow_')]\n",
    "            if model_dirs:\n",
    "                latest_model = max(model_dirs, key=lambda d: d.stat().st_mtime)\n",
    "                log(f\"üìÇ Loading most recent trained model from: {latest_model}\")\n",
    "                model = SentenceTransformer(str(latest_model))\n",
    "                model_source = str(latest_model)\n",
    "            else:\n",
    "                log(\"[ERROR] No trained model found. Please run training cells first.\")\n",
    "                return None, None\n",
    "        else:\n",
    "            log(\"[ERROR] No trained model found. Please run training cells first.\")\n",
    "            return None, None\n",
    "    \n",
    "    log(f\"[INSTALL] Using model from: {model_source}\")\n",
    "    \n",
    "    # Example tickets\n",
    "    example_tickets = [\n",
    "        \"User cannot login to SAP system. Error message: authentication failed. Tried resetting password but issue persists.\",\n",
    "        \"SAP login issue - getting access denied error when trying to connect to production system.\",\n",
    "        \"Outlook keeps crashing when opening large attachments. Have tried restarting but problem continues.\",\n",
    "        \"Email client crashes randomly. Users report Outlook freezing when opening emails with attachments.\",\n",
    "        \"Request to provision new laptop for incoming employee starting next Monday.\",\n",
    "    ]\n",
    "    \n",
    "    # Encode all tickets\n",
    "    embeddings = model.encode(example_tickets, show_progress_bar=False)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Display results\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"SIMILARITY MATRIX DEMO\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nTickets:\")\n",
    "    for i, ticket in enumerate(example_tickets):\n",
    "        print(f\"  [{i}] {ticket[:80]}...\")\n",
    "    \n",
    "    print(\"\\nSimilarity Matrix:\")\n",
    "    print(\"     \", end=\"\")\n",
    "    for i in range(len(example_tickets)):\n",
    "        print(f\"  [{i}]  \", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for i, row in enumerate(sim_matrix):\n",
    "        print(f\"[{i}]  \", end=\"\")\n",
    "        for val in row:\n",
    "            print(f\" {val:.3f} \", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Find similar ticket pairs\n",
    "    log(\"\\n\" + \"-\"*60)\n",
    "    log(\"HIGH SIMILARITY PAIRS (> 0.7):\")\n",
    "    for i in range(len(example_tickets)):\n",
    "        for j in range(i+1, len(example_tickets)):\n",
    "            if sim_matrix[i][j] > 0.7:\n",
    "                print(f\"  Tickets [{i}] & [{j}]: {sim_matrix[i][j]:.3f}\")\n",
    "                print(f\"    [{i}]: {example_tickets[i][:60]}...\")\n",
    "                print(f\"    [{j}]: {example_tickets[j][:60]}...\")\n",
    "                print()\n",
    "    \n",
    "    return model, embeddings\n",
    "\n",
    "# Run demonstration\n",
    "demo_model, demo_embeddings = demonstrate_model_usage()\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# V2 IMPROVEMENT SUMMARY\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"[STATS] V2 IMPROVEMENT SUMMARY\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "üÜï V2 ENHANCEMENTS IMPLEMENTED:\n",
    "\n",
    "1. HARDER NEGATIVE MINING\n",
    "   ‚îú‚îÄ Increased hard_neg_ratio: 35% ‚Üí 45%\n",
    "   ‚îú‚îÄ Stricter neg_tfidf_threshold: 0.20 ‚Üí 0.12\n",
    "   ‚îî‚îÄ Reduced easy negatives: 30% ‚Üí 25%\n",
    "\n",
    "2. CURRICULUM LEARNING\n",
    "   ‚îú‚îÄ Phase 1 (Epochs 1-2): Easier pairs (25% hard neg, threshold 0.15)\n",
    "   ‚îî‚îÄ Phase 2 (Epochs 3-4): Harder pairs (55% hard neg, threshold 0.10)\n",
    "\n",
    "3. BORDERLINE TEST SET\n",
    "   ‚îú‚îÄ TF-IDF range: 0.25 - 0.35 (ambiguous cases)\n",
    "   ‚îî‚îÄ Tests model on genuinely difficult pairs\n",
    "\n",
    "4. ERROR ANALYSIS\n",
    "   ‚îú‚îÄ Identifies worst false positives/negatives\n",
    "   ‚îú‚îÄ Analyzes text length patterns\n",
    "   ‚îî‚îÄ Provides systematic failure insights\n",
    "\n",
    "5. CROSS-VALIDATED THRESHOLD\n",
    "   ‚îú‚îÄ 5-fold CV for robust threshold selection\n",
    "   ‚îî‚îÄ Reports threshold variance across folds\n",
    "\n",
    "NEXT STEPS FOR FURTHER IMPROVEMENT:\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "üìà BASED ON RESULTS, CONSIDER:\n",
    "\n",
    "If borderline ROC-AUC < 0.80:\n",
    "   ‚Üí Increase curriculum phases to 3 (add intermediate difficulty)\n",
    "   ‚Üí Add triplet loss fine-tuning stage\n",
    "\n",
    "If many false negatives on short texts:\n",
    "   ‚Üí Reduce min_text_length filter\n",
    "   ‚Üí Add text augmentation (paraphrasing)\n",
    "\n",
    "If many false positives on same-category pairs:\n",
    "   ‚Üí Increase hard_neg_ratio further (45% ‚Üí 55%)\n",
    "   ‚Üí Remove category context from training text\n",
    "\n",
    "PRODUCTION DEPLOYMENT:\n",
    "   ‚Üí Model path: {CONFIG['output_dir']}\n",
    "   ‚Üí Use CV threshold: {cv_threshold:.4f} (¬± {cv_std:.4f})\n",
    "   ‚Üí Pre-compute embeddings for ticket corpus\n",
    "\"\"\")\n",
    "\n",
    "log(\"[OK] V2 training and evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

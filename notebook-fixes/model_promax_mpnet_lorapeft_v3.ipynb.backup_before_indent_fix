{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune MPNet with LoRA/PEFT for ITSM Ticket Similarity (v3)\n",
    "\n",
    "**Model:** sentence-transformers/all-mpnet-base-v2\n",
    "**Method:** LoRA (Low-Rank Adaptation) fine-tuning\n",
    "**Data:** Pre-generated curriculum pairs (easy ‚Üí medium ‚Üí hard)\n",
    "**Goal:** Beat baseline Spearman 0.504\n",
    "\n",
    "---\n",
    "\n",
    "## Changelog (v3)\n",
    "\n",
    "- ‚úÖ Consolidated all imports into single cell (Cell 3)\n",
    "- ‚úÖ Fixed section numbering (Score Distribution before Evaluation)\n",
    "- ‚úÖ Removed dead code (pair generation, legacy training blocks)\n",
    "- ‚úÖ Cleaned up if-guards and conditional execution\n",
    "- ‚úÖ Consolidated device detection\n",
    "- ‚úÖ **Preserved all fixes:** train_pairs_path, epochs=12, lr=CONFIG['lr']\n",
    "- ‚úÖ Clear linear execution flow (no hidden branches)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Quick Configuration Summary\n",
    "\n",
    "| Component | Setting | Rationale |\n",
    "|-----------|---------|-----------|\n",
    "| **LoRA Rank** | 16 | Balanced capacity (8=low, 32=high) |\n",
    "| **LoRA Alpha** | 32 | Standard scaling (2√órank) |\n",
    "| **Target Modules** | q_proj, v_proj | Attention query/value projections |\n",
    "| **Learning Rate** | 1e-4 | Lower than full fine-tune (2e-5) |\n",
    "| **MNRL Weight** | 0.9 | Primary ranking loss |\n",
    "| **Cosine Weight** | 0.1 | Auxiliary calibration |\n",
    "| **Batch Size** | 16 (8 on MPS) | Auto-adjusted for device |\n",
    "| **Epochs** | 4 | 2 phases √ó 2 epochs each |\n",
    "\n",
    "**Key Metrics to Watch:**\n",
    "- **Trainable params**: Should be ~3-5M (vs ~30-50M baseline)\n",
    "- **Spearman correlation**: Target > 0.75 (current V2 best: 0.7516)\n",
    "- **ROC-AUC**: Target > 0.93 (current V2 best: 0.9369)\n",
    "- **F1 @ optimal threshold**: Target > 0.84 (current V2 best: 0.8442)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81222c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] All imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS (Consolidated)\n",
    "# =============================================================================\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import subprocess\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data & ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    precision_recall_curve, \n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve\n",
    ")\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# LoRA/PEFT\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "print('[OK] All imports loaded successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebac020",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Configure warnings, check packages, and detect compute device (GPU/MPS/CPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c33bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEVICE] Using CUDA: NVIDIA GeForce RTX 5090\n",
      "[TORCH] Version: 2.11.0.dev20251223+cu128\n",
      "[PEFT] Version: 0.18.0\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "log = logging.info\n",
    "\n",
    "# Install required packages if missing\n",
    "try:\n",
    "    import peft\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'peft>=0.4.0'])\n",
    "    import peft\n",
    "\n",
    "# Device detection\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f'[DEVICE] Using CUDA: {device_name}')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    print(f'[DEVICE] Using Apple Silicon MPS')\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print(f'[DEVICE] Using CPU')\n",
    "\n",
    "print(f'[TORCH] Version: {torch.__version__}')\n",
    "print(f'[PEFT] Version: {peft.__version__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6fdac",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "All training hyperparameters in one place.\n",
    "\n",
    "**Key fixes applied:**\n",
    "- `train_pairs_path`: curriculum_training_pairs_complete.json\n",
    "- `epochs`: 12 (up from 6)\n",
    "- `lr`: 5e-5 (LoRA-optimized learning rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc626db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURATION (V3 - Curriculum Learning)\n",
      "======================================================================\n",
      "Model: sentence-transformers/all-mpnet-base-v2\n",
      "Output: models/real_servicenow_finetuned_mpnet_lora\n",
      "\n",
      "Data:\n",
      "  Using pre-generated pairs: True\n",
      "  Pairs file: data_new/curriculum_training_pairs_complete.json\n",
      "\n",
      "LoRA Config:\n",
      "  Rank: 16\n",
      "  Alpha: 32\n",
      "  Dropout: 0.1\n",
      "\n",
      "Training:\n",
      "  Total epochs: 12\n",
      "  Learning rate: 5e-06 (INCREASED for LoRA)\n",
      "  Batch size: 64\n",
      "  Max seq length: 256 (REDUCED to match baseline)\n",
      "\n",
      "Curriculum:\n",
      "  Use curriculum: True\n",
      "  Legacy mode: False\n",
      "  Epochs per phase: 4\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Simple logging\n",
    "def log(msg, level=logging.INFO):\n",
    "    print(msg)\n",
    "\n",
    "# --- CONFIGURATION (V3 - Curriculum Learning with Pre-generated Pairs) ---\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'sentence-transformers/all-mpnet-base-v2',\n",
    "    'output_dir': 'models/real_servicenow_finetuned_mpnet_lora',\n",
    "\n",
    "    # Data - USING PRE-GENERATED CURRICULUM PAIRS\n",
    "    'use_pre_generated_pairs': True,  # NEW: Use pre-generated pairs\n",
    "    'train_pairs_path': 'data_new/curriculum_training_pairs_complete.json',  # NEW: Curriculum dataset\n",
    "    'source_data': 'data_new\\\\SNow_incident_ticket_data.csv',  # Fallback (not used)\n",
    "\n",
    "    # LoRA/PEFT Configuration\n",
    "    'use_lora': True,\n",
    "    'lora_r': 16,              # Rank (8-32, higher = more capacity)\n",
    "    'lora_alpha': 32,          # Scaling factor (typically 2*r)\n",
    "    'lora_dropout': 0.1,       # Dropout for LoRA layers\n",
    "    'lora_target_modules': ['query', 'key', 'value'],\n",
    "\n",
    "    # Loss Function\n",
    "    'use_multi_loss': False,   # Use single loss for simplicity\n",
    "    'loss_type': 'cosine',     # 'cosine' or 'mnrl'\n",
    "\n",
    "    # Training hyperparameters (UPDATED FOR CURRICULUM)\n",
    "    'epochs': 12,              # 12 total epochs (4 per curriculum phase)\n",
    "    'batch_size': 64,          # Will auto-reduce for MPS/CPU if needed\n",
    "    'lr': 5e-6,                # REDUCED from 5e-5 (prevent catastrophic forgetting) (LoRA needs higher LR)\n",
    "    'max_seq_length': 256,     # REDUCED from 384 (match baseline)\n",
    "    'warmup_ratio': 0.15,\n",
    "\n",
    "    # Curriculum Learning (NEW - Using phase_indicators from data)\n",
    "    'use_curriculum': True,    # Train in 3 phases\n",
    "    'use_legacy_mode': False,  # Legacy (non-curriculum) evaluation features\n",
    "    'epochs_per_phase': 4,     # 4 epochs per phase (12 total / 3 phases)\n",
    "\n",
    "    # Data splits (only used if NOT using pre-generated)\n",
    "    'eval_split': 0.15,\n",
    "    'holdout_split': 0.10,\n",
    "    'min_text_length': 25,\n",
    "\n",
    "    # Pair generation (LEGACY - not used with pre-generated pairs)\n",
    "    'num_pairs': 50000,        # Not used\n",
    "    'pos_ratio': 0.30,         # Not used\n",
    "\n",
    "    # Reproducibility\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Set seeds\n",
    "random.seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "\n",
    "log(\"=\"*70)\n",
    "log(\"CONFIGURATION (V3 - Curriculum Learning)\")\n",
    "log(\"=\"*70)\n",
    "log(f\"Model: {CONFIG['model_name']}\")\n",
    "log(f\"Output: {CONFIG['output_dir']}\")\n",
    "log(f\"\\nData:\")\n",
    "log(f\"  Using pre-generated pairs: {CONFIG['use_pre_generated_pairs']}\")\n",
    "log(f\"  Pairs file: {CONFIG['train_pairs_path']}\")\n",
    "log(f\"\\nLoRA Config:\")\n",
    "log(f\"  Rank: {CONFIG['lora_r']}\")\n",
    "log(f\"  Alpha: {CONFIG['lora_alpha']}\")\n",
    "log(f\"  Dropout: {CONFIG['lora_dropout']}\")\n",
    "log(f\"\\nTraining:\")\n",
    "log(f\"  Total epochs: {CONFIG['epochs']}\")\n",
    "log(f\"  Learning rate: {CONFIG['lr']} (INCREASED for LoRA)\")\n",
    "log(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "log(f\"  Max seq length: {CONFIG['max_seq_length']} (REDUCED to match baseline)\")\n",
    "log(f\"\\nCurriculum:\")\n",
    "log(f\"  Use curriculum: {CONFIG['use_curriculum']}\")\n",
    "log(f\"  Legacy mode: {CONFIG.get('use_legacy_mode', False)}\")\n",
    "log(f\"  Epochs per phase: {CONFIG['epochs_per_phase']}\")\n",
    "log(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe73e8",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Preprocessing\n",
    "\n",
    "Load ServiceNow incident data from CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c5436a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Looking for data file:\n",
      "   Configured path: data_new\\SNow_incident_ticket_data.csv\n",
      "   Absolute path: C:\\Users\\donov\\Downloads\\nexustism\\nexustism\\data_new\\SNow_incident_ticket_data.csv\n",
      "   Current working directory: c:\\Users\\donov\\Downloads\\nexustism\\nexustism\n",
      "   File exists: True\n",
      "üìÇ Loading real ServiceNow data from: data_new\\SNow_incident_ticket_data.csv\n",
      "[STATS] Loaded 10,633 raw records\n",
      "[OK] Available columns: ['Number', 'Description', 'Opened by', 'Company', 'ITSM Department', 'Created', 'Urgency', 'Impact', 'Priority', 'Assignment group', 'Assigned to', 'State', 'Service', 'Service offering', 'Closed', 'Closed by', 'Category', 'Subcategory', 'Resolution code', 'Resolution notes', 'User input', 'Comments and Work notes', 'Manday Effort (hrs)', 'Ticket Type', 'AMS Domain', 'AMS System Type', 'AMS Category Type', 'AMS Service Type', 'AMS Business Related', 'AMS IT Related']\n",
      "üìâ After filtering short descriptions: 10,486 records (dropped 147)\n",
      "\n",
      "[STATS] Data Summary:\n",
      "   Total records: 10,486\n",
      "   Unique categories: 30\n",
      "   Avg text length: 561 chars\n",
      "   Min text length: 66 chars\n",
      "   Max text length: 15545 chars\n",
      "\n",
      "üìù Sample preprocessed text:\n",
      "   'GRPT not working as expected. ZMMM_PO_REV is not generating correct dates as per maintained in GRPT table. E.g. P/O# 100024066 Vendor Ship mode is 03. As per GRPT route days are 12 days and GR days is...'\n",
      "\n",
      "[OK] Loaded 10,486 incidents ready for training\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_real_data(config):\n",
    "    \"\"\"\n",
    "    Load and preprocess real ServiceNow incident data.\n",
    "    \n",
    "    Key differences from dummy data:\n",
    "    - Only 'Description' column (no 'Short Description')\n",
    "    - Real data has multi-line descriptions with embedded newlines\n",
    "    - Richer vocabulary and more varied text\n",
    "    \"\"\"\n",
    "    source_path = Path(config['source_data'])\n",
    "    \n",
    "    # Debug: Show absolute path\n",
    "    log(f\"üîç Looking for data file:\")\n",
    "    log(f\"   Configured path: {config['source_data']}\")\n",
    "    log(f\"   Absolute path: {source_path.resolve()}\")\n",
    "    log(f\"   Current working directory: {Path.cwd()}\")\n",
    "    log(f\"   File exists: {source_path.exists()}\")\n",
    "    \n",
    "    if not source_path.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {source_path.resolve()}\")\n",
    "    \n",
    "    log(f\"üìÇ Loading real ServiceNow data from: {source_path}\")\n",
    "    \n",
    "    # Load CSV - handle multi-line fields\n",
    "    df = pd.read_csv(source_path, encoding='utf-8', on_bad_lines='skip')\n",
    "    initial_count = len(df)\n",
    "    log(f\"[STATS] Loaded {initial_count:,} raw records\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['Number', 'Description', 'Category', 'Subcategory', \n",
    "                     'Service', 'Service offering', 'Assignment group']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        log(f\"[WARN] Missing columns: {missing_cols}\")\n",
    "        # Use available columns\n",
    "        required_cols = [col for col in required_cols if col in df.columns]\n",
    "    \n",
    "    log(f\"[OK] Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Clean text function\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text) or text is None:\n",
    "            return \"\"\n",
    "        text = str(text).strip()\n",
    "        # Normalize whitespace (collapse multiple spaces/newlines)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove common boilerplate\n",
    "        text = re.sub(r'Note\\s*:\\s*This is an automated.*?\\.', '', text, flags=re.IGNORECASE)\n",
    "        return text.strip()\n",
    "    \n",
    "    # Clean Description\n",
    "    df['Description'] = df['Description'].apply(clean_text)\n",
    "    \n",
    "    # Fill NA for context columns\n",
    "    context_cols = ['Category', 'Subcategory', 'Service', 'Service offering', 'Assignment group']\n",
    "    for col in context_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('').astype(str).str.strip().str.lower()\n",
    "    \n",
    "    # Build contextual text representation\n",
    "    # Format: \"Description (Context: [Service|Offering] [Category|Subcategory] Group: X)\"\n",
    "    def build_context(row):\n",
    "        parts = []\n",
    "        \n",
    "        # Service context\n",
    "        service_parts = []\n",
    "        if row.get('Service', ''):\n",
    "            service_parts.append(row['Service'])\n",
    "        if row.get('Service offering', ''):\n",
    "            service_parts.append(row['Service offering'])\n",
    "        if service_parts:\n",
    "            parts.append(f\"[{' | '.join(service_parts)}]\")\n",
    "        \n",
    "        # Category context\n",
    "        cat_parts = []\n",
    "        if row.get('Category', ''):\n",
    "            cat_parts.append(row['Category'])\n",
    "        if row.get('Subcategory', ''):\n",
    "            cat_parts.append(row['Subcategory'])\n",
    "        if cat_parts:\n",
    "            parts.append(f\"[{' | '.join(cat_parts)}]\")\n",
    "        \n",
    "        # Assignment group\n",
    "        if row.get('Assignment group', ''):\n",
    "            parts.append(f\"Group: {row['Assignment group']}\")\n",
    "        \n",
    "        return ' '.join(parts) if parts else ''\n",
    "    \n",
    "    # Build full text: Description + Context suffix\n",
    "    df['context'] = df.apply(build_context, axis=1)\n",
    "    df['text'] = df.apply(\n",
    "        lambda row: f\"{row['Description']} (Context: {row['context']})\" if row['context'] else row['Description'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Filter out short/empty descriptions\n",
    "    df = df[df['Description'].str.len() >= config['min_text_length']].copy()\n",
    "    log(f\"üìâ After filtering short descriptions: {len(df):,} records (dropped {initial_count - len(df):,})\")\n",
    "    \n",
    "    # Create category_id for stratified splitting\n",
    "    if 'Category' in df.columns and 'Subcategory' in df.columns:\n",
    "        df['category_id'] = df.groupby(['Category', 'Subcategory']).ngroup()\n",
    "    else:\n",
    "        df['category_id'] = 0\n",
    "    \n",
    "    # Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Summary stats\n",
    "    log(f\"\\n[STATS] Data Summary:\")\n",
    "    log(f\"   Total records: {len(df):,}\")\n",
    "    log(f\"   Unique categories: {df['category_id'].nunique()}\")\n",
    "    log(f\"   Avg text length: {df['text'].str.len().mean():.0f} chars\")\n",
    "    log(f\"   Min text length: {df['text'].str.len().min()} chars\")\n",
    "    log(f\"   Max text length: {df['text'].str.len().max()} chars\")\n",
    "    \n",
    "    # Sample text\n",
    "    log(f\"\\nüìù Sample preprocessed text:\")\n",
    "    log(f\"   '{df['text'].iloc[0][:200]}...'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "df_incidents = load_and_preprocess_real_data(CONFIG)\n",
    "print(f\"\\n[OK] Loaded {len(df_incidents):,} incidents ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4958e34",
   "metadata": {},
   "source": [
    "## 4. Data Splitting\n",
    "\n",
    "Split into train/eval/holdout sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e018f6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Using stratified split\n",
      "[STATS] Data Splits:\n",
      "   Train:   8,021 incidents (76.5%)\n",
      "   Eval:    1,416 incidents (13.5%)\n",
      "   Holdout: 1,049 incidents (10.0%)\n",
      "\n",
      "üîç Overlap Check:\n",
      "   Train ‚à© Eval: 0 incidents\n",
      "   Train ‚à© Holdout: 0 incidents\n",
      "   Eval ‚à© Holdout: 0 incidents\n"
     ]
    }
   ],
   "source": [
    "def split_data(df, config):\n",
    "    \"\"\"\n",
    "    Three-way split: Train / Eval / Holdout\n",
    "    - Holdout is completely unseen (for adversarial diagnostic)\n",
    "    - Stratified by category to ensure representation\n",
    "    \"\"\"\n",
    "    # Handle rare categories: group categories with <2 samples\n",
    "    category_counts = df['category_id'].value_counts()\n",
    "    rare_categories = category_counts[category_counts < 2].index\n",
    "    \n",
    "    # Create stratification column: use category_id for common categories, -1 for rare\n",
    "    df['stratify_col'] = df['category_id'].copy()\n",
    "    df.loc[df['category_id'].isin(rare_categories), 'stratify_col'] = -1\n",
    "    \n",
    "    # Check if we can stratify (need at least 2 samples per class)\n",
    "    stratify_counts = df['stratify_col'].value_counts()\n",
    "    can_stratify = all(stratify_counts >= 2)\n",
    "    \n",
    "    if can_stratify:\n",
    "        # First split: separate holdout set (stratified)\n",
    "        train_eval_df, holdout_df = train_test_split(\n",
    "            df,\n",
    "            test_size=config['holdout_split'],\n",
    "            stratify=df['stratify_col'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "        \n",
    "        # Second split: train/eval from remaining (stratified)\n",
    "        train_df, eval_df = train_test_split(\n",
    "            train_eval_df,\n",
    "            test_size=config['eval_split'],\n",
    "            stratify=train_eval_df['stratify_col'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "        log(\"[OK] Using stratified split\")\n",
    "    else:\n",
    "        # Fallback: random split without stratification\n",
    "        log(\"[WARN] Using random split (categories too imbalanced for stratification)\")\n",
    "        train_eval_df, holdout_df = train_test_split(\n",
    "            df,\n",
    "            test_size=config['holdout_split'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "        \n",
    "        train_df, eval_df = train_test_split(\n",
    "            train_eval_df,\n",
    "            test_size=config['eval_split'],\n",
    "            random_state=config['seed']\n",
    "        )\n",
    "    \n",
    "    return train_df, eval_df, holdout_df\n",
    "\n",
    "# Split the data\n",
    "train_df, eval_df, holdout_df = split_data(df_incidents, CONFIG)\n",
    "\n",
    "log(f\"[STATS] Data Splits:\")\n",
    "log(f\"   Train:   {len(train_df):,} incidents ({len(train_df)/len(df_incidents)*100:.1f}%)\")\n",
    "log(f\"   Eval:    {len(eval_df):,} incidents ({len(eval_df)/len(df_incidents)*100:.1f}%)\")\n",
    "log(f\"   Holdout: {len(holdout_df):,} incidents ({len(holdout_df)/len(df_incidents)*100:.1f}%)\")\n",
    "\n",
    "# Check for data leakage\n",
    "def check_overlap(df1, df2, name1, name2):\n",
    "    overlap = len(set(df1['Number']) & set(df2['Number']))\n",
    "    log(f\"   {name1} ‚à© {name2}: {overlap} incidents\")\n",
    "\n",
    "log(f\"\\nüîç Overlap Check:\")\n",
    "check_overlap(train_df, eval_df, \"Train\", \"Eval\")\n",
    "check_overlap(train_df, holdout_df, \"Train\", \"Holdout\")\n",
    "check_overlap(eval_df, holdout_df, \"Eval\", \"Holdout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb20908",
   "metadata": {},
   "source": [
    "## 5. Load Pre-Generated Curriculum Pairs\n",
    "\n",
    "Load curriculum pairs from JSON (Phase 1: easy, Phase 2: medium, Phase 3: hard).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9df6368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "USING PRE-GENERATED CURRICULUM PAIRS\n",
      "======================================================================\n",
      "\n",
      "Loading curriculum pairs from: data_new/curriculum_training_pairs_complete.json\n",
      "Loaded 15,000 total pairs\n",
      "  Positives: 10,000 (66.7%)\n",
      "  Negatives: 5,000 (33.3%)\n",
      "\n",
      "Curriculum phases: 3\n",
      "  Phase 1 (easy): 5,000 pairs, pos>=0.52, neg<=0.36\n",
      "  Phase 2 (medium): 5,000 pairs, pos>=0.4, neg<=0.45\n",
      "  Phase 3 (hard): 5,000 pairs, pos>=0.3, neg<=0.5\n",
      "\n",
      "Separated into phases:\n",
      "  Phase 1: 5,000 pairs\n",
      "  Phase 2: 5,000 pairs\n",
      "  Phase 3: 5,000 pairs\n",
      "\n",
      "Total training examples: 15,000\n",
      "\n",
      "Note: Will train in 3 curriculum phases\n",
      "\n",
      "[OK] Curriculum Verification:\n",
      "   Total pairs: 15,000\n",
      "   Phase 1 (easy): 5,000\n",
      "   Phase 2 (medium): 5,000\n",
      "   Phase 3 (hard): 5,000\n",
      "\n",
      "Loading test pairs from: data_new/fixed_test_pairs.json\n",
      "[OK] Loaded 1,000 test pairs for evaluation\n",
      "   Positives: 500 (50.0%)\n",
      "   Negatives: 500 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# LOAD PRE-GENERATED CURRICULUM PAIRS\n",
    "# ========================================\n",
    "\n",
    "\n",
    "def load_curriculum_pairs(pairs_path, use_curriculum=True):\n",
    "    \"\"\"\n",
    "    Load pre-generated curriculum pairs from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        pairs_path: Path to curriculum_training_pairs_*.json\n",
    "        use_curriculum: If True, return separate phases; if False, return all mixed\n",
    "    \n",
    "    Returns:\n",
    "        If use_curriculum=True: (phase1_pairs, phase2_pairs, phase3_pairs)\n",
    "        If use_curriculum=False: all_pairs (mixed)\n",
    "    \"\"\"\n",
    "    log(f\"\\nLoading curriculum pairs from: {pairs_path}\")\n",
    "    \n",
    "    with open(pairs_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    texts1 = data['texts1']\n",
    "    texts2 = data['texts2']\n",
    "    labels = data['labels']\n",
    "    phase_indicators = data.get('phase_indicators', [1] * len(labels))\n",
    "    \n",
    "    log(f\"Loaded {len(labels):,} total pairs\")\n",
    "    log(f\"  Positives: {sum(labels):,} ({100*sum(labels)/len(labels):.1f}%)\")\n",
    "    log(f\"  Negatives: {len(labels) - sum(labels):,} ({100*(len(labels)-sum(labels))/len(labels):.1f}%)\")\n",
    "    \n",
    "    # Show metadata\n",
    "    metadata = data.get('metadata', {})\n",
    "    if metadata:\n",
    "        log(f\"\\nCurriculum phases: {metadata.get('curriculum_phases', 'N/A')}\")\n",
    "        for phase_num in [1, 2, 3]:\n",
    "            phase_key = f'phase{phase_num}_config'\n",
    "            if phase_key in metadata:\n",
    "                phase_cfg = metadata[phase_key]\n",
    "                log(f\"  Phase {phase_num} ({phase_cfg.get('difficulty', 'N/A')}): \"\n",
    "                    f\"{phase_cfg.get('pairs', 0):,} pairs, \"\n",
    "                    f\"pos>={phase_cfg.get('pos_threshold', 'N/A')}, \"\n",
    "                    f\"neg<={phase_cfg.get('neg_threshold', 'N/A')}\")\n",
    "    \n",
    "    # Convert to InputExample format\n",
    "    if use_curriculum:\n",
    "        # Separate by phase\n",
    "        phase1_pairs = []\n",
    "        phase2_pairs = []\n",
    "        phase3_pairs = []\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            example = InputExample(texts=[texts1[i], texts2[i]], label=float(labels[i]))\n",
    "            phase = phase_indicators[i]\n",
    "            if phase == 1:\n",
    "                phase1_pairs.append(example)\n",
    "            elif phase == 2:\n",
    "                phase2_pairs.append(example)\n",
    "            elif phase == 3:\n",
    "                phase3_pairs.append(example)\n",
    "        \n",
    "        log(f\"\\nSeparated into phases:\")\n",
    "        log(f\"  Phase 1: {len(phase1_pairs):,} pairs\")\n",
    "        log(f\"  Phase 2: {len(phase2_pairs):,} pairs\")\n",
    "        log(f\"  Phase 3: {len(phase3_pairs):,} pairs\")\n",
    "        \n",
    "        return phase1_pairs, phase2_pairs, phase3_pairs\n",
    "    else:\n",
    "        # Return all mixed\n",
    "        all_pairs = [\n",
    "            InputExample(texts=[texts1[i], texts2[i]], label=float(labels[i]))\n",
    "            for i in range(len(labels))\n",
    "        ]\n",
    "        log(f\"Returning {len(all_pairs):,} mixed pairs\")\n",
    "        return all_pairs\n",
    "\n",
    "# ========================================\n",
    "# LOAD OR GENERATE PAIRS\n",
    "# ========================================\n",
    "\n",
    "if CONFIG.get('use_pre_generated_pairs', False):\n",
    "    log(\"=\"*70)\n",
    "    log(\"USING PRE-GENERATED CURRICULUM PAIRS\")\n",
    "    log(\"=\"*70)\n",
    "    \n",
    "    pairs_path = CONFIG['train_pairs_path']\n",
    "    \n",
    "    if CONFIG.get('use_curriculum', False):\n",
    "        # Load phases separately for curriculum training\n",
    "        phase1_train, phase2_train, phase3_train = load_curriculum_pairs(\n",
    "            pairs_path, use_curriculum=True\n",
    "        )\n",
    "        \n",
    "        # For now, combine for evaluation split\n",
    "        # (In production, you'd want separate eval sets per phase)\n",
    "        train_examples = phase1_train + phase2_train + phase3_train\n",
    "        \n",
    "        log(f\"\\nTotal training examples: {len(train_examples):,}\")\n",
    "        log(\"\\nNote: Will train in 3 curriculum phases\")\n",
    "        \n",
    "        # Store phases for later use\n",
    "        CURRICULUM_PHASES = {\n",
    "            'phase1': phase1_train,\n",
    "            'phase2': phase2_train,\n",
    "            'phase3': phase3_train\n",
    "        }\n",
    "\n",
    "# Verify curriculum loaded correctly\n",
    "if CONFIG.get('use_curriculum') and CURRICULUM_PHASES:\n",
    "    total_pairs = sum(len(v) for v in CURRICULUM_PHASES.values())\n",
    "    print(f\"\\n[OK] Curriculum Verification:\")\n",
    "    print(f\"   Total pairs: {total_pairs:,}\")\n",
    "    print(f\"   Phase 1 (easy): {len(CURRICULUM_PHASES.get('phase1', [])):,}\")\n",
    "    print(f\"   Phase 2 (medium): {len(CURRICULUM_PHASES.get('phase2', [])):,}\")\n",
    "    print(f\"   Phase 3 (hard): {len(CURRICULUM_PHASES.get('phase3', [])):,}\")\n",
    "    \n",
    "    assert len(CURRICULUM_PHASES) == 3, f\"Expected 3 phases, got {len(CURRICULUM_PHASES)}\"\n",
    "    assert total_pairs == 15000, f\"Expected 15K pairs, got {total_pairs:,}\"\n",
    "    \n",
    "    # Load test/eval pairs from fixed_test_pairs.json\n",
    "    test_pairs_path = 'data_new/fixed_test_pairs.json'\n",
    "    log(f\"\\nLoading test pairs from: {test_pairs_path}\")\n",
    "    \n",
    "    with open(test_pairs_path, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    # Convert to InputExample\n",
    "    eval_examples = [\n",
    "        InputExample(texts=[t1, t2], label=float(label))\n",
    "        for t1, t2, label in zip(test_data['texts1'], test_data['texts2'], test_data['labels'])\n",
    "    ]\n",
    "    \n",
    "    # For curriculum training, we don't have separate holdout/borderline\n",
    "    # Use eval_examples for all evaluation metrics\n",
    "    holdout_examples = eval_examples  # Reuse for holdout metrics\n",
    "    borderline_examples = []  # Empty - not applicable for curriculum\n",
    "    \n",
    "    log(f\"[OK] Loaded {len(eval_examples):,} test pairs for evaluation\")\n",
    "    pos_count = sum(1 for ex in eval_examples if ex.label == 1.0)\n",
    "    log(f\"   Positives: {pos_count:,} ({100*pos_count/len(eval_examples):.1f}%)\")\n",
    "    log(f\"   Negatives: {len(eval_examples)-pos_count:,} ({100*(len(eval_examples)-pos_count)/len(eval_examples):.1f}%)\")\n",
    "    \n",
    "    \n",
    "    # Skip the pair generation cells below\n",
    "    SKIP_PAIR_GENERATION = True\n",
    "    \n",
    "else:\n",
    "    log(\"=\"*70)\n",
    "    # eval_examples, holdout_examples, borderline_examples will be generated below\n",
    "    log(\"GENERATING PAIRS ON-THE-FLY (LEGACY MODE)\")\n",
    "    log(\"=\"*70)\n",
    "    log(\"Note: Consider using pre-generated curriculum pairs instead!\")\n",
    "    log(\"      Run fix_train_test_mismatch.ipynb to generate them.\")\n",
    "    \n",
    "    SKIP_PAIR_GENERATION = False\n",
    "    CURRICULUM_PHASES = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f29a77e",
   "metadata": {},
   "source": [
    "## 6. Training Functions & LoRA Setup\n",
    "\n",
    "Define training utilities, evaluator class, and LoRA initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edbd89e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading base model: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TARGET] Applying LoRA adapters...\n",
      "   Rank (r): 16\n",
      "   Alpha: 32\n",
      "   Target modules (resolved): ['dense']\n",
      "   Dropout: 0.1\n",
      "[GPU] Moving PEFT model to cuda\n",
      "[OK] LoRA Applied!\n",
      "   Trainable params: 1,499,136 (1.35%)\n",
      "   Total params: 110,985,600\n",
      "   Parameter reduction: 98.65%\n",
      "[OK] Model loaded on cuda, max_seq_length=256\n",
      "\n",
      "[STATS] Preparing training data...\n",
      "   Train pairs (all): 15,000\n",
      "   Eval pairs (all):  1,000\n",
      "   Train balance: 10,000 pos (66.7%), 5,000 neg (33.3%)\n",
      "üîß Using MatryoshkaLoss + MultipleNegativesRankingLoss (SOTA 2024)\n",
      "   Dimensions: [768, 512, 256, 128, 64]\n",
      "   In-batch negatives: automatic\n",
      "   Batch size: 16\n",
      "   Utilizes curated hard negatives: YES\n",
      "   Total training pairs: 15,000\n",
      "\n",
      "[STATS] Training Setup:\n",
      "   Batches per epoch: 938\n",
      "   Total training steps: 11256\n",
      "   Warmup steps: 1688\n",
      "   Curriculum learning: True\n",
      "\n",
      "[STATS] Training Setup:\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.evaluation import SentenceEvaluator\n",
    "\n",
    "# --- Custom Evaluator ---\n",
    "class ITSMEvaluator(SentenceEvaluator):\n",
    "    \"\"\"Evaluator for ITSM ticket similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, examples, batch_size=16, name=\"\"):\n",
    "        self.examples = examples\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "        \n",
    "        self.texts1 = [ex.texts[0] for ex in examples]\n",
    "        self.texts2 = [ex.texts[1] for ex in examples]\n",
    "        self.labels = np.array([ex.label for ex in examples])\n",
    "        \n",
    "        self.csv_file = f\"{name}_eval_results.csv\"\n",
    "        self.csv_headers = [\"epoch\", \"steps\", \"spearman\", \"pearson\", \"roc_auc\", \"pr_auc\"]\n",
    "    \n",
    "    def __call__(self, model, output_path=None, epoch=-1, steps=-1):\n",
    "        model.eval()\n",
    "        \n",
    "        # Encode pairs\n",
    "        emb1 = model.encode(self.texts1, batch_size=self.batch_size, \n",
    "                          show_progress_bar=False, convert_to_numpy=True)\n",
    "        emb2 = model.encode(self.texts2, batch_size=self.batch_size, \n",
    "                          show_progress_bar=False, convert_to_numpy=True)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        scores = np.sum(emb1 * emb2, axis=1) / (\n",
    "            np.linalg.norm(emb1, axis=1) * np.linalg.norm(emb2, axis=1) + 1e-8\n",
    "        )\n",
    "        \n",
    "        # Compute metrics\n",
    "        spearman, _ = spearmanr(self.labels, scores)\n",
    "        pearson, _ = pearsonr(self.labels, scores)\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(self.labels, scores)\n",
    "            pr_auc = average_precision_score(self.labels, scores)\n",
    "        except ValueError:\n",
    "            roc_auc, pr_auc = 0.0, 0.0\n",
    "        \n",
    "        log(f\"  [{self.name}] Epoch {epoch}: Spearman={spearman:.4f}, ROC-AUC={roc_auc:.4f}, PR-AUC={pr_auc:.4f}\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        if output_path:\n",
    "            csv_path = Path(output_path) / self.csv_file\n",
    "            if not csv_path.exists():\n",
    "                with open(csv_path, 'w') as f:\n",
    "                    f.write(','.join(self.csv_headers) + '\\n')\n",
    "            with open(csv_path, 'a') as f:\n",
    "                f.write(f\"{epoch},{steps},{spearman},{pearson},{roc_auc},{pr_auc}\\n\")\n",
    "        \n",
    "        return spearman  # Primary metric\n",
    "\n",
    "\n",
    "# --- Model Initialization with LoRA/PEFT ---\n",
    "def init_model_with_lora(config, device):\n",
    "    \"\"\"Initialize model with optional LoRA/PEFT adapters.\"\"\"\n",
    "    # Clear GPU memory\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    log(f\"üîß Loading base model: {config['model_name']}\")\n",
    "    model = SentenceTransformer(config['model_name'], device=device)\n",
    "    # Cap max sequence length to reduce memory footprint\n",
    "    model.max_seq_length = min(config['max_seq_length'], 256)\n",
    "    \n",
    "    # Apply LoRA if enabled\n",
    "    if config['use_lora']:\n",
    "        \n",
    "        # Inspect module names to find viable targets\n",
    "        base_model = model[0].auto_model\n",
    "        linear_names = [name for name, module in base_model.named_modules() if isinstance(module, nn.Linear)]\n",
    "        configured_targets = config['lora_target_modules']\n",
    "        resolved_targets = [t for t in configured_targets if any(t in n for n in linear_names)]\n",
    "        \n",
    "        if not resolved_targets:\n",
    "            # Fallback heuristics for BERT/MPNet-like models\n",
    "            priority = ['q_proj', 'k_proj', 'v_proj', 'query', 'key', 'value', 'dense']\n",
    "            resolved_targets = [p for p in priority if any(p in n for n in linear_names)]\n",
    "        \n",
    "        if not resolved_targets:\n",
    "            log(\"[ERROR] Could not find matching target modules for LoRA. Available linear submodules include:\")\n",
    "            sample = linear_names[:50]\n",
    "            for n in sample:\n",
    "                log(f\"   - {n}\")\n",
    "            raise ValueError(\"LoRA target_modules could not be resolved; please adjust CONFIG['lora_target_modules'].\")\n",
    "        \n",
    "        log(f\"[TARGET] Applying LoRA adapters...\")\n",
    "        log(f\"   Rank (r): {config['lora_r']}\")\n",
    "        log(f\"   Alpha: {config['lora_alpha']}\")\n",
    "        log(f\"   Target modules (resolved): {resolved_targets}\")\n",
    "        log(f\"   Dropout: {config['lora_dropout']}\")\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            r=config['lora_r'],\n",
    "            lora_alpha=config['lora_alpha'],\n",
    "            target_modules=resolved_targets,\n",
    "            lora_dropout=config['lora_dropout'],\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.FEATURE_EXTRACTION,\n",
    "        )\n",
    "        \n",
    "        # Apply PEFT wrapper\n",
    "        peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "        # Explicitly move PEFT model to device\n",
    "        log(f\"[GPU] Moving PEFT model to {device}\")\n",
    "        peft_model = peft_model.to(device)\n",
    "        model[0].auto_model = peft_model\n",
    "        \n",
    "        # Enable gradient checkpointing to reduce memory\n",
    "        try:\n",
    "            model[0].auto_model.gradient_checkpointing_enable()\n",
    "            log(\"[BUILD] Gradient checkpointing enabled\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "        trainable_pct = 100 * trainable_params / total_params\n",
    "        \n",
    "        log(f\"[OK] LoRA Applied!\")\n",
    "        log(f\"   Trainable params: {trainable_params:,} ({trainable_pct:.2f}%)\")\n",
    "        log(f\"   Total params: {total_params:,}\")\n",
    "        log(f\"   Parameter reduction: {100 - trainable_pct:.2f}%\")\n",
    "    else:\n",
    "        log(f\"‚ÑπÔ∏è Full fine-tuning (no LoRA)\")\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        log(f\"   Total params: {total_params:,}\")\n",
    "    \n",
    "    log(f\"[OK] Model loaded on {device}, max_seq_length={model.max_seq_length}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# --- V2: Cross-Validated Threshold Selection ---\n",
    "def get_cv_threshold(examples, model, n_folds=5):\n",
    "    \"\"\"\n",
    "    V2 NEW: Use k-fold cross-validation to find robust threshold.\n",
    "    \n",
    "    Returns the average best threshold across folds.\n",
    "    \"\"\"\n",
    "    \n",
    "    texts1 = [ex.texts[0] for ex in examples]\n",
    "    texts2 = [ex.texts[1] for ex in examples]\n",
    "    labels = np.array([ex.label for ex in examples])\n",
    "    \n",
    "    # Encode all pairs\n",
    "    emb1 = model.encode(texts1, batch_size=CONFIG['batch_size'], show_progress_bar=False)\n",
    "    emb2 = model.encode(texts2, batch_size=CONFIG['batch_size'], show_progress_bar=False)\n",
    "    scores = np.sum(emb1 * emb2, axis=1) / (np.linalg.norm(emb1, axis=1) * np.linalg.norm(emb2, axis=1) + 1e-8)\n",
    "    \n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=CONFIG['seed'])\n",
    "    best_thresholds = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(scores)):\n",
    "        val_scores = scores[val_idx]\n",
    "        val_labels = labels[val_idx]\n",
    "        \n",
    "        precision, recall, thresholds = precision_recall_curve(val_labels, val_scores)\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        best_threshold = thresholds[best_idx-1] if 0 < best_idx < len(thresholds)+1 else 0.5\n",
    "        best_thresholds.append(best_threshold)\n",
    "    \n",
    "    avg_threshold = np.mean(best_thresholds)\n",
    "    std_threshold = np.std(best_thresholds)\n",
    "    \n",
    "    log(f\"[STATS] CV Threshold ({n_folds}-fold): {avg_threshold:.4f} ¬± {std_threshold:.4f}\")\n",
    "    log(f\"   Per-fold thresholds: {[f'{t:.3f}' for t in best_thresholds]}\")\n",
    "    \n",
    "    return avg_threshold, std_threshold\n",
    "\n",
    "\n",
    "# --- Multi-Loss Wrapper ---\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combines MultipleNegativesRankingLoss + CosineSimilarityLoss with a shared forward pass.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, mnrl_weight=0.9, cosine_weight=0.1, mnrl_scale=20.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.cosine = losses.CosineSimilarityLoss(model)\n",
    "        self.mnrl_weight = mnrl_weight\n",
    "        self.cosine_weight = cosine_weight\n",
    "        self.mnrl_scale = mnrl_scale\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        \n",
    "        log(f\"üîÄ Combined Loss Initialized:\")\n",
    "        log(f\"   MNRL weight: {mnrl_weight:.2f}\")\n",
    "        log(f\"   Cosine weight: {cosine_weight:.2f}\")\n",
    "        log(f\"   MNRL scale: {mnrl_scale:.1f}\")\n",
    "    \n",
    "    def forward(self, sentence_features, labels):\n",
    "        \"\"\"Single forward pass to compute embeddings, then both losses.\"\"\"\n",
    "        embeddings = [self.model(sentence_feature)[\"sentence_embedding\"] for sentence_feature in sentence_features]\n",
    "        # Normalize for cosine similarity consistency\n",
    "        emb1 = F.normalize(embeddings[0], p=2, dim=1)\n",
    "        emb2 = F.normalize(embeddings[1], p=2, dim=1)\n",
    "        \n",
    "        # MultipleNegativesRankingLoss (manual computation)\n",
    "        sim_matrix = torch.matmul(emb1, emb2.transpose(0, 1)) * self.mnrl_scale\n",
    "        mnrl_labels = torch.arange(sim_matrix.size(0), device=sim_matrix.device)\n",
    "        mnrl_loss = self.ce(sim_matrix, mnrl_labels)\n",
    "        \n",
    "        # CosineSimilarityLoss using precomputed embeddings\n",
    "        cosine_loss = self.cosine.compute_loss_from_embeddings([emb1, emb2], labels)\n",
    "        \n",
    "        return self.mnrl_weight * mnrl_loss + self.cosine_weight * cosine_loss\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = init_model_with_lora(CONFIG, DEVICE)\n",
    "\n",
    "# ‚ú® IMPROVEMENT: Use ALL training pairs (positives + negatives) instead of filtering\n",
    "log(f\"\\n[STATS] Preparing training data...\")\n",
    "log(f\"   Train pairs (all): {len(train_examples):,}\")\n",
    "log(f\"   Eval pairs (all):  {len(eval_examples):,}\")\n",
    "pos_count = sum(1 for ex in train_examples if ex.label == 1.0)\n",
    "neg_count = len(train_examples) - pos_count\n",
    "log(f\"   Train balance: {pos_count:,} pos ({pos_count/len(train_examples)*100:.1f}%), {neg_count:,} neg ({neg_count/len(train_examples)*100:.1f}%)\")\n",
    "\n",
    "# ‚ú® IMPROVEMENT: Increase batch size from 2 to 16 for better gradient estimates\n",
    "_train_batch = 16\n",
    "train_dataloader = DataLoader(\n",
    "    train_examples,  # Use ALL pairs, not just positives\n",
    "    shuffle=True,\n",
    "    batch_size=_train_batch,\n",
    "    num_workers=0,\n",
    "    pin_memory=(DEVICE in ['cuda', 'mps'])\n",
    ")\n",
    "# ‚ú® IMPROVEMENT V3: Use MatryoshkaLoss + MultipleNegativesRankingLoss (SOTA 2024)\n",
    "# - Matryoshka: Variable embedding dimensions (768/512/256/128/64)\n",
    "# - MNRL: Contrastive learning with in-batch negatives\n",
    "# - Research: NeurIPS 2022, Nomic/BGE/E5 (2024)\n",
    "# - Expected: +15-25% performance vs CosineSimilarityLoss\n",
    "\n",
    "matryoshka_dimensions = [768, 512, 256, 128, 64]  # Flexible embedding sizes\n",
    "\n",
    "base_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "train_loss = losses.MatryoshkaLoss(\n",
    "    model,\n",
    "    base_loss,\n",
    "    matryoshka_dims=matryoshka_dimensions\n",
    ")\n",
    "\n",
    "log(f\"üîß Using MatryoshkaLoss + MultipleNegativesRankingLoss (SOTA 2024)\")\n",
    "log(f\"   Dimensions: {matryoshka_dimensions}\")\n",
    "log(f\"   In-batch negatives: automatic\")\n",
    "log(f\"   Batch size: {_train_batch}\")\n",
    "log(f\"   Utilizes curated hard negatives: YES\")\n",
    "log(f\"   Total training pairs: {len(train_examples):,}\")\n",
    "\n",
    "# Evaluator\n",
    "_evaluator_batch = _train_batch\n",
    "_evaluator_examples = eval_examples\n",
    "_evaluator = ITSMEvaluator(_evaluator_examples, batch_size=_evaluator_batch, name=\"eval\")\n",
    "evaluator = _evaluator\n",
    "\n",
    "log(f\"\\n[STATS] Training Setup:\")\n",
    "log(f\"   Batches per epoch: {len(train_dataloader)}\")\n",
    "log(f\"   Total training steps: {len(train_dataloader) * CONFIG['epochs']}\")\n",
    "log(f\"   Warmup steps: {int(len(train_dataloader) * CONFIG['epochs'] * CONFIG['warmup_ratio'])}\")\n",
    "log(f\"   Curriculum learning: {CONFIG['use_curriculum']}\")\n",
    "log(f\"\\n[STATS] Training Setup:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb82745",
   "metadata": {},
   "source": [
    "## 7. Execute Training\n",
    "\n",
    "Train model with curriculum learning (Phases 1-3).\n",
    "\n",
    "**Fixed:** Uses `CONFIG['lr']` (5e-5) instead of hardcoded 2e-5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6381e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 90)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<tokenize>:90\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mlog(f\"\\n{'='*70}\")\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CURRICULUM TRAINING (Pre-generated pairs only)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# --- Training Execution (V2: with Curriculum Learning) ---\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "if 'CONFIG' not in globals():\n",
    "    raise NameError(\"CONFIG is not defined. Run the configuration cell (Section 2) before training.\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "save_path = Path(CONFIG['output_dir']) / f\"real_servicenow_v2_{timestamp}\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log(f\"\\nüöÄ Starting Training (V2)...\")\n",
    "log(f\"   Output: {save_path}\")\n",
    "log(f\"   Epochs: {CONFIG['epochs']}\")\n",
    "log(f\"   Device: {DEVICE}\")\n",
    "\n",
    "# Calculate warmup steps\n",
    "# Calculate warmup steps (use phase1 size as reference)\n",
    "sample_phase_size = len(list(CURRICULUM_PHASES.values())[0])\n",
    "batches_per_phase = (sample_phase_size + CONFIG['batch_size'] - 1) // CONFIG['batch_size']\n",
    "total_steps = batches_per_phase * CONFIG['epochs_per_phase'] * 3  # 3 phases\n",
    "warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "eval_steps = max(100, len(train_dataloader) // 2)  # Evaluate twice per epoch\n",
    "use_amp = DEVICE != 'cuda'  # Flip: CUDA stays fp32, CPU/MPS use fp16 autocast when available\n",
    "\n",
    "try:\n",
    "\n",
    "    if CONFIG['use_curriculum']:\n",
    "# V2: Curriculum Learning - train in phases\n",
    "        log(\"\\n[CURRICULUM] Training in 3 phases (easy -> medium -> hard)\")\n",
    "\n",
    "        for phase_idx, (phase_name, phase_examples) in enumerate(sorted(CURRICULUM_PHASES.items())):\n",
    "            log(f\"\\n{'='*60}\")\n",
    "            log(f\"[PHASE {phase_idx + 1}] {phase_name.upper()}: {CONFIG['epochs_per_phase']} epochs\")\n",
    "            log(f\"   Training examples: {len(phase_examples):,}\")\n",
    "            log(f\"{'='*60}\")\n",
    "\n",
    "            # Create DataLoader for this phase\n",
    "            phase_dataloader = DataLoader(\n",
    "                phase_examples,\n",
    "                batch_size=CONFIG['batch_size'],\n",
    "                shuffle=True,\n",
    "                num_workers=0\n",
    "            )\n",
    "\n",
    "            log(f\"   Batches per epoch: {len(phase_dataloader)}\")\n",
    "            log(f\"   Total steps this phase: {len(phase_dataloader) * CONFIG['epochs_per_phase']}\")\n",
    "\n",
    "            # Train this phase\n",
    "            log(f\"\\n[TRAINING] {phase_name}...\")\n",
    "\n",
    "# Verify configuration before training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING CONFIGURATION VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Learning Rate: {CONFIG['lr']}\")\n",
    "print(f\"Curriculum Learning: {CONFIG['use_curriculum']}\")\n",
    "if CONFIG.get('use_curriculum'):\n",
    "    if CURRICULUM_PHASES:\n",
    "        print(f\"  Phases loaded: {len(CURRICULUM_PHASES)}\")\n",
    "        for phase_name, phase_data in sorted(CURRICULUM_PHASES.items()):\n",
    "            print(f\"    {phase_name}: {len(phase_data):,} pairs\")\n",
    "    else:\n",
    "        print(\"  WARNING: CURRICULUM_PHASES is None/empty!\")\n",
    "print(f\"Loss Function: MatryoshkaLoss + MultipleNegativesRankingLoss\")\n",
    "print(f\"Warmup Ratio: {CONFIG['warmup_ratio']}\")\n",
    "print(f\"Batch Size: {CONFIG['batch_size']}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "            model.fit(\n",
    "                train_objectives=[(phase_dataloader, train_loss)],\n",
    "                evaluator=evaluator,\n",
    "                epochs=CONFIG['epochs_per_phase'],\n",
    "                warmup_steps=warmup_steps,\n",
    "                optimizer_params={'lr': CONFIG['lr']},\n",
    "                scheduler='WarmupCosine',  # Cosine decay with warmup\n",
    "                output_path=str(save_path),\n",
    "                evaluation_steps=eval_steps,\n",
    "                save_best_model=True,\n",
    "                show_progress_bar=True,\n",
    "                use_amp=use_amp,\n",
    "            )\n",
    "\n",
    "            log(f\"[OK] {phase_name} complete!\")\n",
    "\n",
    "        log(f\"\\n{'='*70}\")\n",
    "        log(\"[SUCCESS] All curriculum phases complete!\")\n",
    "        log(f\"{'='*70}\")\n",
    "\n",
    "    else:\n",
    "        # Non-curriculum: train on all examples together\n",
    "        log(\"\\n[TRAINING] Standard training (no curriculum)\")\n",
    "        train_dataloader = DataLoader(\n",
    "            train_examples,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            train_objectives=[(train_dataloader, train_loss)],\n",
    "            evaluator=evaluator,\n",
    "            epochs=CONFIG['epochs'],\n",
    "            warmup_steps=warmup_steps,\n",
    "            optimizer_params={'lr': CONFIG['lr']},\n",
    "            scheduler='WarmupCosine',  # Cosine decay with warmup\n",
    "            output_path=str(save_path),\n",
    "            evaluation_steps=eval_steps,\n",
    "            save_best_model=True,\n",
    "            show_progress_bar=True,\n",
    "            use_amp=use_amp,\n",
    "        )\n",
    "except RuntimeError as e:\n",
    "    err_msg = str(e).lower()\n",
    "    if (\"out of memory\" in err_msg) or (\"no kernel image\" in err_msg) or (\"not compatible\" in err_msg):\n",
    "        log(f\"[ERROR] Runtime Error: {e}\")\n",
    "        log(\"üí° Falling back to CPU/MPS to continue training...\")\n",
    "        # Cleanup GPU\n",
    "        try:\n",
    "            del model\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "        gc.collect()\n",
    "    \n",
    "        # Fallback device\n",
    "        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            DEVICE = 'mps'\n",
    "            log(\"üçé Switching to MPS\")\n",
    "        else:\n",
    "            DEVICE = 'cpu'\n",
    "            log(\"[BUILD] Switching to CPU\")\n",
    "    \n",
    "        # Re-init model and loss on fallback device\n",
    "        model = init_model_with_lora(CONFIG, DEVICE)\n",
    "        # Always use CosineSimilarityLoss with all pairs\n",
    "        # Fallback to MatryoshkaLoss + MNRL\n",
    "        base_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "        train_loss = losses.MatryoshkaLoss(\n",
    "            model, base_loss,\n",
    "            matryoshka_dims=[768, 512, 256, 128, 64]\n",
    "        )\n",
    "    \n",
    "        # Recreate dataloaders with safe batch size on fallback device\n",
    "        safe_batch = 8 if DEVICE != 'cuda' else 16\n",
    "        fallback_use_amp = DEVICE != 'cuda'\n",
    "        if CONFIG['use_curriculum']:\n",
    "            for phase_idx, (phase_name, phase_examples) in enumerate(sorted(CURRICULUM_PHASES.items())):\n",
    "                log(f\"\\n[Fallback] Phase {phase_idx + 1}: {CONFIG['epochs_per_phase']} epochs\")\n",
    "                # Use pre-loaded phase examples (already in memory)\n",
    "                # phase_examples already available from loop\n",
    "                phase_train_examples = phase_examples\n",
    "                phase_dataloader = DataLoader(\n",
    "                    phase_train_examples,\n",
    "                    shuffle=True,\n",
    "                    batch_size=safe_batch,\n",
    "                    num_workers=0,\n",
    "                    pin_memory=(DEVICE in ['cuda', 'mps'])\n",
    "                )\n",
    "                phase_warmup = int(len(phase_dataloader) * CONFIG['epochs_per_phase'] * CONFIG['warmup_ratio'])\n",
    "                model.fit(\n",
    "                    train_objectives=[(phase_dataloader, train_loss)],\n",
    "                    evaluator=evaluator,\n",
    "                    epochs=CONFIG['epochs_per_phase'],\n",
    "                    warmup_steps=phase_warmup,\n",
    "                    optimizer_params={'lr': CONFIG['lr']},\n",
    "                    scheduler='WarmupCosine',  # Cosine decay with warmup\n",
    "                    output_path=str(save_path),\n",
    "                    evaluation_steps=eval_steps,\n",
    "                    save_best_model=True,\n",
    "                    show_progress_bar=True,\n",
    "                    use_amp=fallback_use_amp,\n",
    "                )\n",
    "        else:\n",
    "            model.fit(\n",
    "                train_objectives=[(phase_dataloader, train_loss)],\n",
    "                evaluator=evaluator,\n",
    "                epochs=CONFIG['epochs'],\n",
    "                warmup_steps=warmup_steps,\n",
    "                optimizer_params={'lr': CONFIG['lr']},\n",
    "                scheduler='WarmupCosine',  # Cosine decay with warmup\n",
    "                output_path=str(save_path),\n",
    "                evaluation_steps=eval_steps,\n",
    "                save_best_model=True,\n",
    "                show_progress_bar=True,\n",
    "                use_amp=fallback_use_amp,\n",
    "            )\n",
    "    \n",
    "        log(\"\\n[OK] Training complete on fallback device!\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Reload best model\n",
    "log(\"\\n[STATS] Loading best model for final evaluation...\")\n",
    "best_model = SentenceTransformer(str(save_path), device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e330a26a",
   "metadata": {},
   "source": [
    "## 8. Score Distribution Diagnostic\n",
    "\n",
    "Analyze predicted similarity score distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6907f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# SCORE DISTRIBUTION DIAGNOSTIC (V2)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# This validates that the model learned to separate positive/negative pairs\n",
    "\n",
    "log(\"\\nüîç SCORE DISTRIBUTION DIAGNOSTIC\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Sample from eval set\n",
    "sample_size = min(500, len(eval_examples))\n",
    "sample_pos = [ex for ex in eval_examples if ex.label == 1.0][:sample_size//2]\n",
    "sample_neg = [ex for ex in eval_examples if ex.label == 0.0][:sample_size//2]\n",
    "\n",
    "log(f\"Analyzing {len(sample_pos)} positive + {len(sample_neg)} negative pairs...\")\n",
    "\n",
    "# Compute scores for positive pairs\n",
    "pos_scores = []\n",
    "for ex in tqdm(sample_pos, desc=\"Positive pairs\"):\n",
    "    emb1 = best_model.encode([ex.texts[0]], show_progress_bar=False)[0]\n",
    "    emb2 = best_model.encode([ex.texts[1]], show_progress_bar=False)[0]\n",
    "    score = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-8)\n",
    "    pos_scores.append(score)\n",
    "\n",
    "# Compute scores for negative pairs\n",
    "neg_scores = []\n",
    "for ex in tqdm(sample_neg, desc=\"Negative pairs\"):\n",
    "    emb1 = best_model.encode([ex.texts[0]], show_progress_bar=False)[0]\n",
    "    emb2 = best_model.encode([ex.texts[1]], show_progress_bar=False)[0]\n",
    "    score = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-8)\n",
    "    neg_scores.append(score)\n",
    "\n",
    "# Statistics\n",
    "pos_mean, pos_std = np.mean(pos_scores), np.std(pos_scores)\n",
    "neg_mean, neg_std = np.mean(neg_scores), np.std(neg_scores)\n",
    "separability = pos_mean - neg_mean\n",
    "\n",
    "log(f\"\\n[STATS] SCORE STATISTICS:\")\n",
    "log(f\"   Positive pairs: mean={pos_mean:.4f}, std={pos_std:.4f}, range=[{min(pos_scores):.4f}, {max(pos_scores):.4f}]\")\n",
    "log(f\"   Negative pairs: mean={neg_mean:.4f}, std={neg_std:.4f}, range=[{min(neg_scores):.4f}, {max(neg_scores):.4f}]\")\n",
    "log(f\"   Separability (Œî): {separability:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "log(f\"\\nüí° INTERPRETATION:\")\n",
    "if separability >= 0.15:\n",
    "    log(f\"   [OK] EXCELLENT: Strong separation (Œî={separability:.4f} >= 0.15)\")\n",
    "    log(f\"      Model clearly distinguishes positive from negative pairs\")\n",
    "elif separability >= 0.08:\n",
    "    log(f\"   OK GOOD: Moderate separation (Œî={separability:.4f} >= 0.08)\")\n",
    "    log(f\"      Model shows useful discrimination\")\n",
    "elif separability >= 0.03:\n",
    "    log(f\"   [WARN]  WEAK: Minimal separation (Œî={separability:.4f} >= 0.03)\")\n",
    "    log(f\"      Model barely learned to discriminate\")\n",
    "else:\n",
    "    log(f\"   [ERROR] FAILURE: No separation (Œî={separability:.4f} < 0.03)\")\n",
    "    log(f\"      Model produces similar scores for both positive and negative pairs!\")\n",
    "    log(f\"      Training likely failed - check batch size, loss function, or data quality\")\n",
    "\n",
    "# Distribution overlap check\n",
    "overlap_threshold = 0.5\n",
    "overlap_count = sum(1 for s in neg_scores if s > overlap_threshold)\n",
    "log(f\"\\n[STATS] DISTRIBUTION OVERLAP:\")\n",
    "log(f\"   Negatives scoring > {overlap_threshold}: {overlap_count}/{len(neg_scores)} ({overlap_count/len(neg_scores)*100:.1f}%)\")\n",
    "if overlap_count / len(neg_scores) > 0.3:\n",
    "    log(f\"   [WARN]  WARNING: High overlap - many negatives score like positives\")\n",
    "\n",
    "log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441eb57",
   "metadata": {},
   "source": [
    "## 9. Evaluation & Visualization\n",
    "\n",
    "ROC curve, PR curve, confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22805671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip borderline evaluation if not using legacy mode\n",
    "use_legacy_mode = CONFIG.get('use_legacy_mode', False)\n",
    "include_borderline = use_legacy_mode and borderline_examples and len(borderline_examples) > 0\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def comprehensive_eval(examples, model, name=\"\", use_cv_threshold=False):\n",
    "    \"\"\"Run comprehensive evaluation on a set of pairs (V2: with CV threshold option).\"\"\"\n",
    "    texts1 = [ex.texts[0] for ex in examples]\n",
    "    texts2 = [ex.texts[1] for ex in examples]\n",
    "    labels = np.array([ex.label for ex in examples])\n",
    "\n",
    "    # Encode\n",
    "    log(f\"? Encoding {len(examples)} pairs for {name}...\")\n",
    "    emb1 = model.encode(texts1, batch_size=CONFIG['batch_size'], \n",
    "                       show_progress_bar=True, convert_to_numpy=True)\n",
    "    emb2 = model.encode(texts2, batch_size=CONFIG['batch_size'], \n",
    "                       show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "    # Cosine similarity\n",
    "    scores = np.sum(emb1 * emb2, axis=1) / (\n",
    "        np.linalg.norm(emb1, axis=1) * np.linalg.norm(emb2, axis=1) + 1e-8\n",
    "    )\n",
    "\n",
    "    # Metrics\n",
    "    spearman, _ = spearmanr(labels, scores)\n",
    "    pearson, _ = pearsonr(labels, scores)\n",
    "    roc_auc = roc_auc_score(labels, scores)\n",
    "    pr_auc = average_precision_score(labels, scores)\n",
    "\n",
    "    # Find best threshold\n",
    "    fpr, tpr, roc_thresholds = roc_curve(labels, scores)\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(labels, scores)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = pr_thresholds[best_idx-1] if 0 < best_idx < len(pr_thresholds)+1 else 0.5\n",
    "\n",
    "    # V2: Use CV threshold if requested\n",
    "    if use_cv_threshold and 'cv_threshold' in globals():\n",
    "        best_threshold = cv_threshold\n",
    "        log(f\"   Using CV threshold: {best_threshold:.4f}\")\n",
    "\n",
    "    # Metrics at best threshold\n",
    "    preds = (scores >= best_threshold).astype(int)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds)\n",
    "    rec = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    log(f\"[STATS] {name} Results:\")\n",
    "    log(f\"   Spearman:  {spearman:.4f}\")\n",
    "    log(f\"   Pearson:   {pearson:.4f}\")\n",
    "    log(f\"   ROC-AUC:   {roc_auc:.4f}\")\n",
    "    log(f\"   PR-AUC:    {pr_auc:.4f}\")\n",
    "    log(f\"   Best Threshold: {best_threshold:.3f}\")\n",
    "    log(f\"   F1 @ best: {f1:.4f}\")\n",
    "    log(f\"   Precision: {prec:.4f}\")\n",
    "    log(f\"   Recall:    {rec:.4f}\")\n",
    "    log(f\"   Accuracy:  {acc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'labels': labels, 'scores': scores,\n",
    "        'spearman': spearman, 'pearson': pearson,\n",
    "        'roc_auc': roc_auc, 'pr_auc': pr_auc,\n",
    "        'fpr': fpr, 'tpr': tpr,\n",
    "        'precision': precision, 'recall': recall,\n",
    "        'best_threshold': best_threshold,\n",
    "        'f1': f1, 'prec': prec, 'rec': rec, 'acc': acc,\n",
    "        'cm': cm, 'texts1': texts1, 'texts2': texts2  # V2: Keep texts for error analysis\n",
    "    }\n",
    "\n",
    "# V2: Get cross-validated threshold first\n",
    "log(\"=\"*60)\n",
    "log(\"?? CROSS-VALIDATED THRESHOLD SELECTION (V2)\")\n",
    "log(\"=\"*60)\n",
    "cv_threshold, cv_std = get_cv_threshold(\n",
    "    eval_examples, \n",
    "    best_model, \n",
    "    n_folds=CONFIG.get('threshold_cv_folds', 5)  # Default to 5 folds if not set\n",
    ")\n",
    "\n",
    "# Evaluate on all sets\n",
    "log(\"=\"*60)\n",
    "log(\"?? FINAL EVALUATION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "eval_results = comprehensive_eval(eval_examples, best_model, \"Eval Set\")\n",
    "holdout_results = comprehensive_eval(holdout_examples, best_model, \"Holdout Set\")\n",
    "\n",
    "# V2 NEW: Evaluate on borderline (harder) test set\n",
    "if include_borderline:\n",
    "    log(\"=\"*60)\n",
    "    log(\"?? BORDERLINE TEST (V2 - Harder Evaluation)\")\n",
    "    log(\"=\"*60)\n",
    "    borderline_results = comprehensive_eval(borderline_examples, best_model, \"Borderline Set\")\n",
    "else:\n",
    "    borderline_results = None\n",
    "    log(\"=\"*60)\n",
    "    log(\"[SKIP] Borderline Test\")\n",
    "    log(\"=\"*60)\n",
    "    if not use_legacy_mode:\n",
    "        log(\"Legacy mode disabled - borderline evaluation skipped.\")\n",
    "    if not borderline_examples:\n",
    "        log(\"No borderline examples available.\")\n",
    "    if CONFIG.get('use_curriculum', False):\n",
    "        log(\"Using curriculum learning - borderline test not generated.\")\n",
    "        log(\"Evaluation uses fixed_test_pairs.json instead.\")\n",
    "\n",
    "\n",
    "# Visualization (V2: Added borderline results)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 14))\n",
    "\n",
    "# ROC Curves\n",
    "axes[0,0].plot(eval_results['fpr'], eval_results['tpr'], \n",
    "               label=f\"Eval ROC-AUC = {eval_results['roc_auc']:.3f}\")\n",
    "axes[0,0].plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "axes[0,0].set_title('ROC Curve (Eval)')\n",
    "axes[0,0].set_xlabel('False Positive Rate')\n",
    "axes[0,0].set_ylabel('True Positive Rate')\n",
    "axes[0,0].legend()\n",
    "\n",
    "axes[0,1].plot(holdout_results['fpr'], holdout_results['tpr'], \n",
    "               label=f\"Holdout ROC-AUC = {holdout_results['roc_auc']:.3f}\", color='orange')\n",
    "axes[0,1].plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "axes[0,1].set_title('ROC Curve (Holdout)')\n",
    "axes[0,1].set_xlabel('False Positive Rate')\n",
    "axes[0,1].set_ylabel('True Positive Rate')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# V2 NEW: Borderline ROC\n",
    "if include_borderline:\n",
    "    axes[0,2].plot(borderline_results['fpr'], borderline_results['tpr'], \n",
    "                   label=f\"Borderline ROC-AUC = {borderline_results['roc_auc']:.3f}\", color='red')\n",
    "    axes[0,2].plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "    axes[0,2].set_title('ROC Curve (Borderline - V2)')\n",
    "    axes[0,2].set_xlabel('False Positive Rate')\n",
    "    axes[0,2].set_ylabel('True Positive Rate')\n",
    "    axes[0,2].legend()\n",
    "else:\n",
    "    axes[0,2].axis('off')\n",
    "    axes[0,2].text(0.5, 0.5, 'Borderline skipped', ha='center', va='center')\n",
    "\n",
    "# PR Curves\n",
    "axes[1,0].plot(eval_results['recall'], eval_results['precision'], \n",
    "               label=f\"Eval PR-AUC = {eval_results['pr_auc']:.3f}\")\n",
    "axes[1,0].scatter([eval_results['rec']], [eval_results['prec']], \n",
    "                  color='red', s=100, zorder=5,\n",
    "                  label=f\"Best F1={eval_results['f1']:.3f}\")\n",
    "axes[1,0].set_title('Precision-Recall (Eval)')\n",
    "axes[1,0].set_xlabel('Recall')\n",
    "axes[1,0].set_ylabel('Precision')\n",
    "axes[1,0].legend()\n",
    "\n",
    "axes[1,1].plot(holdout_results['recall'], holdout_results['precision'], \n",
    "               label=f\"Holdout PR-AUC = {holdout_results['pr_auc']:.3f}\", color='orange')\n",
    "axes[1,1].scatter([holdout_results['rec']], [holdout_results['prec']], \n",
    "                  color='red', s=100, zorder=5,\n",
    "                  label=f\"Best F1={holdout_results['f1']:.3f}\")\n",
    "axes[1,1].set_title('Precision-Recall (Holdout)')\n",
    "axes[1,1].set_xlabel('Recall')\n",
    "axes[1,1].set_ylabel('Precision')\n",
    "axes[1,1].legend()\n",
    "\n",
    "# V2 NEW: Borderline PR\n",
    "if include_borderline:\n",
    "    axes[1,2].plot(borderline_results['recall'], borderline_results['precision'], \n",
    "                   label=f\"Borderline PR-AUC = {borderline_results['pr_auc']:.3f}\", color='red')\n",
    "    axes[1,2].scatter([borderline_results['rec']], [borderline_results['prec']], \n",
    "                      color='green', s=100, zorder=5,\n",
    "                      label=f\"Best F1={borderline_results['f1']:.3f}\")\n",
    "    axes[1,2].set_title('Precision-Recall (Borderline - V2)')\n",
    "    axes[1,2].set_xlabel('Recall')\n",
    "    axes[1,2].set_ylabel('Precision')\n",
    "    axes[1,2].legend()\n",
    "else:\n",
    "    axes[1,2].axis('off')\n",
    "    axes[1,2].text(0.5, 0.5, 'Borderline skipped', ha='center', va='center')\n",
    "\n",
    "# Score Distributions\n",
    "score_sets = [\n",
    "    (eval_results, 'Eval', 'blue'), \n",
    "    (holdout_results, 'Holdout', 'orange')\n",
    "]\n",
    "if include_borderline:\n",
    "    score_sets.append((borderline_results, 'Borderline', 'red'))\n",
    "\n",
    "for idx, (results, name, color) in enumerate(score_sets):\n",
    "    ax = axes[2, idx]\n",
    "    neg_scores = results['scores'][results['labels'] == 0]\n",
    "    pos_scores = results['scores'][results['labels'] == 1]\n",
    "\n",
    "    ax.hist(neg_scores, bins=30, alpha=0.6, label='Negative (0)', color='blue')\n",
    "    ax.hist(pos_scores, bins=30, alpha=0.6, label='Positive (1)', color='orange')\n",
    "    ax.axvline(results['best_threshold'], color='red', linestyle='--', \n",
    "               label=f\"Threshold={results['best_threshold']:.3f}\")\n",
    "    ax.axvline(cv_threshold, color='green', linestyle=':', \n",
    "               label=f\"CV Threshold={cv_threshold:.3f}\")\n",
    "    ax.set_title(f'Score Distribution ({name})')\n",
    "    ax.set_xlabel('Cosine Similarity')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "if not include_borderline:\n",
    "    axes[2,2].axis('off')\n",
    "    axes[2,2].text(0.5, 0.5, 'Borderline skipped', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path / 'evaluation_plots_v2.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "log(f\"[STATS] Plots saved to {save_path / 'evaluation_plots_v2.png'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4da157",
   "metadata": {},
   "source": [
    "## 10. Error Analysis\n",
    "\n",
    "Examine false positives and false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a25c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# V2 NEW: ERROR ANALYSIS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def analyze_errors(results, name=\"\", top_k=10):\n",
    "    \"\"\"\n",
    "    Analyze systematic failure patterns in model predictions.\n",
    "    \n",
    "    Returns insights about:\n",
    "    - Worst false positives (high score, label=0)\n",
    "    - Worst false negatives (low score, label=1)\n",
    "    - Text length patterns\n",
    "    \"\"\"\n",
    "    labels = results['labels']\n",
    "    scores = results['scores']\n",
    "    texts1 = results['texts1']\n",
    "    texts2 = results['texts2']\n",
    "    threshold = results['best_threshold']\n",
    "    \n",
    "    # Identify errors\n",
    "    preds = (scores >= threshold).astype(int)\n",
    "    \n",
    "    # False Positives: predicted 1, actual 0\n",
    "    fp_mask = (preds == 1) & (labels == 0)\n",
    "    fp_indices = np.where(fp_mask)[0]\n",
    "    fp_scores = scores[fp_mask]\n",
    "    \n",
    "    # False Negatives: predicted 0, actual 1\n",
    "    fn_mask = (preds == 0) & (labels == 1)\n",
    "    fn_indices = np.where(fn_mask)[0]\n",
    "    fn_scores = scores[fn_mask]\n",
    "    \n",
    "    log(f\"\\n{'='*60}\")\n",
    "    log(f\"üîç ERROR ANALYSIS: {name}\")\n",
    "    log(f\"{'='*60}\")\n",
    "    log(f\"Total pairs: {len(labels):,}\")\n",
    "    log(f\"False Positives: {len(fp_indices):,} ({len(fp_indices)/len(labels)*100:.2f}%)\")\n",
    "    log(f\"False Negatives: {len(fn_indices):,} ({len(fn_indices)/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    # Worst False Positives (highest scoring negatives)\n",
    "    if len(fp_indices) > 0:\n",
    "        log(f\"\\nüìõ WORST FALSE POSITIVES (Top {min(top_k, len(fp_indices))}):\")\n",
    "        log(f\"   These pairs scored HIGH but should be DISSIMILAR\")\n",
    "        worst_fp_order = np.argsort(fp_scores)[::-1][:top_k]\n",
    "        \n",
    "        for rank, idx in enumerate(worst_fp_order):\n",
    "            orig_idx = fp_indices[idx]\n",
    "            score = scores[orig_idx]\n",
    "            t1, t2 = texts1[orig_idx], texts2[orig_idx]\n",
    "            log(f\"\\n   [{rank+1}] Score: {score:.4f}\")\n",
    "            log(f\"       Text 1: {t1[:100]}...\")\n",
    "            log(f\"       Text 2: {t2[:100]}...\")\n",
    "    \n",
    "    # Worst False Negatives (lowest scoring positives)\n",
    "    if len(fn_indices) > 0:\n",
    "        log(f\"\\nüìõ WORST FALSE NEGATIVES (Top {min(top_k, len(fn_indices))}):\")\n",
    "        log(f\"   These pairs scored LOW but should be SIMILAR\")\n",
    "        worst_fn_order = np.argsort(fn_scores)[:top_k]\n",
    "        \n",
    "        for rank, idx in enumerate(worst_fn_order):\n",
    "            orig_idx = fn_indices[idx]\n",
    "            score = scores[orig_idx]\n",
    "            t1, t2 = texts1[orig_idx], texts2[orig_idx]\n",
    "            log(f\"\\n   [{rank+1}] Score: {score:.4f}\")\n",
    "            log(f\"       Text 1: {t1[:100]}...\")\n",
    "            log(f\"       Text 2: {t2[:100]}...\")\n",
    "    \n",
    "    # Text length analysis\n",
    "    log(f\"\\nüìè TEXT LENGTH ANALYSIS:\")\n",
    "    all_lengths = [len(t) for t in texts1 + texts2]\n",
    "    fp_lengths = [len(texts1[i]) + len(texts2[i]) for i in fp_indices] if len(fp_indices) > 0 else [0]\n",
    "    fn_lengths = [len(texts1[i]) + len(texts2[i]) for i in fn_indices] if len(fn_indices) > 0 else [0]\n",
    "    \n",
    "    log(f\"   Overall avg length: {np.mean(all_lengths):.0f} chars\")\n",
    "    log(f\"   FP pairs avg length: {np.mean(fp_lengths):.0f} chars\")\n",
    "    log(f\"   FN pairs avg length: {np.mean(fn_lengths):.0f} chars\")\n",
    "    \n",
    "    # Score distribution in errors\n",
    "    log(f\"\\n[STATS] SCORE DISTRIBUTION IN ERRORS:\")\n",
    "    if len(fp_scores) > 0:\n",
    "        log(f\"   FP scores: min={fp_scores.min():.4f}, max={fp_scores.max():.4f}, mean={fp_scores.mean():.4f}\")\n",
    "    if len(fn_scores) > 0:\n",
    "        log(f\"   FN scores: min={fn_scores.min():.4f}, max={fn_scores.max():.4f}, mean={fn_scores.mean():.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'fp_count': len(fp_indices),\n",
    "        'fn_count': len(fn_indices),\n",
    "        'fp_scores': fp_scores,\n",
    "        'fn_scores': fn_scores,\n",
    "        'fp_avg_length': np.mean(fp_lengths),\n",
    "        'fn_avg_length': np.mean(fn_lengths)\n",
    "    }\n",
    "\n",
    "# Run error analysis on all sets\n",
    "eval_errors = analyze_errors(eval_results, \"Eval Set\", top_k=5) if eval_results is not None else None\n",
    "holdout_errors = analyze_errors(holdout_results, \"Holdout Set\", top_k=5) if holdout_results is not None else None\n",
    "borderline_errors = analyze_errors(borderline_results, \"Borderline Set\", top_k=5) if borderline_results is not None else None\n",
    "\n",
    "# Summary comparison\n",
    "log(f\"\\n{'='*60}\")\n",
    "log(f\"[STATS] ERROR SUMMARY COMPARISON\")\n",
    "log(f\"{'='*60}\")\n",
    "log(f\"{'Set':<15} {'FP Count':<12} {'FN Count':<12} {'FP Rate':<12} {'FN Rate':<12}\")\n",
    "log(f\"{'-'*60}\")\n",
    "for name, errors, results in [\n",
    "    (\"Eval\", eval_errors, eval_results),\n",
    "    (\"Holdout\", holdout_errors, holdout_results),\n",
    "    (\"Borderline\", borderline_errors, borderline_results)\n",
    "]:\n",
    "    # Skip if results is None (e.g., borderline not available)\n",
    "    if results is None:\n",
    "        continue\n",
    "    \n",
    "    total = len(results['labels'])\n",
    "    fp_rate = errors['fp_count'] / total * 100\n",
    "    fn_rate = errors['fn_count'] / total * 100\n",
    "    log(f\"{name:<15} {errors['fp_count']:<12} {errors['fn_count']:<12} {fp_rate:<12.2f}% {fn_rate:<12.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43651e3",
   "metadata": {},
   "source": [
    "## 11. Adversarial Diagnostic\n",
    "\n",
    "Test for category leakage (cross-category positives, same-category negatives).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa7c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TFIDFSimilarityCalculator for adversarial diagnostic\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class TFIDFSimilarityCalculator:\n",
    "    \"\"\"Calculate TF-IDF based similarity for text pairs.\"\"\"\n",
    "\n",
    "    def __init__(self, texts, max_features=5000):\n",
    "        \"\"\"Initialize with corpus of texts.\"\"\"\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            lowercase=True,\n",
    "            stop_words='english'\n",
    "        )\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "\n",
    "    def similarity(self, idx1, idx2):\n",
    "        \"\"\"Get cosine similarity between two texts by index.\"\"\"\n",
    "        vec1 = self.tfidf_matrix[idx1]\n",
    "        vec2 = self.tfidf_matrix[idx2]\n",
    "        return cosine_similarity(vec1, vec2)[0, 0]\n",
    "\n",
    "\n",
    "log(\"=\"*60)\n",
    "log(\"üî¨ ADVERSARIAL DIAGNOSTIC: Testing Category Leakage\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Use holdout data for adversarial test (completely unseen)\n",
    "diag_df = holdout_df.reset_index(drop=True)\n",
    "\n",
    "# Build content-only text (remove category context to test pure semantic understanding)\n",
    "diag_df['content_only'] = diag_df['Description'].str.strip()\n",
    "\n",
    "# Build TF-IDF on content-only text\n",
    "log(\"‚è≥ Building TF-IDF for adversarial pair mining...\")\n",
    "diag_tfidf = TFIDFSimilarityCalculator(diag_df['content_only'].tolist(), max_features=10000)\n",
    "\n",
    "# Generate adversarial pairs\n",
    "hard_positives = []  # Cross-category, high TF-IDF\n",
    "hard_negatives = []  # Same-category, low TF-IDF\n",
    "\n",
    "target_each = 300\n",
    "attempts, max_attempts = 0, 100000\n",
    "\n",
    "log(\"‚è≥ Mining adversarial pairs...\")\n",
    "pbar = tqdm(total=target_each * 2, desc=\"Adversarial pairs\")\n",
    "\n",
    "while (len(hard_positives) < target_each or len(hard_negatives) < target_each) and attempts < max_attempts:\n",
    "    attempts += 1\n",
    "    i1, i2 = random.sample(range(len(diag_df)), 2)\n",
    "    \n",
    "    cat1 = diag_df.at[i1, 'category_id']\n",
    "    cat2 = diag_df.at[i2, 'category_id']\n",
    "    tfidf_sim = diag_tfidf.similarity(i1, i2)\n",
    "    \n",
    "    # Hard Positive: DIFFERENT category but HIGH content similarity\n",
    "    if len(hard_positives) < target_each and cat1 != cat2 and tfidf_sim > 0.4:\n",
    "        hard_positives.append(InputExample(\n",
    "            texts=[diag_df.at[i1, 'content_only'], diag_df.at[i2, 'content_only']],\n",
    "            label=1.0\n",
    "        ))\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Hard Negative: SAME category but LOW content similarity\n",
    "    if len(hard_negatives) < target_each and cat1 == cat2 and tfidf_sim < 0.15:\n",
    "        hard_negatives.append(InputExample(\n",
    "            texts=[diag_df.at[i1, 'content_only'], diag_df.at[i2, 'content_only']],\n",
    "            label=0.0\n",
    "        ))\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "log(f\"[OK] Generated {len(hard_positives)} hard positives, {len(hard_negatives)} hard negatives\")\n",
    "\n",
    "# Evaluate on adversarial pairs\n",
    "adversarial_examples = hard_positives + hard_negatives\n",
    "if len(adversarial_examples) >= 100:\n",
    "    adv_results = comprehensive_eval(adversarial_examples, best_model, \"Adversarial\")\n",
    "    \n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"[TARGET] ADVERSARIAL DIAGNOSTIC RESULTS\")\n",
    "    log(\"=\"*60)\n",
    "    log(f\"Standard Eval ROC-AUC:     {eval_results['roc_auc']:.4f}\")\n",
    "    log(f\"Adversarial ROC-AUC:       {adv_results['roc_auc']:.4f}\")\n",
    "    log(f\"Adversarial F1 @ best:     {adv_results['f1']:.4f}\")\n",
    "    \n",
    "    # Verdict\n",
    "    if adv_results['roc_auc'] >= 0.70 and adv_results['f1'] >= 0.65:\n",
    "        log(\"\\n[OK] VERDICT: Model is ROBUST to category shortcuts!\")\n",
    "        log(\"   ‚Üí Performance holds when categories don't predict similarity\")\n",
    "        log(\"   ‚Üí Model learned semantic content understanding\")\n",
    "        DIAGNOSTIC_PASSED = True\n",
    "    else:\n",
    "        log(\"\\n[WARN] VERDICT: Model may be exploiting category shortcuts\")\n",
    "        log(\"   ‚Üí Consider increasing hard negatives ratio\")\n",
    "        log(\"   ‚Üí Or remove category context from training text\")\n",
    "        DIAGNOSTIC_PASSED = False\n",
    "else:\n",
    "    log(\"[WARN] Could not generate enough adversarial pairs\")\n",
    "    DIAGNOSTIC_PASSED = None\n",
    "\n",
    "# Cleanup\n",
    "del diag_tfidf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf92b0b",
   "metadata": {},
   "source": [
    "## 12. Save Training Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f247e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TRAINING METADATA EXPORT (V2)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "\n",
    "def save_training_metadata(output_dir: str, config: dict, metrics: dict, \n",
    "                          data_stats: dict, adversarial_results: dict = None,\n",
    "                          error_analysis: dict = None):\n",
    "    \"\"\"\n",
    "    Save comprehensive training metadata for reproducibility (V2: includes error analysis).\n",
    "    \n",
    "    Following project convention: all model outputs include training_metadata.json\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"training_timestamp\": datetime.now().isoformat(),\n",
    "        \"model_version\": \"V2\",\n",
    "        \"model_name\": config.get('model_name', 'all-mpnet-finetuned'),\n",
    "        \"base_model\": config.get('model_name', 'sentence-transformers/all-mpnet-base-v2'),\n",
    "        \n",
    "        # Hyperparameters\n",
    "        \"hyperparameters\": {\n",
    "            \"epochs\": config.get('epochs'),\n",
    "            \"batch_size\": config.get('batch_size'),\n",
    "            \"learning_rate\": config.get('lr'),  # V2: Use actual config value\n",
    "            \"max_seq_length\": config.get('max_seq_length'),\n",
    "            \"warmup_ratio\": config.get('warmup_ratio'),\n",
    "            \"loss_function\": \"MatryoshkaLoss + MultipleNegativesRankingLoss\"  # V2: Changed to use all pairs\n",
    "        },\n",
    "        \n",
    "        # V2: Curriculum learning config\n",
    "        \"curriculum_learning\": {\n",
    "            \"enabled\": config.get('use_curriculum', False),\n",
    "            \"num_phases\": 3 if config.get('use_curriculum') else 0,\n",
    "            \"epochs_per_phase\": config.get('epochs_per_phase', 4),\n",
    "            \"total_pairs\": 15000 if config.get('use_curriculum') else config.get('num_pairs', 0)\n",
    "        },\n",
    "        \n",
    "        # Data configuration (V2: stricter thresholds)\n",
    "        \"data_config\": {\n",
    "            \"source_data\": config.get('source_data'),\n",
    "            \"num_pairs\": config.get('num_pairs'),\n",
    "            \"min_text_length\": config.get('min_text_length'),\n",
    "            \"eval_split\": config.get('eval_split'),\n",
    "            \"holdout_split\": config.get('holdout_split'),\n",
    "            \"pos_tfidf_threshold\": config.get('pos_tfidf_threshold'),\n",
    "            \"neg_tfidf_threshold\": config.get('neg_tfidf_threshold'),\n",
    "            \"pair_ratios\": {\n",
    "                \"positives\": config.get('pos_ratio', 0.30),\n",
    "                \"hard_negatives\": config.get('hard_neg_ratio', 0.45),\n",
    "                \"easy_negatives\": config.get('easy_neg_ratio', 0.25)\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Data statistics\n",
    "        \"data_statistics\": data_stats,\n",
    "        \n",
    "        # TF-IDF configuration\n",
    "        \"tfidf_config\": {\n",
    "            \"max_features\": 15000,\n",
    "            \"ngram_range\": [1, 2],\n",
    "            \"min_df\": 2,\n",
    "            \"max_df\": 0.95\n",
    "        },\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        \"evaluation_metrics\": metrics,\n",
    "        \n",
    "        # V2: Cross-validated threshold\n",
    "        \"cv_threshold\": {\n",
    "            \"value\": float(cv_threshold) if 'cv_threshold' in dir() else None,\n",
    "            \"std\": float(cv_std) if 'cv_std' in dir() else None,\n",
    "            \"n_folds\": config.get('threshold_cv_folds', 5)\n",
    "        },\n",
    "        \n",
    "        # Adversarial diagnostic results\n",
    "        \"adversarial_diagnostic\": adversarial_results,\n",
    "        \n",
    "        # V2: Error analysis summary\n",
    "        \"error_analysis\": error_analysis,\n",
    "        \n",
    "        # Environment info\n",
    "        \"environment\": {\n",
    "            \"device\": DEVICE,\n",
    "            \"random_seed\": config.get('seed'),\n",
    "            \"python_version\": __import__('sys').version,\n",
    "            \"torch_version\": torch.__version__,\n",
    "            \"transformers_version\": __import__('transformers').__version__,\n",
    "            \"sentence_transformers_version\": __import__('sentence_transformers').__version__\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(output_dir, \"training_metadata.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    log(f\"üìù Training metadata saved to: {metadata_path}\")\n",
    "    return metadata_path\n",
    "\n",
    "# Collect data statistics\n",
    "data_stats = {\n",
    "    \"total_records\": len(df_incidents),\n",
    "    \"train_size\": len(train_df),\n",
    "    \"eval_size\": len(eval_df),\n",
    "    \"holdout_size\": len(holdout_df),\n",
    "    \"unique_categories\": df_incidents['Category'].nunique() if 'Category' in df_incidents.columns else None,\n",
    "    \"unique_subcategories\": df_incidents['Subcategory'].nunique() if 'Subcategory' in df_incidents.columns else None,\n",
    "    \"unique_assignment_groups\": df_incidents['Assignment group'].nunique() if 'Assignment group' in df_incidents.columns else None,\n",
    "    \"avg_text_length\": df_incidents['text'].str.len().mean(),\n",
    "    \"tfidf_vocabulary_size\": None\n",
    "}\n",
    "\n",
    "# Collect metrics\n",
    "try:\n",
    "    eval_metrics = {\n",
    "        \"eval_roc_auc\": float(eval_results['roc_auc']),\n",
    "        \"eval_pr_auc\": float(eval_results['pr_auc']),\n",
    "        \"eval_spearman\": float(eval_results['spearman']),\n",
    "        \"eval_pearson\": float(eval_results['pearson']),\n",
    "        \"eval_f1\": float(eval_results['f1']),\n",
    "        \"holdout_roc_auc\": float(holdout_results['roc_auc']),\n",
    "        \"holdout_pr_auc\": float(holdout_results['pr_auc']),\n",
    "        \"holdout_spearman\": float(holdout_results['spearman']),\n",
    "        \"holdout_pearson\": float(holdout_results['pearson']),\n",
    "        \"holdout_f1\": float(holdout_results['f1']),\n",
    "        # V2: Borderline metrics\n",
    "        \"borderline_roc_auc\": float(borderline_results['roc_auc']),\n",
    "        \"borderline_pr_auc\": float(borderline_results['pr_auc']),\n",
    "        \"borderline_f1\": float(borderline_results['f1']),\n",
    "    }\n",
    "except:\n",
    "    eval_metrics = {\"note\": \"Run evaluation cells first\"}\n",
    "\n",
    "# Collect adversarial results\n",
    "try:\n",
    "    adversarial_results_dict = {\n",
    "        \"roc_auc\": float(adv_results['roc_auc']) if 'adv_results' in dir() else None,\n",
    "        \"f1_score\": float(adv_results['f1']) if 'adv_results' in dir() else None,\n",
    "        \"pass_status\": DIAGNOSTIC_PASSED if 'DIAGNOSTIC_PASSED' in dir() else None\n",
    "    }\n",
    "except:\n",
    "    adversarial_results_dict = {\"note\": \"Run adversarial diagnostic first\"}\n",
    "\n",
    "# V2: Collect error analysis summary\n",
    "try:\n",
    "    error_analysis_dict = {\n",
    "        \"eval_fp_count\": eval_errors['fp_count'],\n",
    "        \"eval_fn_count\": eval_errors['fn_count'],\n",
    "        \"holdout_fp_count\": holdout_errors['fp_count'],\n",
    "        \"holdout_fn_count\": holdout_errors['fn_count'],\n",
    "        \"borderline_fp_count\": borderline_errors['fp_count'],\n",
    "        \"borderline_fn_count\": borderline_errors['fn_count'],\n",
    "    }\n",
    "except:\n",
    "    error_analysis_dict = {\"note\": \"Run error analysis first\"}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = save_training_metadata(\n",
    "    output_dir=str(save_path),\n",
    "    config=CONFIG,\n",
    "    metrics=eval_metrics,\n",
    "    data_stats=data_stats,\n",
    "    adversarial_results=adversarial_results_dict,\n",
    "    error_analysis=error_analysis_dict\n",
    ")\n",
    "\n",
    "log(\"[OK] Training pipeline V2 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456cf8db",
   "metadata": {},
   "source": [
    "## 13. Usage Examples & Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c0212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# USAGE EXAMPLES & V2 SUMMARY\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def demonstrate_model_usage():\n",
    "    \"\"\"\n",
    "    Demonstrate how to use the fine-tuned model for ITSM ticket similarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Try to use best_model if available (just trained), otherwise load from disk\n",
    "    if 'best_model' in globals():\n",
    "        model = globals()['best_model']\n",
    "        model_source = f\"memory (just trained: {globals()['save_path']})\"\n",
    "    else:\n",
    "        # Get output directory\n",
    "        if 'CONFIG' in globals():\n",
    "            output_dir = globals()['CONFIG']['output_dir']\n",
    "        else:\n",
    "            output_dir = 'models/real_servicenow_finetuned_v2'\n",
    "            log(\"[WARN] CONFIG not loaded. Using default model directory.\")\n",
    "        \n",
    "        # Look for most recent trained model\n",
    "        base_dir = Path(output_dir)\n",
    "        if base_dir.exists():\n",
    "            model_dirs = [d for d in base_dir.iterdir() if d.is_dir() and d.name.startswith('real_servicenow_')]\n",
    "            if model_dirs:\n",
    "                latest_model = max(model_dirs, key=lambda d: d.stat().st_mtime)\n",
    "                log(f\"üìÇ Loading most recent trained model from: {latest_model}\")\n",
    "                model = SentenceTransformer(str(latest_model))\n",
    "                model_source = str(latest_model)\n",
    "            else:\n",
    "                log(\"[ERROR] No trained model found. Please run training cells first.\")\n",
    "                return None, None\n",
    "        else:\n",
    "            log(\"[ERROR] No trained model found. Please run training cells first.\")\n",
    "            return None, None\n",
    "    \n",
    "    log(f\"[INSTALL] Using model from: {model_source}\")\n",
    "    \n",
    "    # Example tickets\n",
    "    example_tickets = [\n",
    "        \"User cannot login to SAP system. Error message: authentication failed. Tried resetting password but issue persists.\",\n",
    "        \"SAP login issue - getting access denied error when trying to connect to production system.\",\n",
    "        \"Outlook keeps crashing when opening large attachments. Have tried restarting but problem continues.\",\n",
    "        \"Email client crashes randomly. Users report Outlook freezing when opening emails with attachments.\",\n",
    "        \"Request to provision new laptop for incoming employee starting next Monday.\",\n",
    "    ]\n",
    "    \n",
    "    # Encode all tickets\n",
    "    embeddings = model.encode(example_tickets, show_progress_bar=False)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Display results\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"SIMILARITY MATRIX DEMO\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nTickets:\")\n",
    "    for i, ticket in enumerate(example_tickets):\n",
    "        print(f\"  [{i}] {ticket[:80]}...\")\n",
    "    \n",
    "    print(\"\\nSimilarity Matrix:\")\n",
    "    print(\"     \", end=\"\")\n",
    "    for i in range(len(example_tickets)):\n",
    "        print(f\"  [{i}]  \", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for i, row in enumerate(sim_matrix):\n",
    "        print(f\"[{i}]  \", end=\"\")\n",
    "        for val in row:\n",
    "            print(f\" {val:.3f} \", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Find similar ticket pairs\n",
    "    log(\"\\n\" + \"-\"*60)\n",
    "    log(\"HIGH SIMILARITY PAIRS (> 0.7):\")\n",
    "    for i in range(len(example_tickets)):\n",
    "        for j in range(i+1, len(example_tickets)):\n",
    "            if sim_matrix[i][j] > 0.7:\n",
    "                print(f\"  Tickets [{i}] & [{j}]: {sim_matrix[i][j]:.3f}\")\n",
    "                print(f\"    [{i}]: {example_tickets[i][:60]}...\")\n",
    "                print(f\"    [{j}]: {example_tickets[j][:60]}...\")\n",
    "                print()\n",
    "    \n",
    "    return model, embeddings\n",
    "\n",
    "# Run demonstration\n",
    "demo_model, demo_embeddings = demonstrate_model_usage()\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# V2 IMPROVEMENT SUMMARY\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"[STATS] V2 IMPROVEMENT SUMMARY\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "üÜï V2 ENHANCEMENTS IMPLEMENTED:\n",
    "\n",
    "1. HARDER NEGATIVE MINING\n",
    "   ‚îú‚îÄ Increased hard_neg_ratio: 35% ‚Üí 45%\n",
    "   ‚îú‚îÄ Stricter neg_tfidf_threshold: 0.20 ‚Üí 0.12\n",
    "   ‚îî‚îÄ Reduced easy negatives: 30% ‚Üí 25%\n",
    "\n",
    "2. CURRICULUM LEARNING\n",
    "   ‚îú‚îÄ Phase 1 (Epochs 1-2): Easier pairs (25% hard neg, threshold 0.15)\n",
    "   ‚îî‚îÄ Phase 2 (Epochs 3-4): Harder pairs (55% hard neg, threshold 0.10)\n",
    "\n",
    "3. BORDERLINE TEST SET\n",
    "   ‚îú‚îÄ TF-IDF range: 0.25 - 0.35 (ambiguous cases)\n",
    "   ‚îî‚îÄ Tests model on genuinely difficult pairs\n",
    "\n",
    "4. ERROR ANALYSIS\n",
    "   ‚îú‚îÄ Identifies worst false positives/negatives\n",
    "   ‚îú‚îÄ Analyzes text length patterns\n",
    "   ‚îî‚îÄ Provides systematic failure insights\n",
    "\n",
    "5. CROSS-VALIDATED THRESHOLD\n",
    "   ‚îú‚îÄ 5-fold CV for robust threshold selection\n",
    "   ‚îî‚îÄ Reports threshold variance across folds\n",
    "\n",
    "NEXT STEPS FOR FURTHER IMPROVEMENT:\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "üìà BASED ON RESULTS, CONSIDER:\n",
    "\n",
    "If borderline ROC-AUC < 0.80:\n",
    "   ‚Üí Increase curriculum phases to 3 (add intermediate difficulty)\n",
    "   ‚Üí Add triplet loss fine-tuning stage\n",
    "\n",
    "If many false negatives on short texts:\n",
    "   ‚Üí Reduce min_text_length filter\n",
    "   ‚Üí Add text augmentation (paraphrasing)\n",
    "\n",
    "If many false positives on same-category pairs:\n",
    "   ‚Üí Increase hard_neg_ratio further (45% ‚Üí 55%)\n",
    "   ‚Üí Remove category context from training text\n",
    "\n",
    "PRODUCTION DEPLOYMENT:\n",
    "   ‚Üí Model path: {CONFIG['output_dir']}\n",
    "   ‚Üí Use CV threshold: {cv_threshold:.4f} (¬± {cv_std:.4f})\n",
    "   ‚Üí Pre-compute embeddings for ticket corpus\n",
    "\"\"\")\n",
    "\n",
    "log(\"[OK] V2 training and evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
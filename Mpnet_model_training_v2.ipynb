{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPNet Dual-Objective Training Pipeline\n",
    "## Similar Ticket Detection + Classification\n",
    "\n",
    "**Project:** ITSM Incident Management AI  \n",
    "**Model:** all-mpnet-base-v2 (sentence-transformers)  \n",
    "**Date:** December 2025\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Training Objectives\n",
    "\n",
    "### PRIMARY: Similar Ticket Detection\n",
    "Train model to produce embeddings where similar tickets are **semantically close**.\n",
    "\n",
    "**Use Cases:**\n",
    "- üîç Find duplicate tickets\n",
    "- üîó Link related incidents for root cause analysis\n",
    "- üí° Suggest similar resolved tickets to speed up resolution\n",
    "- üìä Cluster tickets by problem type\n",
    "\n",
    "**Method:** Contrastive Learning\n",
    "- Positive pairs: Tickets from same category (similar)\n",
    "- Negative pairs: Tickets from different categories (dissimilar)\n",
    "- Loss: Contrastive loss with margin\n",
    "\n",
    "### SECONDARY: Ticket Classification\n",
    "Predict category labels for automatic routing.\n",
    "\n",
    "**Method:** Cross-Entropy Loss with classification head\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Pipeline Overview\n",
    "\n",
    "1. Data Loading & EDA\n",
    "2. Create Contrastive Pairs (similar/dissimilar tickets)\n",
    "3. Model Training with Dual Loss\n",
    "4. **Similarity Search Index Building**\n",
    "5. **Similar Ticket Retrieval Demo**\n",
    "6. Classification Evaluation\n",
    "7. Visualization & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\donov\\anaconda3\\envs\\itsm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data and ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Sentence transformers\n",
    "from sentence_transformers import SentenceTransformer, losses, InputExample\n",
    "\n",
    "# Metrics and utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Database (optional)\n",
    "try:\n",
    "    import psycopg2\n",
    "    from sqlalchemy import create_engine\n",
    "    DB_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    DB_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è psycopg2/sqlalchemy not available; DB loading will be skipped. Using sample data instead.\")\n",
    "    print(f\"Import error: {e}\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"‚úì All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model: sentence-transformers/all-mpnet-base-v2\n",
      "Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Model settings\n",
    "MODEL_NAME = 'sentence-transformers/all-mpnet-base-v2'\n",
    "MAX_SEQ_LENGTH = 512\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 10\n",
    "CONTRASTIVE_MARGIN = 0.5  # For contrastive loss\n",
    "\n",
    "# Directories - use relative paths that work on Windows and Linux\n",
    "OUTPUT_DIR = Path('models')\n",
    "MODEL_DIR = OUTPUT_DIR / 'mpnet_similarity_model'\n",
    "PLOTS_DIR = OUTPUT_DIR / 'plots'\n",
    "RESULTS_DIR = OUTPUT_DIR / 'results'\n",
    "\n",
    "for d in [MODEL_DIR, PLOTS_DIR, RESULTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loading real data from CSV: data_new\\SNow_incident_ticket_data.csv\n",
      "‚úì Loaded 10633 rows from CSV\n",
      "\n",
      "Dataset shape: (10633, 31)\n",
      "Categories: ['Application/Software' 'Network' nan 'Server' 'Hardware']\n",
      "‚úì Loaded 10633 rows from CSV\n",
      "\n",
      "Dataset shape: (10633, 31)\n",
      "Categories: ['Application/Software' 'Network' nan 'Server' 'Hardware']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>description</th>\n",
       "      <th>opened_by</th>\n",
       "      <th>company</th>\n",
       "      <th>itsm_department</th>\n",
       "      <th>created</th>\n",
       "      <th>urgency</th>\n",
       "      <th>impact</th>\n",
       "      <th>priority</th>\n",
       "      <th>assignment_group</th>\n",
       "      <th>...</th>\n",
       "      <th>comments_and_work_notes</th>\n",
       "      <th>manday_effort_(hrs)</th>\n",
       "      <th>ticket_type</th>\n",
       "      <th>ams_domain</th>\n",
       "      <th>ams_system_type</th>\n",
       "      <th>ams_category_type</th>\n",
       "      <th>ams_service_type</th>\n",
       "      <th>ams_business_related</th>\n",
       "      <th>ams_it_related</th>\n",
       "      <th>ticket_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INC0010171</td>\n",
       "      <td>GRPT not working as expected. ZMMM_PO_REV is n...</td>\n",
       "      <td>Indah Humairah Sulaiman</td>\n",
       "      <td>PIDSAP</td>\n",
       "      <td>PIDSAP</td>\n",
       "      <td>18/3/24 9:07</td>\n",
       "      <td>2 - Medium</td>\n",
       "      <td>3 - Low</td>\n",
       "      <td>4 - Low</td>\n",
       "      <td>PISCAP L2 SD BRS</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-04-11 13:26:58 - BALAKUMAR GANESAN (Addit...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Issue</td>\n",
       "      <td>IS</td>\n",
       "      <td>S4HANA</td>\n",
       "      <td>Non-Genesis</td>\n",
       "      <td>Business-Related</td>\n",
       "      <td>BZ-B12-Master Data (Wrong Maintenance)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INC0010171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INC0010181</td>\n",
       "      <td>eTR-S1-24000073\\r\\nExchange Rate did not auto ...</td>\n",
       "      <td>Indah Humairah Sulaiman</td>\n",
       "      <td>PIDSAP</td>\n",
       "      <td>PIDSAP</td>\n",
       "      <td>18/3/24 9:51</td>\n",
       "      <td>2 - Medium</td>\n",
       "      <td>3 - Low</td>\n",
       "      <td>4 - Low</td>\n",
       "      <td>PISCAP L2 Workflow (SN)</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-04-05 02:49:36 - Reeman Mathur (Additiona...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INC0010181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INC0010188</td>\n",
       "      <td>There is no GRPT maintenance for Sold-To: 3901...</td>\n",
       "      <td>Indah Humairah Sulaiman</td>\n",
       "      <td>PIDSAP</td>\n",
       "      <td>PIDSAP</td>\n",
       "      <td>18/3/24 10:19</td>\n",
       "      <td>3 - Low</td>\n",
       "      <td>3 - Low</td>\n",
       "      <td>4 - Low</td>\n",
       "      <td>PISCAP L2 SD BRS</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-05-13 12:57:15 - BALAKUMAR GANESAN (Addit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INC0010188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INC0010189</td>\n",
       "      <td>Interface\\t                fpl\\r\\nSubsidiary\\t...</td>\n",
       "      <td>Chenxing Cao</td>\n",
       "      <td>PA</td>\n",
       "      <td>PISCAP</td>\n",
       "      <td>18/3/24 10:24</td>\n",
       "      <td>3 - Low</td>\n",
       "      <td>3 - Low</td>\n",
       "      <td>4 - Low</td>\n",
       "      <td>PISCAP L2 Mulesoft/SOA</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-03-18 10:30:07 - Chenxing Cao (Work notes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INC0010189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INC0010192</td>\n",
       "      <td>retrieve new SAP password, thank you.</td>\n",
       "      <td>SOOK FONG NG</td>\n",
       "      <td>PM</td>\n",
       "      <td>PM</td>\n",
       "      <td>18/3/24 10:33</td>\n",
       "      <td>1 - High</td>\n",
       "      <td>3 - Low</td>\n",
       "      <td>3 - Moderate</td>\n",
       "      <td>PISCAP L2 SAP BASIS</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INC0010192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       number                                        description  \\\n",
       "0  INC0010171  GRPT not working as expected. ZMMM_PO_REV is n...   \n",
       "1  INC0010181  eTR-S1-24000073\\r\\nExchange Rate did not auto ...   \n",
       "2  INC0010188  There is no GRPT maintenance for Sold-To: 3901...   \n",
       "3  INC0010189  Interface\\t                fpl\\r\\nSubsidiary\\t...   \n",
       "4  INC0010192              retrieve new SAP password, thank you.   \n",
       "\n",
       "                 opened_by company itsm_department        created     urgency  \\\n",
       "0  Indah Humairah Sulaiman  PIDSAP          PIDSAP   18/3/24 9:07  2 - Medium   \n",
       "1  Indah Humairah Sulaiman  PIDSAP          PIDSAP   18/3/24 9:51  2 - Medium   \n",
       "2  Indah Humairah Sulaiman  PIDSAP          PIDSAP  18/3/24 10:19     3 - Low   \n",
       "3             Chenxing Cao      PA          PISCAP  18/3/24 10:24     3 - Low   \n",
       "4             SOOK FONG NG      PM              PM  18/3/24 10:33    1 - High   \n",
       "\n",
       "    impact      priority         assignment_group  ...  \\\n",
       "0  3 - Low       4 - Low         PISCAP L2 SD BRS  ...   \n",
       "1  3 - Low       4 - Low  PISCAP L2 Workflow (SN)  ...   \n",
       "2  3 - Low       4 - Low         PISCAP L2 SD BRS  ...   \n",
       "3  3 - Low       4 - Low   PISCAP L2 Mulesoft/SOA  ...   \n",
       "4  3 - Low  3 - Moderate      PISCAP L2 SAP BASIS  ...   \n",
       "\n",
       "                             comments_and_work_notes manday_effort_(hrs)  \\\n",
       "0  2025-04-11 13:26:58 - BALAKUMAR GANESAN (Addit...                 3.0   \n",
       "1  2024-04-05 02:49:36 - Reeman Mathur (Additiona...                 NaN   \n",
       "2  2024-05-13 12:57:15 - BALAKUMAR GANESAN (Addit...                 NaN   \n",
       "3  2024-03-18 10:30:07 - Chenxing Cao (Work notes...                 NaN   \n",
       "4                                                NaN                 NaN   \n",
       "\n",
       "  ticket_type ams_domain ams_system_type ams_category_type  ams_service_type  \\\n",
       "0       Issue         IS          S4HANA       Non-Genesis  Business-Related   \n",
       "1       Issue        NaN             NaN               NaN               NaN   \n",
       "2       Issue        NaN             NaN               NaN               NaN   \n",
       "3       Issue        NaN             NaN               NaN               NaN   \n",
       "4       Issue        NaN             NaN               NaN               NaN   \n",
       "\n",
       "                     ams_business_related ams_it_related   ticket_id  \n",
       "0  BZ-B12-Master Data (Wrong Maintenance)            NaN  INC0010171  \n",
       "1                                     NaN            NaN  INC0010181  \n",
       "2                                     NaN            NaN  INC0010188  \n",
       "3                                     NaN            NaN  INC0010189  \n",
       "4                                     NaN            NaN  INC0010192  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATA_CSV_PATH = Path('data_new/SNow_incident_ticket_data.csv')\n",
    "\n",
    "def load_or_create_sample_data():\n",
    "    \"\"\"Load from CSV; fallback to sample data if CSV missing.\"\"\"\n",
    "    if DATA_CSV_PATH.exists():\n",
    "        print(f\"‚úì Loading real data from CSV: {DATA_CSV_PATH}\")\n",
    "        df = pd.read_csv(DATA_CSV_PATH)\n",
    "        \n",
    "        # Normalize column names: lowercase and replace spaces with underscores\n",
    "        df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "        \n",
    "        # Basic required columns check\n",
    "        required = ['description', 'category']\n",
    "        missing = [c for c in required if c not in df.columns]\n",
    "        if missing:\n",
    "            print(f\"Available columns: {df.columns.tolist()}\")\n",
    "            raise ValueError(f\"CSV missing required columns: {missing}\")\n",
    "        \n",
    "        # Minimal cleanup for expected columns\n",
    "        if 'number' in df.columns and 'ticket_id' not in df.columns:\n",
    "            df['ticket_id'] = df['number']\n",
    "        elif 'ticket_id' not in df.columns:\n",
    "            df['ticket_id'] = [f\"INC{i:06d}\" for i in range(len(df))]\n",
    "        \n",
    "        if 'service_offering' not in df.columns:\n",
    "            df['service_offering'] = ''\n",
    "        \n",
    "        if 'subcategory' not in df.columns:\n",
    "            df['subcategory'] = df['category']\n",
    "        \n",
    "        if 'assignment_group' not in df.columns:\n",
    "            df['assignment_group'] = ''\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(df)} rows from CSV\")\n",
    "        return df\n",
    "    \n",
    "    # Fallback: synthetic sample data\n",
    "    print(f\"‚ö†Ô∏è CSV not found at {DATA_CSV_PATH.resolve()}; using sample data instead.\")\n",
    "    categories = ['Access Issues', 'Hardware Issue', 'Password Reset', \n",
    "                 'Software Issue', 'Network Issue']\n",
    "    samples = []\n",
    "    descriptions = {\n",
    "        'Access Issues': [\n",
    "            \"Cannot access email account. Login fails with authentication error.\",\n",
    "            \"User locked out of system after failed login attempts.\",\n",
    "            \"Access denied to shared folder on network drive.\",\n",
    "        ],\n",
    "        'Hardware Issue': [\n",
    "            \"Printer not responding. Paper jam error on printer HP4500.\",\n",
    "            \"Monitor display flickering. Screen shows vertical lines.\",\n",
    "            \"Keyboard keys not working properly. Multiple keys stuck.\",\n",
    "        ],\n",
    "        'Password Reset': [\n",
    "            \"Need password reset for SAP system. Locked out after failed attempts.\",\n",
    "            \"Forgot password for email account. Cannot log in.\",\n",
    "            \"Password expired for VPN access. Need immediate reset.\",\n",
    "        ],\n",
    "        'Software Issue': [\n",
    "            \"Application crashes when opening reports. Error code 0x8007000E.\",\n",
    "            \"Software freezes during data export. Have to force quit.\",\n",
    "            \"Cannot install software update. Error message appears.\",\n",
    "        ],\n",
    "        'Network Issue': [\n",
    "            \"Network connection keeps dropping. Unable to access shared drives.\",\n",
    "            \"Internet connectivity issues. Cannot browse websites.\",\n",
    "            \"VPN connection fails. Cannot connect to office network.\",\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category in categories:\n",
    "        for desc in descriptions[category] * 67:  # ~1000 samples\n",
    "            samples.append({\n",
    "                'description': desc,\n",
    "                'category': category,\n",
    "                'service_offering': category.split()[0] + ' Services'\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(samples)\n",
    "    df['ticket_id'] = [f'INC{i:06d}' for i in range(len(df))]\n",
    "    df['subcategory'] = df['category']\n",
    "    df['assignment_group'] = 'Support Team'\n",
    "    \n",
    "    print(f\"‚úì Created {len(df)} sample tickets\")\n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df = load_or_create_sample_data()\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Categories: {df['category'].unique()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: 10633 rows, 563 rows with missing category\n",
      "After preprocessing: 9808 tickets\n",
      "Categories: 4\n",
      "Category list: ['Application/Software', 'Hardware', 'Network', 'Server']\n",
      "After preprocessing: 9808 tickets\n",
      "Categories: 4\n",
      "Category list: ['Application/Software', 'Hardware', 'Network', 'Server']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)  # Mask emails\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing\n",
    "df['description_clean'] = df['description'].apply(preprocess_text)\n",
    "\n",
    "# Remove rows with NaN or missing categories\n",
    "print(f\"Before cleaning: {len(df)} rows, {df['category'].isna().sum()} rows with missing category\")\n",
    "df = df.dropna(subset=['category', 'description']).copy()\n",
    "df = df[df['description_clean'].str.split().str.len() >= 5].copy()\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['category_id'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "print(f\"After preprocessing: {len(df)} tickets\")\n",
    "print(f\"Categories: {len(label_encoder.classes_)}\")\n",
    "print(f\"Category list: {label_encoder.classes_.tolist()}\")\n",
    "\n",
    "# Save label encoder\n",
    "with open(MODEL_DIR / 'label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    6864 (70.0%)\n",
      "Validation:  1472 (15.0%)\n",
      "Test:        1472 (15.0%)\n"
     ]
    }
   ],
   "source": [
    "# Stratified split\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df, test_size=0.15, stratify=df['category_id'], random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df, test_size=0.15/(1-0.15), stratify=train_val_df['category_id'], \n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Training:   {len(train_df):5d} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(val_df):5d} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test:       {len(test_df):5d} ({len(test_df)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Contrastive Pairs for Similarity Learning\n",
    "\n",
    "**This is the KEY step for learning good embeddings!**\n",
    "\n",
    "We create pairs of tickets:\n",
    "- **Positive pairs (label=1):** Two tickets from SAME category ‚Üí should be close in embedding space\n",
    "- **Negative pairs (label=0):** Two tickets from DIFFERENT categories ‚Üí should be far apart\n",
    "\n",
    "The model learns to minimize distance for positive pairs and maximize distance for negative pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating contrastive pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating pairs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6864/6864 [00:29<00:00, 231.00it/s]\n",
      "Creating pairs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6864/6864 [00:29<00:00, 231.00it/s]\n",
      "Creating pairs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1472/1472 [00:02<00:00, 604.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contrastive Pairs Created:\n",
      "  Training: 27456 pairs\n",
      "  - Positive (similar): 13728\n",
      "  - Negative (dissimilar): 13728\n",
      "  Validation: 2943 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_contrastive_pairs(df, num_pairs_per_ticket=2):\n",
    "    \"\"\"\n",
    "    Create contrastive pairs for training.\n",
    "    \n",
    "    Returns:\n",
    "        list of (text1, text2, label) tuples\n",
    "        label=1 for similar, 0 for dissimilar\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    grouped = df.groupby('category_id')\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Creating pairs\"):\n",
    "        text1 = row['description_clean']\n",
    "        category = row['category_id']\n",
    "        \n",
    "        # Positive pairs: same category\n",
    "        same_cat = grouped.get_group(category)\n",
    "        same_cat = same_cat[same_cat.index != idx]\n",
    "        \n",
    "        if len(same_cat) > 0:\n",
    "            n_pos = min(num_pairs_per_ticket, len(same_cat))\n",
    "            pos_samples = same_cat.sample(n=n_pos, random_state=RANDOM_SEED)\n",
    "            for _, pos_row in pos_samples.iterrows():\n",
    "                pairs.append((text1, pos_row['description_clean'], 1))\n",
    "        \n",
    "        # Negative pairs: different category  \n",
    "        diff_cat = df[df['category_id'] != category]\n",
    "        if len(diff_cat) > 0:\n",
    "            n_neg = min(num_pairs_per_ticket, len(diff_cat))\n",
    "            neg_samples = diff_cat.sample(n=n_neg, random_state=RANDOM_SEED)\n",
    "            for _, neg_row in neg_samples.iterrows():\n",
    "                pairs.append((text1, neg_row['description_clean'], 0))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Create pairs\n",
    "print(\"Creating contrastive pairs...\")\n",
    "train_pairs = create_contrastive_pairs(train_df, num_pairs_per_ticket=2)\n",
    "val_pairs = create_contrastive_pairs(val_df, num_pairs_per_ticket=1)\n",
    "\n",
    "print(f\"\\nContrastive Pairs Created:\")\n",
    "print(f\"  Training: {len(train_pairs)} pairs\")\n",
    "print(f\"  - Positive (similar): {sum(1 for p in train_pairs if p[2]==1)}\")\n",
    "print(f\"  - Negative (dissimilar): {sum(1 for p in train_pairs if p[2]==0)}\")\n",
    "print(f\"  Validation: {len(val_pairs)} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training with Contrastive Loss\n",
    "\n",
    "We use Sentence-Transformers library which handles:\n",
    "- MPNet encoding\n",
    "- Contrastive loss computation  \n",
    "- Efficient batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded model: sentence-transformers/all-mpnet-base-v2\n",
      "\n",
      "Training Setup:\n",
      "  - Loss: OnlineContrastiveLoss (margin=0.5)\n",
      "  - Batch size: 16\n",
      "  - Batches per epoch: 1716\n",
      "  - Total training steps: 17160\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load pre-trained model\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "print(f\"‚úì Loaded model: {MODEL_NAME}\")\n",
    "\n",
    "# Convert pairs to InputExample format\n",
    "train_examples = [\n",
    "    InputExample(texts=[text1, text2], label=float(label))\n",
    "    for text1, text2, label in train_pairs\n",
    "]\n",
    "\n",
    "val_examples = [\n",
    "    InputExample(texts=[text1, text2], label=float(label))\n",
    "    for text1, text2, label in val_pairs\n",
    "]\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Use OnlineContrastiveLoss - perfect for our use case!\n",
    "train_loss = losses.OnlineContrastiveLoss(\n",
    "    model=model,\n",
    "    margin=CONTRASTIVE_MARGIN\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining Setup:\")\n",
    "print(f\"  - Loss: OnlineContrastiveLoss (margin={CONTRASTIVE_MARGIN})\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Batches per epoch: {len(train_dataloader)}\")\n",
    "print(f\"  - Total training steps: {len(train_dataloader) * NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Training\n",
      "============================================================\n",
      "‚úì GPU cache cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='342' max='17160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  342/17160 5:34:13 < 275:32:24, 0.02 it/s, Epoch 0.20/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úì GPU cache cleared\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMODEL_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmpnet_similarity_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úì Training complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_DIR\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmpnet_similarity_model\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\donov\\anaconda3\\envs\\itsm\\Lib\\site-packages\\sentence_transformers\\fit_mixin.py:408\u001b[39m, in \u001b[36mFitMixin.fit\u001b[39m\u001b[34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit, resume_from_checkpoint)\u001b[39m\n\u001b[32m    405\u001b[39m         logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheckpoint directory does not exist or is not a directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    406\u001b[39m         resume_from_checkpoint = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\donov\\anaconda3\\envs\\itsm\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\donov\\anaconda3\\envs\\itsm\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\donov\\anaconda3\\envs\\itsm\\Lib\\site-packages\\transformers\\trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\donov\\anaconda3\\envs\\itsm\\Lib\\site-packages\\accelerate\\accelerator.py:2852\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2850\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2851\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2852\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\donov\\anaconda3\\envs\\itsm\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\donov\\anaconda3\\envs\\itsm\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\donov\\anaconda3\\envs\\itsm\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train model with GPU memory management\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úì GPU cache cleared\")\n",
    "\n",
    "try:\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=NUM_EPOCHS,\n",
    "        warmup_steps=100,\n",
    "        output_path=str(MODEL_DIR / 'mpnet_similarity_model'),\n",
    "        show_progress_bar=True,\n",
    "        save_best_model=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úì Training complete!\")\n",
    "    print(f\"Model saved to: {MODEL_DIR / 'mpnet_similarity_model'}\")\n",
    "except RuntimeError as e:\n",
    "    if 'CUDA' in str(e) or 'out of memory' in str(e):\n",
    "        print(f\"‚ö†Ô∏è GPU Memory error: {e}\")\n",
    "        print(\"Try reducing BATCH_SIZE or using CPU by setting device='cpu'\")\n",
    "        raise\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Similarity Search Index\n",
    "\n",
    "Now we encode ALL training tickets to create a searchable index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building similarity search index...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get all training tickets\n",
    "all_train_texts = train_df['description_clean'].tolist()\n",
    "all_train_ids = train_df['ticket_id'].tolist()\n",
    "all_train_categories = train_df['category'].tolist()\n",
    "\n",
    "print(f\"Encoding {len(all_train_texts)} tickets...\")\n",
    "train_embeddings = model.encode(\n",
    "    all_train_texts,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=False,\n",
    "    normalize_embeddings=True  # Normalize for cosine similarity\n",
    ")\n",
    "\n",
    "# Create searchable index\n",
    "embedding_index = {\n",
    "    'embeddings': train_embeddings,\n",
    "    'ticket_ids': all_train_ids,\n",
    "    'texts': all_train_texts,\n",
    "    'categories': all_train_categories\n",
    "}\n",
    "\n",
    "# Save index\n",
    "with open(MODEL_DIR / 'embedding_index.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_index, f)\n",
    "\n",
    "print(f\"\\n‚úì Index created!\")\n",
    "print(f\"  - Size: {len(all_train_texts)} tickets\")\n",
    "print(f\"  - Dimensions: {train_embeddings.shape}\")\n",
    "print(f\"  - Saved to: {MODEL_DIR / 'embedding_index.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üîç Similar Ticket Search Demo\n",
    "\n",
    "**This is what we trained for!** Let's find similar tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_tickets(query_text, embedding_index, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Find most similar tickets to query.\n",
    "    \n",
    "    Returns:\n",
    "        list of dicts with ticket info and similarity scores\n",
    "    \"\"\"\n",
    "    if not query_text or len(query_text.strip()) == 0:\n",
    "        print(\"‚ö†Ô∏è Empty query text provided\")\n",
    "        return []\n",
    "    \n",
    "    # Encode query\n",
    "    query_emb = model.encode(\n",
    "        [query_text],\n",
    "        convert_to_tensor=False,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(query_emb, embedding_index['embeddings'])[0]\n",
    "    \n",
    "    # Get top-k\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'ticket_id': embedding_index['ticket_ids'][idx],\n",
    "            'description': embedding_index['texts'][idx],\n",
    "            'category': embedding_index['categories'][idx],\n",
    "            'similarity': float(similarities[idx])\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"SIMILAR TICKET SEARCH DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try examples from test set (with safety checks)\n",
    "if len(test_df) == 0:\n",
    "    print(\"‚ö†Ô∏è Test set is empty - skipping demo\")\n",
    "else:\n",
    "    num_demos = min(3, len(test_df))\n",
    "    for i in range(num_demos):\n",
    "        query = test_df.iloc[i]\n",
    "        print(f\"\\n{'‚îÄ'*60}\")\n",
    "        print(f\"QUERY TICKET #{i+1}: {query['ticket_id']}\")\n",
    "        print(f\"Category: {query['category']}\")\n",
    "        print(f\"Description: {query['description_clean'][:100]}...\")\n",
    "        print(f\"\\nTop 5 Similar Tickets:\")\n",
    "        print(f\"{'‚îÄ'*60}\")\n",
    "        \n",
    "        similar = find_similar_tickets(\n",
    "            query['description_clean'], \n",
    "            embedding_index, \n",
    "            model, \n",
    "            top_k=5\n",
    "        )\n",
    "        \n",
    "        if not similar:\n",
    "            print(\"‚ö†Ô∏è No similar tickets found\")\n",
    "            continue\n",
    "        \n",
    "        for rank, ticket in enumerate(similar, 1):\n",
    "            match = \"‚úì\" if ticket['category'] == query['category'] else \"‚úó\"\n",
    "            print(f\"\\n{rank}. {ticket['ticket_id']} | Similarity: {ticket['similarity']:.4f} {match}\")\n",
    "            print(f\"   Category: {ticket['category']}\")\n",
    "            print(f\"   Description: {ticket['description'][:80]}...\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Similarity Retrieval Quality\n",
    "\n",
    "Measure how well the model finds similar tickets using:\n",
    "- **Mean Reciprocal Rank (MRR):** Average rank of first relevant result\n",
    "- **Precision@K:** % of relevant results in top-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_similarity_retrieval(test_df, embedding_index, model, k=10):\n",
    "    \"\"\"Evaluate with MRR and Precision@K\"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    precisions = []\n",
    "    \n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating\"):\n",
    "        query_text = row['description_clean']\n",
    "        query_category = row['category']\n",
    "        \n",
    "        # Find similar\n",
    "        similar = find_similar_tickets(query_text, embedding_index, model, top_k=k)\n",
    "        \n",
    "        # Calculate MRR\n",
    "        first_correct_rank = None\n",
    "        for rank, ticket in enumerate(similar, 1):\n",
    "            if ticket['category'] == query_category:\n",
    "                first_correct_rank = rank\n",
    "                break\n",
    "        \n",
    "        if first_correct_rank:\n",
    "            reciprocal_ranks.append(1.0 / first_correct_rank)\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "        \n",
    "        # Calculate Precision@K\n",
    "        correct = sum(1 for t in similar if t['category'] == query_category)\n",
    "        precisions.append(correct / k)\n",
    "    \n",
    "    return np.mean(reciprocal_ranks), np.mean(precisions)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating similarity retrieval...\")\n",
    "mrr, precision_10 = evaluate_similarity_retrieval(test_df, embedding_index, model, k=10)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SIMILARITY RETRIEVAL RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
    "print(f\"Precision@10:               {precision_10:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if mrr > 0:\n",
    "    print(f\"  - First relevant result appears at rank ~{1/mrr:.1f} on average\")\n",
    "else:\n",
    "    print(f\"  - No relevant results found in top-10\")\n",
    "print(f\"  - {precision_10*100:.1f}% of top-10 results are relevant\")\n",
    "\n",
    "if mrr > 0.75:\n",
    "    print(f\"\\n‚úì EXCELLENT: MRR > 0.75 (Target achieved!)\")\n",
    "elif mrr > 0.60:\n",
    "    print(f\"\\n‚úì GOOD: MRR > 0.60 (Close to target)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† Needs improvement (Target: MRR > 0.75)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization: t-SNE Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for visualization\n",
    "sample_size = min(500, len(test_df))\n",
    "sample_texts = test_df['description_clean'].sample(sample_size, random_state=RANDOM_SEED)\n",
    "sample_labels = test_df.loc[sample_texts.index, 'category']\n",
    "\n",
    "print(f\"Computing embeddings for {sample_size} samples...\")\n",
    "embeddings = model.encode(sample_texts.tolist(), show_progress_bar=True)\n",
    "\n",
    "print(\"Computing t-SNE projection...\")\n",
    "tsne = TSNE(n_components=2, random_state=RANDOM_SEED, perplexity=30)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "unique_cats = sample_labels.unique()\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_cats)))\n",
    "\n",
    "for cat, color in zip(unique_cats, colors):\n",
    "    mask = sample_labels == cat\n",
    "    ax.scatter(\n",
    "        embeddings_2d[mask, 0],\n",
    "        embeddings_2d[mask, 1],\n",
    "        c=[color],\n",
    "        label=cat,\n",
    "        alpha=0.6,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('t-SNE Dimension 1', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('t-SNE Dimension 2', fontsize=12, fontweight='bold')\n",
    "ax.set_title('t-SNE: Ticket Embeddings by Category', fontsize=14, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'tsne_embeddings.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Saved to {PLOTS_DIR / 'tsne_embeddings.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Next Steps\n",
    "\n",
    "### ‚úÖ Completed:\n",
    "- [x] Loaded and preprocessed ticket data\n",
    "- [x] Created contrastive pairs for similarity learning\n",
    "- [x] Trained MPNet with contrastive loss\n",
    "- [x] Built searchable embedding index\n",
    "- [x] Demonstrated similar ticket search\n",
    "- [x] Evaluated retrieval quality (MRR, Precision@K)\n",
    "- [x] Visualized embedding space\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Deploy similarity search API** - Expose find_similar_tickets() as REST endpoint\n",
    "2. **Integrate with ServiceNow** - Auto-link similar tickets when created\n",
    "3. **Add classification head** - For category prediction (secondary objective)\n",
    "4. **Implement caching** - Store embeddings for fast retrieval\n",
    "5. **Monitor performance** - Track MRR in production\n",
    "6. **Fine-tune on feedback** - Retrain with user relevance signals\n",
    "\n",
    "### üíæ Saved Artifacts:\n",
    "- Model: `models/mpnet_similarity_model/`\n",
    "- Embedding index: `models/embedding_index.pkl`\n",
    "- Label encoder: `models/label_encoder.pkl`\n",
    "- Visualizations: `plots/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final metrics\n",
    "metrics = {\n",
    "    'mrr': float(mrr),\n",
    "    'precision_at_10': float(precision_10),\n",
    "    'num_train_tickets': len(train_df),\n",
    "    'num_test_tickets': len(test_df),\n",
    "    'num_categories': len(label_encoder.classes_),\n",
    "    'model_name': MODEL_NAME,\n",
    "    'training_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'similarity_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model saved to: {MODEL_DIR}\")\n",
    "print(f\"Results saved to: {RESULTS_DIR}\")\n",
    "print(f\"\\nKey Metrics:\")\n",
    "print(f\"  - MRR: {mrr:.4f}\")\n",
    "print(f\"  - Precision@10: {precision_10:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

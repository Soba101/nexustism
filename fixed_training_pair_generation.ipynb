{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f293b44",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c7d8285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f58f8db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "Configuration:\n",
      "  Batch size: 16\n",
      "  Positive pairs target: 5000\n",
      "  Negative pairs target: 5000\n",
      "  Positive similarity threshold: ‚â•0.35\n",
      "  Negative similarity threshold: <0.45\n",
      "\n",
      "Quality Requirements:\n",
      "  Minimum separability: 0.15\n",
      "  Maximum overlap: 10.0%\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    BATCH_SIZE = 64\n",
    "else:\n",
    "    BATCH_SIZE = 16\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('data_new')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Training pair parameters (ENHANCED for better quality)\n",
    "NUM_POSITIVE_PAIRS = 5000  # Increased from 3000 for more robust training\n",
    "NUM_NEGATIVE_PAIRS = 5000  # Increased from 3000 for more robust training\n",
    "POSITIVE_SIMILARITY_THRESHOLD = 0.35  # Increased from 0.3 for higher quality positives\n",
    "NEGATIVE_SIMILARITY_THRESHOLD = 0.45  # Decreased from 0.5 for clearer negatives\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Quality requirements\n",
    "MIN_SEPARABILITY = 0.15  # Minimum acceptable separability\n",
    "MAX_OVERLAP_PCT = 10.0  # Maximum acceptable overlap percentage\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Positive pairs target: {NUM_POSITIVE_PAIRS}\")\n",
    "print(f\"  Negative pairs target: {NUM_NEGATIVE_PAIRS}\")\n",
    "print(f\"  Positive similarity threshold: ‚â•{POSITIVE_SIMILARITY_THRESHOLD}\")\n",
    "print(f\"  Negative similarity threshold: <{NEGATIVE_SIMILARITY_THRESHOLD}\")\n",
    "print(f\"\\nQuality Requirements:\")\n",
    "print(f\"  Minimum separability: {MIN_SEPARABILITY}\")\n",
    "print(f\"  Maximum overlap: {MAX_OVERLAP_PCT}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e49201d",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcedc418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10633 ServiceNow incidents\n",
      "\n",
      "Columns: ['Number', 'Description', 'Opened by', 'Company', 'ITSM Department', 'Created', 'Urgency', 'Impact', 'Priority', 'Assignment group', 'Assigned to', 'State', 'Service', 'Service offering', 'Closed', 'Closed by', 'Category', 'Subcategory', 'Resolution code', 'Resolution notes', 'User input', 'Comments and Work notes', 'Manday Effort (hrs)', 'Ticket Type', 'AMS Domain', 'AMS System Type', 'AMS Category Type', 'AMS Service Type', 'AMS Business Related', 'AMS IT Related']\n"
     ]
    }
   ],
   "source": [
    "# Load ServiceNow incident data\n",
    "data_path = DATA_DIR / 'SNow_incident_ticket_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Loaded {len(df)} ServiceNow incidents\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61c93718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: 10633 valid incidents\n",
      "\n",
      "Sample text: INC0010171 GRPT not working as expected. ZMMM_PO_REV is not generating correct dates as per maintained in GRPT table. \n",
      "E.g. P/O# 100024066\n",
      "Vendor Ship mode is 03. \n",
      "As per GRPT route days are 12 day...\n"
     ]
    }
   ],
   "source": [
    "# Combine text fields\n",
    "def create_combined_text(row):\n",
    "    \"\"\"Combine available text fields with proper handling of NaN\"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    for col in ['Number', 'Description', 'User input', 'Resolution notes']:\n",
    "        if col in row.index:\n",
    "            value = str(row.get(col, '')).strip() if pd.notna(row.get(col)) else ''\n",
    "            if value and value.lower() != 'nan':\n",
    "                text_parts.append(value)\n",
    "    \n",
    "    return ' '.join(text_parts) if text_parts else ''\n",
    "\n",
    "df['combined_text'] = df.apply(create_combined_text, axis=1)\n",
    "df['combined_text'] = df['combined_text'].astype(str)\n",
    "df = df[df['combined_text'].str.len() > 10].reset_index(drop=True)\n",
    "\n",
    "print(f\"After filtering: {len(df)} valid incidents\")\n",
    "print(f\"\\nSample text: {df['combined_text'].iloc[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca46a35",
   "metadata": {},
   "source": [
    "## 3. Load Baseline Model for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "374fd75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline model: sentence-transformers/all-mpnet-base-v2\n",
      "‚úì Baseline model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load baseline model for semantic validation\n",
    "print(\"Loading baseline model: sentence-transformers/all-mpnet-base-v2\")\n",
    "baseline_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "print(\"‚úì Baseline model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda99d5",
   "metadata": {},
   "source": [
    "## 4. Test Set Consistency Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28dc415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing test pairs for consistency check...\n",
      "\n",
      "================================================================================\n",
      "TEST SET REFERENCE METRICS\n",
      "================================================================================\n",
      "Test pairs: 1000\n",
      "Test separability: 0.1865\n",
      "Test pos threshold: 0.3\n",
      "Test neg threshold: 0.5\n",
      "\n",
      "================================================================================\n",
      "TRAINING vs TEST CONFIGURATION\n",
      "================================================================================\n",
      "Positive threshold: Train=0.35 vs Test=0.3\n",
      "Negative threshold: Train=0.45 vs Test=0.5\n",
      "Target separability: Train ‚â•0.15 vs Test=0.1865\n",
      "\n",
      "‚úì Training and test quality targets are consistent\n"
     ]
    }
   ],
   "source": [
    "# Load existing test pairs to ensure training consistency\n",
    "test_pairs_path = DATA_DIR / 'fixed_test_pairs.json'\n",
    "\n",
    "if test_pairs_path.exists():\n",
    "    print(\"Loading existing test pairs for consistency check...\")\n",
    "    with open(test_pairs_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    test_metadata = test_data.get('metadata', {})\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TEST SET REFERENCE METRICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Test pairs: {test_metadata.get('num_pairs', 'N/A')}\")\n",
    "    print(f\"Test separability: {test_metadata.get('baseline_separability', 'N/A'):.4f}\")\n",
    "    print(f\"Test pos threshold: {test_metadata.get('positive_similarity_threshold', 'N/A')}\")\n",
    "    print(f\"Test neg threshold: {test_metadata.get('negative_similarity_threshold', 'N/A')}\")\n",
    "    \n",
    "    # Compare with current settings\n",
    "    test_sep = test_metadata.get('baseline_separability', 0)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING vs TEST CONFIGURATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Positive threshold: Train={POSITIVE_SIMILARITY_THRESHOLD} vs Test={test_metadata.get('positive_similarity_threshold', 'N/A')}\")\n",
    "    print(f\"Negative threshold: Train={NEGATIVE_SIMILARITY_THRESHOLD} vs Test={test_metadata.get('negative_similarity_threshold', 'N/A')}\")\n",
    "    print(f\"Target separability: Train ‚â•{MIN_SEPARABILITY} vs Test={test_sep:.4f}\")\n",
    "    \n",
    "    if test_sep > 0 and test_sep < MIN_SEPARABILITY:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Training target separability ({MIN_SEPARABILITY}) exceeds test separability ({test_sep:.4f})\")\n",
    "        print(f\"   Consider lowering MIN_SEPARABILITY to match test quality\")\n",
    "    elif test_sep > MIN_SEPARABILITY + 0.05:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Test separability ({test_sep:.4f}) significantly exceeds training minimum ({MIN_SEPARABILITY})\")\n",
    "        print(f\"   Consider raising MIN_SEPARABILITY to {test_sep:.4f} for consistency\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Training and test quality targets are consistent\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Test pairs file not found at {test_pairs_path}\")\n",
    "    print(\"Proceeding without consistency check...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee51fa",
   "metadata": {},
   "source": [
    "## 5. Generate Candidate Pairs (Category-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a8f01c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No category column found - using random pairs\n",
      "\n",
      "‚úì Generated 20000 candidate pairs\n",
      "  Positive: 10000\n",
      "  Negative: 10000\n"
     ]
    }
   ],
   "source": [
    "def generate_candidate_pairs(df: pd.DataFrame, \n",
    "                            num_positives: int,\n",
    "                            num_negatives: int,\n",
    "                            random_state: int = 42) -> Tuple[List[str], List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    Generate candidate pairs based on categories.\n",
    "    These will be filtered with semantic validation.\n",
    "    \n",
    "    Generate MORE than needed since filtering will remove noisy pairs.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    texts1, texts2, labels = [], [], []\n",
    "    \n",
    "    # Check if we have category information\n",
    "    has_categories = 'category' in df.columns\n",
    "    \n",
    "    if has_categories:\n",
    "        categories = df['category'].dropna().unique()\n",
    "        print(f\"Found {len(categories)} categories\")\n",
    "        \n",
    "        # Generate 2x more candidates than needed (will filter later)\n",
    "        target_pos = num_positives * 2\n",
    "        target_neg = num_negatives * 2\n",
    "        \n",
    "        print(f\"\\nGenerating {target_pos} candidate positive pairs...\")\n",
    "        # Positive pairs - same category\n",
    "        for _ in tqdm(range(target_pos)):\n",
    "            cat = np.random.choice(categories)\n",
    "            cat_incidents = df[df['category'] == cat]\n",
    "            if len(cat_incidents) >= 2:\n",
    "                idx1, idx2 = np.random.choice(cat_incidents.index, size=2, replace=False)\n",
    "                texts1.append(df.loc[idx1, 'combined_text'])\n",
    "                texts2.append(df.loc[idx2, 'combined_text'])\n",
    "                labels.append(1)\n",
    "        \n",
    "        print(f\"\\nGenerating {target_neg} candidate negative pairs...\")\n",
    "        # Negative pairs - different categories\n",
    "        for _ in tqdm(range(target_neg)):\n",
    "            cat1, cat2 = np.random.choice(categories, size=2, replace=False)\n",
    "            incidents1 = df[df['category'] == cat1]\n",
    "            incidents2 = df[df['category'] == cat2]\n",
    "            if len(incidents1) > 0 and len(incidents2) > 0:\n",
    "                idx1 = np.random.choice(incidents1.index)\n",
    "                idx2 = np.random.choice(incidents2.index)\n",
    "                texts1.append(df.loc[idx1, 'combined_text'])\n",
    "                texts2.append(df.loc[idx2, 'combined_text'])\n",
    "                labels.append(0)\n",
    "    else:\n",
    "        print(\"No category column found - using random pairs\")\n",
    "        # Random pairs as fallback\n",
    "        for _ in range(num_positives * 2):\n",
    "            idx1, idx2 = np.random.choice(len(df), size=2, replace=False)\n",
    "            texts1.append(df.loc[idx1, 'combined_text'])\n",
    "            texts2.append(df.loc[idx2, 'combined_text'])\n",
    "            labels.append(1)\n",
    "        \n",
    "        for _ in range(num_negatives * 2):\n",
    "            idx1, idx2 = np.random.choice(len(df), size=2, replace=False)\n",
    "            texts1.append(df.loc[idx1, 'combined_text'])\n",
    "            texts2.append(df.loc[idx2, 'combined_text'])\n",
    "            labels.append(0)\n",
    "    \n",
    "    print(f\"\\n‚úì Generated {len(labels)} candidate pairs\")\n",
    "    print(f\"  Positive: {sum(labels)}\")\n",
    "    print(f\"  Negative: {len(labels) - sum(labels)}\")\n",
    "    \n",
    "    return texts1, texts2, labels\n",
    "\n",
    "# Generate candidates\n",
    "candidate_texts1, candidate_texts2, candidate_labels = generate_candidate_pairs(\n",
    "    df,\n",
    "    num_positives=NUM_POSITIVE_PAIRS,\n",
    "    num_negatives=NUM_NEGATIVE_PAIRS,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c0f56b",
   "metadata": {},
   "source": [
    "## 6. Semantic Validation with Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3c1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing embeddings for 20000 text pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf545fe8d7594d998dcbc1ba73f4e55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a018e453957f4dda923479f7035b9d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_similarities(texts1: List[str], \n",
    "                        texts2: List[str],\n",
    "                        model: SentenceTransformer,\n",
    "                        batch_size: int = 16) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute cosine similarities between text pairs.\n",
    "    \"\"\"\n",
    "    print(f\"\\nComputing embeddings for {len(texts1)} text pairs...\")\n",
    "    \n",
    "    # Encode texts\n",
    "    embeddings1 = model.encode(\n",
    "        texts1,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    embeddings2 = model.encode(\n",
    "        texts2,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = np.sum(embeddings1 * embeddings2, axis=1) / (\n",
    "        np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n",
    "    )\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "# Compute similarities for all candidate pairs\n",
    "similarities = compute_similarities(\n",
    "    candidate_texts1,\n",
    "    candidate_texts2,\n",
    "    baseline_model,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Computed {len(similarities)} similarities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c9c04c",
   "metadata": {},
   "source": [
    "## 7. Filter Pairs with Semantic Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_validated_pairs(texts1: List[str],\n",
    "                          texts2: List[str],\n",
    "                          labels: List[int],\n",
    "                          similarities: np.ndarray,\n",
    "                          positive_threshold: float,\n",
    "                          negative_threshold: float,\n",
    "                          target_positives: int,\n",
    "                          target_negatives: int) -> Tuple[List[str], List[str], List[int], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Filter pairs based on semantic similarity thresholds.\n",
    "    \n",
    "    Positive pairs: similarity >= positive_threshold\n",
    "    Negative pairs: similarity < negative_threshold\n",
    "    \"\"\"\n",
    "    labels_array = np.array(labels)\n",
    "    \n",
    "    # Filter positive pairs\n",
    "    positive_mask = (labels_array == 1) & (similarities >= positive_threshold)\n",
    "    valid_positive_indices = np.where(positive_mask)[0]\n",
    "    \n",
    "    print(f\"\\nPositive pairs:\")\n",
    "    print(f\"  Candidates: {sum(labels_array == 1)}\")\n",
    "    print(f\"  Valid (similarity ‚â•{positive_threshold}): {len(valid_positive_indices)}\")\n",
    "    print(f\"  Rejection rate: {(1 - len(valid_positive_indices)/sum(labels_array == 1))*100:.1f}%\")\n",
    "    \n",
    "    # Filter negative pairs\n",
    "    negative_mask = (labels_array == 0) & (similarities < negative_threshold)\n",
    "    valid_negative_indices = np.where(negative_mask)[0]\n",
    "    \n",
    "    print(f\"\\nNegative pairs:\")\n",
    "    print(f\"  Candidates: {sum(labels_array == 0)}\")\n",
    "    print(f\"  Valid (similarity <{negative_threshold}): {len(valid_negative_indices)}\")\n",
    "    print(f\"  Rejection rate: {(1 - len(valid_negative_indices)/sum(labels_array == 0))*100:.1f}%\")\n",
    "    \n",
    "    # Sample to target sizes (if we have enough)\n",
    "    if len(valid_positive_indices) >= target_positives:\n",
    "        selected_positive_indices = np.random.choice(\n",
    "            valid_positive_indices,\n",
    "            size=target_positives,\n",
    "            replace=False\n",
    "        )\n",
    "    else:\n",
    "        selected_positive_indices = valid_positive_indices\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: Only {len(selected_positive_indices)} valid positive pairs (target: {target_positives})\")\n",
    "    \n",
    "    if len(valid_negative_indices) >= target_negatives:\n",
    "        selected_negative_indices = np.random.choice(\n",
    "            valid_negative_indices,\n",
    "            size=target_negatives,\n",
    "            replace=False\n",
    "        )\n",
    "    else:\n",
    "        selected_negative_indices = valid_negative_indices\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: Only {len(selected_negative_indices)} valid negative pairs (target: {target_negatives})\")\n",
    "    \n",
    "    # Combine selected indices\n",
    "    selected_indices = np.concatenate([selected_positive_indices, selected_negative_indices])\n",
    "    \n",
    "    # Extract selected pairs\n",
    "    validated_texts1 = [texts1[i] for i in selected_indices]\n",
    "    validated_texts2 = [texts2[i] for i in selected_indices]\n",
    "    validated_labels = [labels[i] for i in selected_indices]\n",
    "    validated_similarities = similarities[selected_indices]\n",
    "    \n",
    "    return validated_texts1, validated_texts2, validated_labels, validated_similarities\n",
    "\n",
    "# Filter pairs\n",
    "train_texts1, train_texts2, train_labels, train_similarities = filter_validated_pairs(\n",
    "    candidate_texts1,\n",
    "    candidate_texts2,\n",
    "    candidate_labels,\n",
    "    similarities,\n",
    "    positive_threshold=POSITIVE_SIMILARITY_THRESHOLD,\n",
    "    negative_threshold=NEGATIVE_SIMILARITY_THRESHOLD,\n",
    "    target_positives=NUM_POSITIVE_PAIRS,\n",
    "    target_negatives=NUM_NEGATIVE_PAIRS\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL TRAINING SET\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total pairs: {len(train_labels)}\")\n",
    "print(f\"  Positive: {sum(train_labels)} ({sum(train_labels)/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"  Negative: {len(train_labels)-sum(train_labels)} ({(len(train_labels)-sum(train_labels))/len(train_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3355f",
   "metadata": {},
   "source": [
    "## 8. Comparison: Category-Only vs Semantic Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0169faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare quality metrics between category-only and semantic filtering approaches\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPARISON: CATEGORY-ONLY vs SEMANTIC FILTERING\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Category-only metrics (before filtering)\n",
    "candidate_labels_array = np.array(candidate_labels)\n",
    "candidate_pos_sims = similarities[candidate_labels_array == 1]\n",
    "candidate_neg_sims = similarities[candidate_labels_array == 0]\n",
    "\n",
    "print(\"\\nüìä CATEGORY-ONLY METHOD (Before Filtering):\")\n",
    "print(f\"  Positive pairs: {len(candidate_pos_sims)}\")\n",
    "print(f\"    Mean similarity: {candidate_pos_sims.mean():.4f}\")\n",
    "print(f\"    Below threshold (<{POSITIVE_SIMILARITY_THRESHOLD}): {np.sum(candidate_pos_sims < POSITIVE_SIMILARITY_THRESHOLD)} ({np.sum(candidate_pos_sims < POSITIVE_SIMILARITY_THRESHOLD)/len(candidate_pos_sims)*100:.1f}%)\")\n",
    "print(f\"  Negative pairs: {len(candidate_neg_sims)}\")\n",
    "print(f\"    Mean similarity: {candidate_neg_sims.mean():.4f}\")\n",
    "print(f\"    Above threshold (‚â•{NEGATIVE_SIMILARITY_THRESHOLD}): {np.sum(candidate_neg_sims >= NEGATIVE_SIMILARITY_THRESHOLD)} ({np.sum(candidate_neg_sims >= NEGATIVE_SIMILARITY_THRESHOLD)/len(candidate_neg_sims)*100:.1f}%)\")\n",
    "\n",
    "candidate_separability = candidate_pos_sims.mean() - candidate_neg_sims.mean()\n",
    "print(f\"  Separability: {candidate_separability:.4f}\")\n",
    "\n",
    "# Semantic filtering metrics (after filtering)\n",
    "train_labels_array = np.array(train_labels)\n",
    "pos_similarities = train_similarities[train_labels_array == 1]\n",
    "neg_similarities = train_similarities[train_labels_array == 0]\n",
    "\n",
    "print(\"\\n‚ú® SEMANTIC FILTERING METHOD (After Filtering):\")\n",
    "print(f\"  Positive pairs: {len(pos_similarities)}\")\n",
    "print(f\"    Mean similarity: {pos_similarities.mean():.4f}\")\n",
    "print(f\"  Negative pairs: {len(neg_similarities)}\")\n",
    "print(f\"    Mean similarity: {neg_similarities.mean():.4f}\")\n",
    "\n",
    "separability = pos_similarities.mean() - neg_similarities.mean()\n",
    "print(f\"  Separability: {separability:.4f}\")\n",
    "\n",
    "# Show improvement\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"IMPROVEMENT FROM SEMANTIC FILTERING\")\n",
    "print(f\"{'='*80}\")\n",
    "pos_rejected = len(candidate_pos_sims) - len(pos_similarities)\n",
    "neg_rejected = len(candidate_neg_sims) - len(neg_similarities)\n",
    "print(f\"Rejected noisy positives: {pos_rejected} ({pos_rejected/len(candidate_pos_sims)*100:.1f}%)\")\n",
    "print(f\"Rejected ambiguous negatives: {neg_rejected} ({neg_rejected/len(candidate_neg_sims)*100:.1f}%)\")\n",
    "print(f\"Separability improvement: {separability - candidate_separability:+.4f} ({(separability - candidate_separability)/candidate_separability*100:+.1f}%)\")\n",
    "\n",
    "if separability > candidate_separability * 1.2:\n",
    "    print(\"\\n‚úì SIGNIFICANT IMPROVEMENT: Semantic filtering dramatically improved quality\")\n",
    "elif separability > candidate_separability:\n",
    "    print(\"\\n‚úì IMPROVED: Semantic filtering enhanced quality\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  LIMITED IMPROVEMENT: Consider adjusting thresholds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2ea7b",
   "metadata": {},
   "source": [
    "## 9. Quality Analysis & Pre-Flight Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13594b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed quality analysis and validation checks\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DETAILED QUALITY ANALYSIS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nüìà Positive Pairs (label=1):\")\n",
    "print(f\"  Count: {len(pos_similarities)}\")\n",
    "print(f\"  Mean similarity: {pos_similarities.mean():.4f}\")\n",
    "print(f\"  Median similarity: {np.median(pos_similarities):.4f}\")\n",
    "print(f\"  Std dev: {pos_similarities.std():.4f}\")\n",
    "print(f\"  Range: [{pos_similarities.min():.4f}, {pos_similarities.max():.4f}]\")\n",
    "\n",
    "print(f\"\\nüìâ Negative Pairs (label=0):\")\n",
    "print(f\"  Count: {len(neg_similarities)}\")\n",
    "print(f\"  Mean similarity: {neg_similarities.mean():.4f}\")\n",
    "print(f\"  Median similarity: {np.median(neg_similarities):.4f}\")\n",
    "print(f\"  Std dev: {neg_similarities.std():.4f}\")\n",
    "print(f\"  Range: [{neg_similarities.min():.4f}, {neg_similarities.max():.4f}]\")\n",
    "\n",
    "print(f\"\\nüìä Separability Analysis:\")\n",
    "print(f\"  Separability (Pos - Neg): {separability:.4f}\")\n",
    "print(f\"  Required minimum: {MIN_SEPARABILITY:.4f}\")\n",
    "\n",
    "if separability >= MIN_SEPARABILITY:\n",
    "    print(f\"  ‚úì PASS: Meets minimum requirement\")\n",
    "else:\n",
    "    print(f\"  ‚úó FAIL: Below minimum requirement by {MIN_SEPARABILITY - separability:.4f}\")\n",
    "\n",
    "# Check overlap\n",
    "overlap_count = np.sum(\n",
    "    (pos_similarities[:, None] <= neg_similarities[None, :]).any(axis=1)\n",
    ")\n",
    "overlap_pct = overlap_count / len(pos_similarities) * 100\n",
    "\n",
    "print(f\"\\nüîÑ Overlap Analysis:\")\n",
    "print(f\"  Overlap percentage: {overlap_pct:.1f}%\")\n",
    "print(f\"  Maximum allowed: {MAX_OVERLAP_PCT:.1f}%\")\n",
    "\n",
    "if overlap_pct <= MAX_OVERLAP_PCT:\n",
    "    print(f\"  ‚úì PASS: Within acceptable range\")\n",
    "else:\n",
    "    print(f\"  ‚úó FAIL: Exceeds maximum by {overlap_pct - MAX_OVERLAP_PCT:.1f}%\")\n",
    "\n",
    "# Count risky negative pairs (too similar)\n",
    "risky_negatives = np.sum(neg_similarities > POSITIVE_SIMILARITY_THRESHOLD)\n",
    "risky_neg_pct = risky_negatives / len(neg_similarities) * 100\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Risk Assessment:\")\n",
    "print(f\"  Risky negatives (>{POSITIVE_SIMILARITY_THRESHOLD}): {risky_negatives} ({risky_neg_pct:.1f}%)\")\n",
    "if risky_neg_pct < 5:\n",
    "    print(f\"  ‚úì LOW RISK: Very few ambiguous negatives\")\n",
    "elif risky_neg_pct < 15:\n",
    "    print(f\"  ‚ö†Ô∏è  MODERATE RISK: Some ambiguous negatives\")\n",
    "else:\n",
    "    print(f\"  ‚úó HIGH RISK: Many ambiguous negatives (tighten NEGATIVE_SIMILARITY_THRESHOLD)\")\n",
    "\n",
    "# Overall quality gate\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PRE-FLIGHT VALIDATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "validation_passed = True\n",
    "issues = []\n",
    "\n",
    "if separability < MIN_SEPARABILITY:\n",
    "    validation_passed = False\n",
    "    issues.append(f\"Separability {separability:.4f} < {MIN_SEPARABILITY:.4f}\")\n",
    "\n",
    "if overlap_pct > MAX_OVERLAP_PCT:\n",
    "    validation_passed = False\n",
    "    issues.append(f\"Overlap {overlap_pct:.1f}% > {MAX_OVERLAP_PCT:.1f}%\")\n",
    "\n",
    "if risky_neg_pct > 15:\n",
    "    validation_passed = False\n",
    "    issues.append(f\"Risky negatives {risky_neg_pct:.1f}% > 15%\")\n",
    "\n",
    "if len(pos_similarities) < NUM_POSITIVE_PAIRS * 0.9:\n",
    "    validation_passed = False\n",
    "    issues.append(f\"Insufficient positive pairs: {len(pos_similarities)} < {NUM_POSITIVE_PAIRS * 0.9:.0f}\")\n",
    "\n",
    "if len(neg_similarities) < NUM_NEGATIVE_PAIRS * 0.9:\n",
    "    validation_passed = False\n",
    "    issues.append(f\"Insufficient negative pairs: {len(neg_similarities)} < {NUM_NEGATIVE_PAIRS * 0.9:.0f}\")\n",
    "\n",
    "if validation_passed:\n",
    "    print(\"‚úì ALL CHECKS PASSED\")\n",
    "    print(\"\\nTraining data quality is GOOD. Safe to proceed with saving.\")\n",
    "else:\n",
    "    print(\"‚úó VALIDATION FAILED\")\n",
    "    print(\"\\nIssues detected:\")\n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f\"  {i}. {issue}\")\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Training with this data may produce poor models!\")\n",
    "    print(\"Recommended actions:\")\n",
    "    print(\"  - Increase POSITIVE_SIMILARITY_THRESHOLD (currently {POSITIVE_SIMILARITY_THRESHOLD})\")\n",
    "    print(\"  - Decrease NEGATIVE_SIMILARITY_THRESHOLD (currently {NEGATIVE_SIMILARITY_THRESHOLD})\")\n",
    "    print(\"  - Generate more candidate pairs (increase 2x multiplier)\")\n",
    "\n",
    "# Store validation result for later use\n",
    "VALIDATION_PASSED = validation_passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f21a76",
   "metadata": {},
   "source": [
    "## 10. Visualize Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(neg_similarities, bins=50, alpha=0.6, label='Negative (label=0)', color='red', edgecolor='black')\n",
    "plt.hist(pos_similarities, bins=50, alpha=0.6, label='Positive (label=1)', color='green', edgecolor='black')\n",
    "\n",
    "plt.axvline(POSITIVE_SIMILARITY_THRESHOLD, color='green', linestyle='--', linewidth=2, label=f'Pos threshold={POSITIVE_SIMILARITY_THRESHOLD}')\n",
    "plt.axvline(NEGATIVE_SIMILARITY_THRESHOLD, color='red', linestyle='--', linewidth=2, label=f'Neg threshold={NEGATIVE_SIMILARITY_THRESHOLD}')\n",
    "\n",
    "plt.xlabel('Cosine Similarity', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Training Pair Similarity Distribution (Validated)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Distribution looks {'GOOD' if separability > 0.1 else 'NEEDS IMPROVEMENT'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9357f4",
   "metadata": {},
   "source": [
    "## 11. Save Validated Training Pairs (with Quality Gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803229ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality gate check before saving\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"QUALITY GATE CHECK\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if not VALIDATION_PASSED:\n",
    "    print(\"‚úó QUALITY GATE: FAILED\")\n",
    "    print(\"\\nData quality is below acceptable standards.\")\n",
    "    print(\"Refusing to save training pairs to prevent training poor models.\")\n",
    "    print(\"\\nPlease adjust configuration and regenerate:\")\n",
    "    print(f\"  - Current separability: {separability:.4f} (required: ‚â•{MIN_SEPARABILITY})\")\n",
    "    print(f\"  - Current overlap: {overlap_pct:.1f}% (required: ‚â§{MAX_OVERLAP_PCT}%)\")\n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(f\"  1. Increase POSITIVE_SIMILARITY_THRESHOLD from {POSITIVE_SIMILARITY_THRESHOLD} to {POSITIVE_SIMILARITY_THRESHOLD + 0.05}\")\n",
    "    print(f\"  2. Decrease NEGATIVE_SIMILARITY_THRESHOLD from {NEGATIVE_SIMILARITY_THRESHOLD} to {NEGATIVE_SIMILARITY_THRESHOLD - 0.05}\")\n",
    "    print(f\"  3. Rerun notebook from configuration cell\")\n",
    "    \n",
    "    raise ValueError(\"Quality gate failed: Data quality below minimum standards\")\n",
    "\n",
    "print(\"‚úì QUALITY GATE: PASSED\")\n",
    "print(f\"  Separability: {separability:.4f} ‚â• {MIN_SEPARABILITY} ‚úì\")\n",
    "print(f\"  Overlap: {overlap_pct:.1f}% ‚â§ {MAX_OVERLAP_PCT}% ‚úì\")\n",
    "print(\"\\nProceeding with save...\")\n",
    "\n",
    "# Save to JSON\n",
    "output_file = DATA_DIR / 'fixed_training_pairs.json'\n",
    "\n",
    "training_data = {\n",
    "    'texts1': train_texts1,\n",
    "    'texts2': train_texts2,\n",
    "    'labels': train_labels,\n",
    "    'metadata': {\n",
    "        'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'num_pairs': len(train_labels),\n",
    "        'num_positive': sum(train_labels),\n",
    "        'num_negative': len(train_labels) - sum(train_labels),\n",
    "        'positive_similarity_threshold': POSITIVE_SIMILARITY_THRESHOLD,\n",
    "        'negative_similarity_threshold': NEGATIVE_SIMILARITY_THRESHOLD,\n",
    "        'min_separability_requirement': MIN_SEPARABILITY,\n",
    "        'max_overlap_requirement': MAX_OVERLAP_PCT,\n",
    "        'baseline_model': 'sentence-transformers/all-mpnet-base-v2',\n",
    "        'baseline_separability': float(separability),\n",
    "        'baseline_overlap': float(overlap_pct / 100),\n",
    "        'positive_mean_similarity': float(pos_similarities.mean()),\n",
    "        'negative_mean_similarity': float(neg_similarities.mean()),\n",
    "        'quality_status': 'EXCELLENT' if separability > 0.15 else 'GOOD',\n",
    "        'validation_passed': True\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì TRAINING PAIRS SAVED\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"File: {output_file}\")\n",
    "print(f\"Size: {output_file.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Quality: {training_data['metadata']['quality_status']}\")\n",
    "print(f\"\\nYou can now use these validated pairs to train your models!\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Use these pairs in your training notebook\")\n",
    "print(f\"  2. Retrain all models with clean data\")\n",
    "print(f\"  3. Re-evaluate with fixed test pairs\")\n",
    "print(f\"  4. Expect significant performance improvements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca0a15",
   "metadata": {},
   "source": [
    "## 12. Sample Pairs for Manual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca992228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample positive pairs\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE POSITIVE PAIRS (should be semantically similar)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pos_indices = np.where(train_labels_array == 1)[0]\n",
    "sample_pos = np.random.choice(pos_indices, size=min(3, len(pos_indices)), replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_pos, 1):\n",
    "    print(f\"\\nPair {i} (similarity: {train_similarities[idx]:.3f}):\")\n",
    "    print(f\"  Text 1: {train_texts1[idx][:150]}...\")\n",
    "    print(f\"  Text 2: {train_texts2[idx][:150]}...\")\n",
    "\n",
    "# Show sample negative pairs\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE NEGATIVE PAIRS (should be semantically different)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "neg_indices = np.where(train_labels_array == 0)[0]\n",
    "sample_neg = np.random.choice(neg_indices, size=min(3, len(neg_indices)), replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_neg, 1):\n",
    "    print(f\"\\nPair {i} (similarity: {train_similarities[idx]:.3f}):\")\n",
    "    print(f\"  Text 1: {train_texts1[idx][:150]}...\")\n",
    "    print(f\"  Text 2: {train_texts2[idx][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fafdd8",
   "metadata": {},
   "source": [
    "## 13. Save Sample Inspection Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906629d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample pairs to a text file for manual review\n",
    "from datetime import datetime\n",
    "\n",
    "sample_output_file = DATA_DIR / f'sample_pairs_inspection_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "\n",
    "with open(sample_output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"TRAINING PAIRS QUALITY INSPECTION\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    # Write summary\n",
    "    f.write(f\"Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Total pairs: {len(train_labels)}\\n\")\n",
    "    f.write(f\"Positive pairs: {sum(train_labels)} ({sum(train_labels)/len(train_labels)*100:.1f}%)\\n\")\n",
    "    f.write(f\"Negative pairs: {len(train_labels) - sum(train_labels)} ({(len(train_labels)-sum(train_labels))/len(train_labels)*100:.1f}%)\\n\")\n",
    "    f.write(f\"Separability: {separability:.4f}\\n\\n\")\n",
    "    \n",
    "    # Write sample positive pairs\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"SAMPLE POSITIVE PAIRS (should be semantically similar)\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    for i, idx in enumerate(sample_pos, 1):\n",
    "        f.write(f\"Pair {i} (similarity: {train_similarities[idx]:.3f}):\\n\")\n",
    "        f.write(f\"  Text 1: {train_texts1[idx][:150]}...\\n\")\n",
    "        f.write(f\"  Text 2: {train_texts2[idx][:150]}...\\n\\n\")\n",
    "    \n",
    "    # Write sample negative pairs\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"SAMPLE NEGATIVE PAIRS (should be semantically different)\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    for i, idx in enumerate(sample_neg, 1):\n",
    "        f.write(f\"Pair {i} (similarity: {train_similarities[idx]:.3f}):\\n\")\n",
    "        f.write(f\"  Text 1: {train_texts1[idx][:150]}...\\n\")\n",
    "        f.write(f\"  Text 2: {train_texts2[idx][:150]}...\\n\\n\")\n",
    "\n",
    "print(f\"‚úì Sample pairs inspection saved to: {sample_output_file}\")\n",
    "print(f\"File size: {sample_output_file.stat().st_size / 1024:.2f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune SentenceTransformer Models for ITSM Tickets (v4)\n",
    "Self-contained pipeline to train a similarity model and (optionally) a relationship classifier.\n",
    "## V4 changes\n",
    "- Improved positive pair generation using Category and Subcategory.\n",
    "- Increased early stopping patience to 3.\n",
    "- Implemented SMOTE to handle class imbalance in the relationship classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de1b4633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n",
      "PyTorch version: 2.9.0+cu126\n",
      "Device: CPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "from transformers import TrainerControl\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print('imbalanced-learn is not installed. Relationship classifier will be skipped.')\n",
    "    IMBLEARN_AVAILABLE = False\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4475abd",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd40be1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: /content/models/all-mpnet-finetuned-v4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Base configuration (override as needed)\n",
    "CONFIG = {\n",
    "    'base_model': 'sentence-transformers/all-mpnet-base-v2',\n",
    "    'output_dir': 'models/all-mpnet-finetuned-v4',\n",
    "    'source_data': 'data/dummy_data_promax.csv',\n",
    "    'epochs': 8,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-5,\n",
    "    'warmup_steps': 200,\n",
    "    'eval_split': 0.15,\n",
    "    'max_seq_length': 256,\n",
    "    'eval_steps': 50,\n",
    "}\n",
    "\n",
    "# Derived paths (ensure plain string path)\n",
    "output_dir_str = str(CONFIG['output_dir'])\n",
    "output_path = output_dir_str if os.path.isabs(output_dir_str) else os.path.join(os.getcwd(), output_dir_str)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "print(f\"Output directory: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dbf2500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set to 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Global seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "print(f\"Seeds set to {SEED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce17b9",
   "metadata": {},
   "source": [
    "## 3. Load Training Data (dummy_data_promax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9041b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Expected input data at /content/data/dummy_data_promax.csv. Ensure dummy_data_promax is available.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2961103035.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected input data at {raw_data_path}. Ensure dummy_data_promax is available.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mrequired_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Number\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Short Description\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Description\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Category\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Subcategory\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Expected input data at /content/data/dummy_data_promax.csv. Ensure dummy_data_promax is available."
     ]
    }
   ],
   "source": [
    "\n",
    "# Build training pairs directly from dummy_data_promax.csv\n",
    "from itertools import combinations\n",
    "\n",
    "raw_data_path = CONFIG.get('source_data') or os.path.join('data', 'dummy_data_promax.csv')\n",
    "if not os.path.isabs(raw_data_path):\n",
    "    raw_data_path = os.path.join(os.getcwd(), raw_data_path)\n",
    "\n",
    "if not os.path.exists(raw_data_path):\n",
    "    raise FileNotFoundError(f\"Expected input data at {raw_data_path}. Ensure dummy_data_promax is available.\")\n",
    "\n",
    "required_columns = [\"Number\", \"Short Description\", \"Description\", \"Category\", \"Subcategory\"]\n",
    "df = pd.read_csv(raw_data_path)\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing expected columns in {raw_data_path}: {missing_columns}\")\n",
    "\n",
    "# Normalize text fields to align with the new headers\n",
    "for col in [\"Short Description\", \"Description\"]:\n",
    "    df[col] = df[col].fillna(\"\").astype(str).str.strip()\n",
    "for col in [\"Category\", \"Subcategory\"]:\n",
    "    df[col] = df[col].fillna(\"Unknown\").astype(str).str.strip()\n",
    "\n",
    "# Combine short description and description for the embedding text\n",
    "combined_text = (df[\"Short Description\"] + \". \" + df[\"Description\"]).str.strip()\n",
    "combined_text = combined_text.str.replace(r'^\\.\\s*', '', regex=True)\n",
    "df[\"text\"] = combined_text\n",
    "\n",
    "print(f\"Loaded {len(df)} incidents from {raw_data_path}\")\n",
    "print(\"Columns used:\", required_columns)\n",
    "print(df[required_columns].head(3))\n",
    "\n",
    "positive_pairs = []\n",
    "max_pairs_per_group = 150 # Reduced slightly to account for more groups\n",
    "random.seed(SEED)\n",
    "\n",
    "# V4 change: Group by both Category and Subcategory for more specific positive pairs\n",
    "for group_name, group in df.groupby([\"Category\", \"Subcategory\"]):\n",
    "    idxs = list(group.index)\n",
    "    if len(idxs) < 2:\n",
    "        continue\n",
    "    combos = list(combinations(idxs, 2))\n",
    "    random.shuffle(combos)\n",
    "    for idx1, idx2 in combos[:max_pairs_per_group]:\n",
    "        row1 = df.loc[idx1]\n",
    "        row2 = df.loc[idx2]\n",
    "        positive_pairs.append({\n",
    "            \"ticket1_id\": row1[\"Number\"],\n",
    "            \"ticket2_id\": row2[\"Number\"],\n",
    "            \"text1\": row1[\"text\"],\n",
    "            \"text2\": row2[\"text\"],\n",
    "            \"category1\": row1[\"Category\"],\n",
    "            \"category2\": row2[\"Category\"],\n",
    "            \"subcategory1\": row1[\"Subcategory\"],\n",
    "            \"subcategory2\": row2[\"Subcategory\"],\n",
    "        })\n",
    "\n",
    "if not positive_pairs:\n",
    "    raise ValueError(\"No positive pairs could be created; check category distribution in the dataset.\")\n",
    "\n",
    "all_indices = list(df.index)\n",
    "\n",
    "def create_hard_negatives(df, target_count, seed=42):\n",
    "    \"\"\"\n",
    "    Create hard negatives that share some keywords but different categories.\n",
    "    Adds guards for sparse categories and empty texts.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    negatives = []\n",
    "    attempts = 0\n",
    "    max_attempts = target_count * 20\n",
    "\n",
    "    category_groups = {cat: group for cat, group in df.groupby(\"Category\")}\n",
    "    categories = list(category_groups.keys())\n",
    "\n",
    "    if len(categories) < 2:\n",
    "        print(\"Not enough distinct categories for hard negatives; falling back to random negatives.\")\n",
    "        while len(negatives) < target_count:\n",
    "            i, j = random.sample(all_indices, 2)\n",
    "            if df.at[i, \"Category\"] != df.at[j, \"Category\"]:\n",
    "                row1, row2 = df.loc[i], df.loc[j]\n",
    "                negatives.append({\n",
    "                    \"ticket1_id\": row1[\"Number\"],\n",
    "                    \"ticket2_id\": row2[\"Number\"],\n",
    "                    \"text1\": row1[\"text\"],\n",
    "                    \"text2\": row2[\"text\"],\n",
    "                    \"category1\": row1[\"Category\"],\n",
    "                    \"category2\": row2[\"Category\"],\n",
    "                    \"subcategory1\": row1[\"Subcategory\"],\n",
    "                    \"subcategory2\": row2[\"Subcategory\"],\n",
    "                })\n",
    "        return negatives\n",
    "\n",
    "    while len(negatives) < target_count and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "\n",
    "        cat1, cat2 = random.sample(categories, 2)\n",
    "\n",
    "        idx1 = random.choice(category_groups[cat1].index)\n",
    "        idx2 = random.choice(category_groups[cat2].index)\n",
    "\n",
    "        row1 = df.loc[idx1]\n",
    "        row2 = df.loc[idx2]\n",
    "\n",
    "        words1 = set(row1['text'].lower().split())\n",
    "        words2 = set(row2['text'].lower().split())\n",
    "        if not words1 or not words2:\n",
    "            overlap = 0.0\n",
    "        else:\n",
    "            overlap = len(words1 & words2) / max(len(words1), len(words2))\n",
    "\n",
    "        if 0.1 < overlap < 0.4:\n",
    "            negatives.append({\n",
    "                \"ticket1_id\": row1[\"Number\"],\n",
    "                \"ticket2_id\": row2[\"Number\"],\n",
    "                \"text1\": row1[\"text\"],\n",
    "                \"text2\": row2[\"text\"],\n",
    "                \"category1\": row1[\"Category\"],\n",
    "                \"category2\": row2[\"Category\"],\n",
    "                \"subcategory1\": row1[\"Subcategory\"],\n",
    "                \"subcategory2\": row2[\"Subcategory\"],\n",
    "            })\n",
    "\n",
    "    while len(negatives) < target_count:\n",
    "        i, j = random.sample(all_indices, 2)\n",
    "        if df.at[i, \"Category\"] != df.at[j, \"Category\"]:\n",
    "            row1, row2 = df.loc[i], df.loc[j]\n",
    "            negatives.append({\n",
    "                \"ticket1_id\": row1[\"Number\"],\n",
    "                \"ticket2_id\": row2[\"Number\"],\n",
    "                \"text1\": row1[\"text\"],\n",
    "                \"text2\": row2[\"text\"],\n",
    "                \"category1\": row1[\"Category\"],\n",
    "                \"category2\": row2[\"Category\"],\n",
    "                \"subcategory1\": row1[\"Subcategory\"],\n",
    "                \"subcategory2\": row2[\"Subcategory\"],\n",
    "            })\n",
    "\n",
    "    return negatives\n",
    "\n",
    "negative_pairs = create_hard_negatives(df, len(positive_pairs), seed=SEED)\n",
    "print(f\"Created {len(negative_pairs)} hard negative pairs\")\n",
    "\n",
    "print(f\"Loaded pairs from {os.path.basename(raw_data_path)}: {len(positive_pairs)} positive, {len(negative_pairs)} negative\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178b8fbb",
   "metadata": {},
   "source": [
    "## 3.5 Data Augmentation (Optional)\n",
    "Augment training data to increase diversity and reduce overfitting. Consider more advanced techniques like back-translation or synonym replacement using `nlpaug` for further improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685416a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# New Section 3.5: Data Augmentation (Optional)\n",
    "\"\"\"\n",
    "Augment training data to increase diversity and reduce overfitting.\n",
    "\"\"\"\n",
    "\n",
    "def augment_text_simple(text, aug_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Simple augmentation: randomly drop or swap words to add lexical variety.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) < 4:\n",
    "        return text\n",
    "\n",
    "    augmented = words[:]\n",
    "\n",
    "    if random.random() > 0.5:\n",
    "        keep_count = max(3, int(len(words) * (1 - aug_ratio)))\n",
    "        if len(augmented) > keep_count:\n",
    "            indices = sorted(random.sample(range(len(augmented)), keep_count))\n",
    "            augmented = [augmented[i] for i in indices]\n",
    "    else:\n",
    "        swap_count = min(2, len(augmented) // 2)\n",
    "        for _ in range(swap_count):\n",
    "            i, j = random.sample(range(len(augmented)), 2)\n",
    "            augmented[i], augmented[j] = augmented[j], augmented[i]\n",
    "\n",
    "    return ' '.join(augmented)\n",
    "\n",
    "# Augment 20% of positive pairs\n",
    "augment_count = int(len(positive_pairs) * 0.2)\n",
    "augmented_pairs = []\n",
    "\n",
    "for pair in random.sample(positive_pairs, augment_count):\n",
    "    augmented_pairs.append({\n",
    "        \"ticket1_id\": str(pair[\"ticket1_id\"]) + \"_aug\",\n",
    "        \"ticket2_id\": pair[\"ticket2_id\"],\n",
    "        \"text1\": augment_text_simple(pair[\"text1\"]),\n",
    "        \"text2\": pair[\"text2\"],\n",
    "        \"category1\": pair[\"category1\"],\n",
    "        \"category2\": pair[\"category2\"],\n",
    "        \"subcategory1\": pair[\"subcategory1\"],\n",
    "        \"subcategory2\": pair[\"subcategory2\"],\n",
    "    })\n",
    "\n",
    "positive_pairs.extend(augmented_pairs)\n",
    "print(f\"Added {len(augmented_pairs)} augmented positive pairs\")\n",
    "print(f\"Total positive pairs: {len(positive_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af98e01",
   "metadata": {},
   "source": [
    "## 4. Create Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e750e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build InputExamples for MultipleNegativesRankingLoss\n",
    "random.shuffle(positive_pairs)\n",
    "split_idx = int(len(positive_pairs) * (1 - CONFIG['eval_split']))\n",
    "train_positive_pairs = positive_pairs[:split_idx]\n",
    "eval_positive_pairs = positive_pairs[split_idx:]\n",
    "\n",
    "train_examples = [InputExample(texts=[pair['text1'], pair['text2']]) for pair in train_positive_pairs]\n",
    "\n",
    "eval_examples = []\n",
    "for pair in eval_positive_pairs:\n",
    "    eval_examples.append(InputExample(texts=[pair['text1'], pair['text2']], label=1.0))\n",
    "for pair in negative_pairs:\n",
    "    eval_examples.append(InputExample(texts=[pair['text1'], pair['text2']], label=0.0))\n",
    "\n",
    "random.shuffle(eval_examples)\n",
    "\n",
    "print(f\"Created {len(train_examples)} training examples for MultipleNegativesRankingLoss\")\n",
    "print(\"üìä Data Split:\")\n",
    "print(f\"  Training positives: {len(train_examples)} examples\")\n",
    "print(f\"  Evaluation positives: {len(eval_positive_pairs)} examples\")\n",
    "print(f\"  Evaluation negatives: {len(negative_pairs)} examples\")\n",
    "if len(eval_examples) < 25:\n",
    "    print(\"‚ö†Ô∏è Eval set is small; metrics may be noisy. Consider adding more data or increasing eval_split.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e8e32",
   "metadata": {},
   "source": [
    "## 5. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading base model: {CONFIG['base_model']}\")\n",
    "model = SentenceTransformer(CONFIG['base_model'])\n",
    "model.max_seq_length = CONFIG['max_seq_length']\n",
    "print(\"‚úÖ Model loaded successfully\")\n",
    "print(f\"Max sequence length: {model.max_seq_length}\")\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac0a36",
   "metadata": {},
   "source": [
    "## 6. Setup Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=CONFIG['batch_size'])\n",
    "\n",
    "# Loss\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Evaluator (only if we have eval data)\n",
    "evaluator = None\n",
    "if eval_examples:\n",
    "    eval_sents1 = [ex.texts[0] for ex in eval_examples]\n",
    "    eval_sents2 = [ex.texts[1] for ex in eval_examples]\n",
    "    eval_scores = [ex.label for ex in eval_examples]\n",
    "    evaluator = EmbeddingSimilarityEvaluator(eval_sents1, eval_sents2, eval_scores)\n",
    "    print(f\"Evaluator initialized with {len(eval_examples)} examples\")\n",
    "else:\n",
    "    print(\"No eval examples; skipping evaluator setup\")\n",
    "\n",
    "class EarlyStoppingCallback:\n",
    "    \"\"\"Stop training when performance stops improving\"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.01, monitor='spearman'): # V4 change: patience increased to 3\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.monitor = monitor\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.best_model_path = None\n",
    "\n",
    "    def __call__(self, score, epoch, steps):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            logger.info(f\"Early stopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                logger.info(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                return True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "early_stop_callback = EarlyStoppingCallback(patience=3, min_delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427bb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_callback import TrainerControl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372e2341",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6028a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Warmup steps: {CONFIG['warmup_steps']}\")\n",
    "from transformers.trainer_callback import TrainerControl\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Starting training at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "training_log = {\n",
    "    'epoch_scores': [],\n",
    "    'best_epoch': 0,\n",
    "    'best_score': 0\n",
    "}\n",
    "\n",
    "class LoggingCallback:\n",
    "    \"\"\"Log training progress\"\"\"\n",
    "    def __call__(self, score, epoch, steps):\n",
    "        training_log['epoch_scores'].append({\n",
    "            'epoch': epoch,\n",
    "            'steps': steps,\n",
    "            'score': score\n",
    "        })\n",
    "        if score > training_log['best_score']:\n",
    "            training_log['best_score'] = score\n",
    "            training_log['best_epoch'] = epoch\n",
    "\n",
    "        print(f\"\\nüìä Epoch {epoch} - Score: {score:.4f} (Best: {training_log['best_score']:.4f} @ Epoch {training_log['best_epoch']})\")\n",
    "\n",
    "        # Check early stopping\n",
    "        if early_stop_callback(score, epoch, steps):\n",
    "            print(\"‚ö†Ô∏è  Early stopping triggered!\")\n",
    "            control = TrainerControl()\n",
    "            control.should_training_stop = True\n",
    "            return control\n",
    "\n",
    "        return TrainerControl()\n",
    "logging_callback = LoggingCallback()\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    warmup_steps=CONFIG['warmup_steps'],\n",
    "    optimizer_params={'lr': CONFIG['learning_rate']},\n",
    "    output_path=output_path,\n",
    "    evaluation_steps=CONFIG['eval_steps'],\n",
    "    callback=logging_callback,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best epoch: {training_log['best_epoch']} with score: {training_log['best_score']:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e91d78",
   "metadata": {},
   "source": [
    "## 8. Save Training Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1347d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "\n",
    "metadata = {\n",
    "    \"base_model\": CONFIG['base_model'],\n",
    "    \"source_data\": CONFIG.get('source_data'),\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"epochs\": CONFIG['epochs'],\n",
    "    \"batch_size\": CONFIG['batch_size'],\n",
    "    \"learning_rate\": CONFIG['learning_rate'],\n",
    "    \"warmup_steps\": CONFIG['warmup_steps'],\n",
    "    \"eval_split\": CONFIG['eval_split'],\n",
    "    \"num_train_examples\": len(train_examples),\n",
    "    \"num_eval_examples\": len(eval_examples),\n",
    "    \"num_positive_pairs\": len(positive_pairs),\n",
    "    \"num_negative_pairs\": len(negative_pairs),\n",
    "    \"seed\": SEED,\n",
    "    \"torch_version\": torch.__version__,\n",
    "    \"transformers_version\": transformers.__version__,\n",
    "    \"sentence_transformers_version\": __import__('sentence_transformers').__version__,\n",
    "    \"numpy_version\": np.__version__,\n",
    "}\n",
    "\n",
    "meta_path = os.path.join(output_path, 'training_metadata.json')\n",
    "with open(meta_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"Metadata saved to {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e749e6",
   "metadata": {},
   "source": [
    "## 9. Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple cosine similarity check on the first eval pair (if available)\n",
    "if eval_examples:\n",
    "    sample = eval_examples[0]\n",
    "    emb1 = model.encode(sample.texts[0])\n",
    "    emb2 = model.encode(sample.texts[1])\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    sim = float(cosine_similarity([emb1], [emb2])[0][0])\n",
    "    print(f\"Example eval pair similarity: {sim} (label: {sample.label})\")\n",
    "else:\n",
    "    print(\"No eval examples available for quick check.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c0f2fb",
   "metadata": {},
   "source": [
    "## 10. Load and Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91401b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload from disk and run a quick inference\n",
    "loaded_model = SentenceTransformer(output_path)\n",
    "example_a = \"User cannot log into SAP after maintenance\"\n",
    "example_b = \"SAP authentication error following system reboot\"\n",
    "emb_a = loaded_model.encode(example_a)\n",
    "emb_b = loaded_model.encode(example_b)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim = float(cosine_similarity([emb_a], [emb_b])[0][0])\n",
    "print(f\"Example similarity: {sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda38b5",
   "metadata": {},
   "source": [
    "## 11. (Optional) Relationship Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009088dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a simple classifier on labeled relationship pairs if available\n",
    "rel_json_path = os.path.join('data', 'relationship_pairs.json')\n",
    "if not os.path.exists(rel_json_path):\n",
    "    print(f\"relationship_pairs.json not found at {rel_json_path}. Skip this section if you don't need it.\")\n",
    "elif not IMBLEARN_AVAILABLE:\n",
    "    print('imbalanced-learn not installed; skipping relationship classifier training.')\n",
    "else:\n",
    "    with open(rel_json_path, 'r', encoding='utf-8') as f:\n",
    "        rel_data = json.load(f)\n",
    "    df_pairs = pd.DataFrame(rel_data)\n",
    "    valid_labels = [\"duplicate\", \"related\", \"causal\", \"none\"]\n",
    "    df_pairs = df_pairs[df_pairs['label'].isin(valid_labels)].reset_index(drop=True)\n",
    "    if df_pairs.empty:\n",
    "        raise ValueError(\"No valid relationship data found.\")\n",
    "\n",
    "    label_counts = df_pairs['label'].value_counts()\n",
    "    rare_labels = label_counts[label_counts < 2].index.tolist()\n",
    "    if rare_labels:\n",
    "        print(f\"Skipping classes with fewer than 2 samples: {rare_labels}\")\n",
    "        df_pairs = df_pairs[~df_pairs['label'].isin(rare_labels)].reset_index(drop=True)\n",
    "    if df_pairs.empty:\n",
    "        print('No classes left after filtering; skipping relationship classifier.')\n",
    "    else:\n",
    "        print(\"Original class distribution (filtered):\")\n",
    "        print(df_pairs['label'].value_counts())\n",
    "\n",
    "        texts_a = df_pairs['text_a'].astype(str).tolist()\n",
    "        texts_b = df_pairs['text_b'].astype(str).tolist()\n",
    "        y_labels = df_pairs['label'].tolist()\n",
    "\n",
    "        rel_embedder = SentenceTransformer(output_path)\n",
    "        emb_a = rel_embedder.encode(texts_a, batch_size=32, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)\n",
    "        emb_b = rel_embedder.encode(texts_b, batch_size=32, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)\n",
    "\n",
    "        def build_pair_features(emb_a, emb_b):\n",
    "            diff = np.abs(emb_a - emb_b)\n",
    "            prod = emb_a * emb_b\n",
    "            return np.hstack([emb_a, emb_b, diff, prod])\n",
    "\n",
    "        X = build_pair_features(emb_a, emb_b)\n",
    "        label2id = {lbl: i for i, lbl in enumerate(valid_labels) if lbl in df_pairs['label'].unique()}\n",
    "        id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "        y = np.array([label2id[lbl] for lbl in y_labels])\n",
    "\n",
    "        class_counts = pd.Series(y).value_counts()\n",
    "        min_class_size = class_counts.min()\n",
    "        stratify_labels = None\n",
    "\n",
    "        if len(class_counts) < 2:\n",
    "            print('Only one class present after filtering; skipping relationship classifier training.')\n",
    "        else:\n",
    "            if min_class_size < 2:\n",
    "                print('Skipping SMOTE because a class has less than 2 samples; disabling stratified split.')\n",
    "                X_resampled, y_resampled = X, y\n",
    "                stratify_labels = None\n",
    "            else:\n",
    "                k_neighbors = min(5, min_class_size - 1)\n",
    "                smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "                print(f'Applying SMOTE with k_neighbors={k_neighbors}')\n",
    "                X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "                stratify_labels = y_resampled\n",
    "\n",
    "            print(\"Resampled class distribution:\")\n",
    "            print(pd.Series(y_resampled).value_counts())\n",
    "\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=stratify_labels\n",
    "            )\n",
    "\n",
    "            clf = LogisticRegression(max_iter=200, multi_class='multinomial', solver='lbfgs', n_jobs=-1, class_weight='balanced')\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = clf.predict(X_val)\n",
    "            labels_order = list(label2id.values())\n",
    "            label_names_ordered = [id2label[i] for i in labels_order]\n",
    "\n",
    "            print(\"Relationship classifier report:\")\n",
    "            print(classification_report(y_val, y_pred, labels=labels_order, target_names=label_names_ordered, zero_division=0))\n",
    "            print(\"Confusion matrix:\")\n",
    "            print(confusion_matrix(y_val, y_pred, labels=labels_order))\n",
    "\n",
    "            rel_dir = os.path.join(output_path, 'relationship_classifier')\n",
    "            os.makedirs(rel_dir, exist_ok=True)\n",
    "            import joblib\n",
    "            joblib.dump(clf, os.path.join(rel_dir, 'relationship_classifier.joblib'))\n",
    "            with open(os.path.join(rel_dir, 'label_mapping.json'), 'w') as f:\n",
    "                json.dump({'label2id': label2id, 'id2label': id2label}, f, indent=2)\n",
    "            print(f\"Saved relationship classifier to {rel_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e1d82",
   "metadata": {},
   "source": [
    "## 12. Visualize Metrics\n",
    "Graphs for similarity eval and relationship classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f778e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Similarity evaluation plots\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not eval_examples:\n",
    "    print('No eval examples; skipping similarity plots.')\n",
    "else:\n",
    "    eval_sents1 = [ex.texts[0] for ex in eval_examples]\n",
    "    eval_sents2 = [ex.texts[1] for ex in eval_examples]\n",
    "    labels = np.array([ex.label for ex in eval_examples])\n",
    "\n",
    "    emb1 = model.encode(eval_sents1, convert_to_numpy=True, show_progress_bar=False)\n",
    "    emb2 = model.encode(eval_sents2, convert_to_numpy=True, show_progress_bar=False)\n",
    "    # cosine similarity\n",
    "    norms1 = np.linalg.norm(emb1, axis=1)\n",
    "    norms2 = np.linalg.norm(emb2, axis=1)\n",
    "    cos_scores = np.sum(emb1 * emb2, axis=1) / (norms1 * norms2 + 1e-12)\n",
    "\n",
    "    roc_auc = roc_auc_score(labels, cos_scores)\n",
    "    pr_auc = average_precision_score(labels, cos_scores)\n",
    "    fpr, tpr, _ = roc_curve(labels, cos_scores)\n",
    "    precision, recall, _ = precision_recall_curve(labels, cos_scores)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    axes[0].hist(cos_scores[labels==1], bins=20, alpha=0.6, label='positive')\n",
    "    axes[0].hist(cos_scores[labels==0], bins=20, alpha=0.6, label='negative')\n",
    "    axes[0].set_title('Cosine similarity distribution')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(fpr, tpr, label=f'ROC AUC={roc_auc:.3f}')\n",
    "    axes[1].plot([0,1],[0,1],'k--', alpha=0.3)\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].set_title('ROC Curve')\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].plot(recall, precision, label=f'PR AUC={pr_auc:.3f}')\n",
    "    axes[2].set_xlabel('Recall')\n",
    "    axes[2].set_ylabel('Precision')\n",
    "    axes[2].set_title('Precision-Recall')\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Add threshold analysis\n",
    "    thresholds = np.arange(0.3, 1.01, 0.01)\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        preds = (cos_scores >= threshold).astype(int)\n",
    "        f1 = f1_score(labels, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(f\"üìà Optimal Classification Threshold: {best_threshold:.3f}\")\n",
    "    print(f\"   F1 Score at threshold: {best_f1:.3f}\")\n",
    "    print(f\"   Accuracy at threshold: {accuracy_score(labels, (cos_scores >= best_threshold).astype(int)):.3f}\")\n",
    "\n",
    "# Relationship classifier confusion matrix plot (if clf exists)\n",
    "if 'clf' in globals() and 'label_names_ordered' in globals():\n",
    "    cm = confusion_matrix(y_val, y_pred, labels=labels_order)\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "    ax.set_xticks(range(len(label_names_ordered)))\n",
    "    ax.set_xticklabels(label_names_ordered, rotation=45, ha='right')\n",
    "    ax.set_yticks(range(len(label_names_ordered)))\n",
    "    ax.set_yticklabels(label_names_ordered)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title('Relationship Classifier Confusion')\n",
    "    for (i,j), val in np.ndenumerate(cm):\n",
    "        ax.text(j, i, int(val), ha='center', va='center', color='black')\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Relationship classifier not trained in this run; skipping confusion matrix plot.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

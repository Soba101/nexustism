{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune SentenceTransformer Models for ITSM Tickets (v5)\n",
    "\n",
    "## Overview\n",
    "This notebook represents the `v5` iteration of the ITSM similarity model pipeline. It transitions from a functional script to a robust, configurable machine learning pipeline.\n",
    "\n",
    "### Key Enhancements\n",
    "1. **Comprehensive Configuration**: Centralized `CONFIG` for all hyperparameters.\n",
    "2. **Smart Data Generation**: TF-IDF based filtering for high-quality positives and dynamic hard negative mining.\n",
    "3. **Advanced Evaluation**: Real-time tracking of Spearman correlation, ROC AUC, and F1 scores.\n",
    "4. **Reproducibility**: Full seeding of Random, NumPy, and PyTorch.\n",
    "5. **Relationship Classifier**: Integrated training of the secondary classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import pickle\n",
    "import collections\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, precision_recall_curve, roc_curve\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import joblib\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SentenceEvaluator\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# --- Setup & Reproducibility ---\n",
    "\n",
    "# NLTK Downloads\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"training_v5.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    " )\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress non-critical warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    Set seeds for reproducibility across all libraries.\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print(f\"‚úÖ Seeds set to {seed}\")\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "# Check Imbalanced-Learn for Relationship Classifier\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    IMBLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print('‚ö†Ô∏è imbalanced-learn not installed. Relationship classifier will skip SMOTE.')\n",
    "    IMBLEARN_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Paths\n",
    "    'base_model': 'sentence-transformers/all-mpnet-base-v2',\n",
    "    'output_dir': 'models/all-mpnet-finetuned-v5',\n",
    "    'source_data': 'data/dummy_data_promax.csv',\n",
    "    'relationship_data': 'data/relationship_pairs.json',\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    'epochs': 20,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 2e-5,\n",
    "    'max_learning_rate': 5e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'max_seq_length': 384,  # Increased for detailed ticket descriptions\n",
    "    \n",
    "    # Data Generation Strategy\n",
    "    'target_pairs': 50000,  # Total training pairs to generate\n",
    "    'positive_ratio': 0.4,  # 40% positives, 60% negatives\n",
    "    'augmentation_ratio': 0.2,\n",
    "    'eval_split': 0.15,\n",
    "    \n",
    "    # Quality Filtering Thresholds\n",
    "    'quality_threshold': 0.3,      # Minimum TF-IDF similarity for 'good' positive pairs\n",
    "    'hard_negative_min': 0.15,     # Min similarity for hard negatives\n",
    "    'hard_negative_max': 0.45,     # Max similarity for hard negatives (confusing zone)\n",
    "    \n",
    "    # Early Stopping\n",
    "    'early_stopping_patience': 7,\n",
    "    'min_delta': 0.005,\n",
    "    'eval_steps': 100,\n",
    "    \n",
    "    # Loss Weights (Future use for Multi-Loss)\n",
    "    'mnr_loss_weight': 1.0,\n",
    "    'triplet_loss_weight': 0.5,\n",
    "    'cosine_loss_weight': 0.2}\n",
    "\n",
    "# Create Output Directory\n",
    "output_path = os.path.join(os.getcwd(), CONFIG['output_dir'])\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "print(f\"üìÇ Output directory set to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(filepath, min_length=10):\n",
    "    \"\"\"\n",
    "    Loads data, checks columns, and performs cleaning.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    required_cols = [\"Number\", \"Short Description\", \"Description\", \"Category\", \"Subcategory\"]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "    \n",
    "    # Fill NA and Clean Text\n",
    "    for col in [\"Short Description\", \"Description\"]:\n",
    "        df[col] = df[col].fillna(\"\").astype(str).str.strip()\n",
    "        \n",
    "    for col in [\"Category\", \"Subcategory\"]:\n",
    "        df[col] = df[col].fillna(\"Unknown\").astype(str).str.strip()\n",
    "        \n",
    "    # Combined Text\n",
    "    df[\"text\"] = (df[\"Short Description\"] + \" \" + df[\"Description\"]).str.strip()\n",
    "    # Remove excessive whitespace\n",
    "    df[\"text\"] = df[\"text\"].str.replace(r'\\s+', ' ', regex=True)\n",
    "    \n",
    "    # Filter empty or too short\n",
    "    initial_count = len(df)\n",
    "    df = df[df[\"text\"] .str.len() >= min_length].copy()\n",
    "    dropped = initial_count - len(df)\n",
    "    \n",
    "    # Encode Categories\n",
    "    df['category_id'] = df.groupby(['Category', 'Subcategory']).ngroup()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(df)} incidents (dropped {dropped} short/empty)\")\n",
    "    print(f\"   Unique Categories: {df['Category'].nunique()}\")\n",
    "    print(f\"   Unique Subcategories: {df['Subcategory'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load Data\n",
    "df_incidents = load_and_clean_data(CONFIG['source_data'])\n",
    "df_incidents.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Similarity Utilities\n",
    "We use TF-IDF similarity to find \"quality\" pairs (related but not identical) and \"hard negatives\" (different category but lexically similar).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSimilarityCalculator:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "        self.tfidf_matrix = None\n",
    "\n",
    "    def fit_tfidf(self, texts):\n",
    "        print(\"Fitting TF-IDF vectorizer...\")\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "        print(\"TF-IDF fit complete.\")\n",
    "\n",
    "    def get_tfidf_similarity(self, idx1, idx2):\n",
    "        if self.tfidf_matrix is None:\n",
    "            raise ValueError(\"Run fit_tfidf first\")\n",
    "        # Compute cosine similarity between two sparse vectors\n",
    "        return (self.tfidf_matrix[idx1] * self.tfidf_matrix[idx2].T).toarray()[0][0]\n",
    "\n",
    "sim_calculator = TextSimilarityCalculator()\n",
    "sim_calculator.fit_tfidf(df_incidents['text'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Smart Pair Generation\n",
    "Generating positive pairs (same subcategory + semantic overlap) and hard negatives (different category + lexical overlap).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_smart_pairs(df, target_count, pos_ratio=0.4):\n",
    "    positive_target = int(target_count * pos_ratio)\n",
    "    negative_target = target_count - positive_target\n",
    "    \n",
    "    positives = []\n",
    "    negatives = []\n",
    "    \n",
    "    # Group by Category/Subcategory\n",
    "    groups = df.groupby('category_id')\n",
    "    group_indices = {k: v.index.tolist() for k, v in groups}\n",
    "    all_indices = df.index.tolist()\n",
    "    \n",
    "    print(f\"Generating {positive_target} positive and {negative_target} negative pairs...\")\n",
    "    \n",
    "    # --- 1. Positive Pairs ---\n",
    "    # Strategy: Sample pairs from same group, check TF-IDF score to ensure they aren't duplicates or too vague\n",
    "    pbar = tqdm(total=positive_target, desc=\"Positives\")\n",
    "    attempts = 0\n",
    "    while len(positives) < positive_target and attempts < positive_target * 5:\n",
    "        attempts += 1\n",
    "        # Pick random group\n",
    "        gid = random.choice(list(group_indices.keys()))\n",
    "        g_idxs = group_indices[gid]\n",
    "        if len(g_idxs) < 2: continue\n",
    "        \n",
    "        i1, i2 = random.sample(g_idxs, 2)\n",
    "        \n",
    "        # Convert DataFrame index to integer location for TF-IDF\n",
    "        loc1 = df.index.get_loc(i1)\n",
    "        loc2 = df.index.get_loc(i2)\n",
    "        \n",
    "        sim = sim_calculator.get_tfidf_similarity(loc1, loc2)\n",
    "        \n",
    "        # Accept if similarity is decent (avoiding identicals if sim=1.0, though duplicates happen)\n",
    "        if sim > CONFIG['quality_threshold']:\n",
    "            positives.append((i1, i2, sim))\n",
    "            pbar.update(1)\n",
    "            \n",
    "    pbar.close()\n",
    "    \n",
    "    # --- 2. Hard Negative Pairs ---\n",
    "    # Strategy: Different categories but high TF-IDF overlap (confusing examples)\n",
    "    pbar = tqdm(total=negative_target, desc=\"Negatives\")\n",
    "    attempts = 0\n",
    "    while len(negatives) < negative_target:\n",
    "        attempts += 1\n",
    "        \n",
    "        # Random sampling\n",
    "        i1, i2 = random.sample(all_indices, 2)\n",
    "        \n",
    "        # Must be different categories\n",
    "        if df.at[i1, 'Category'] == df.at[i2, 'Category']:\n",
    "            continue\n",
    "            \n",
    "        loc1 = df.index.get_loc(i1)\n",
    "        loc2 = df.index.get_loc(i2)\n",
    "        sim = sim_calculator.get_tfidf_similarity(loc1, loc2)\n",
    "        \n",
    "        # Hard Negative Criteria\n",
    "        is_hard = CONFIG['hard_negative_min'] < sim < CONFIG['hard_negative_max']\n",
    "        \n",
    "        # Accept if hard negative OR we are struggling to find hard ones (fallback to random after many attempts)\n",
    "        if is_hard or attempts > negative_target * 10:\n",
    "            negatives.append((i1, i2, sim))\n",
    "            pbar.update(1)\n",
    "            if attempts > negative_target * 10:\n",
    "                # Reset attempts to avoid infinite fallback loop\n",
    "                attempts = 0 \n",
    "\n",
    "    pbar.close()\n",
    "    \n",
    "    return positives, negatives\n",
    "\n",
    "pos_pairs_idxs, neg_pairs_idxs = generate_smart_pairs(df_incidents, CONFIG['target_pairs'], CONFIG['positive_ratio'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_augment(text):\n",
    "    \"\"\"Randomly swap or delete words.\"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) < 5: return text\n",
    "    \n",
    "    if random.random() > 0.5:\n",
    "        # Swap\n",
    "        idx = random.randint(0, len(words)-2)\n",
    "        words[idx], words[idx+1] = words[idx+1], words[idx]\n",
    "    else:\n",
    "        # Delete\n",
    "        idx = random.randint(0, len(words)-1)\n",
    "        words.pop(idx)\n",
    "    return \" \".join(words)\n",
    "\n",
    "train_examples = []\n",
    "\n",
    "# --- Positives to InputExamples ---\n",
    "for i1, i2, score in pos_pairs_idxs:\n",
    "    t1 = df_incidents.at[i1, 'text']\n",
    "    t2 = df_incidents.at[i2, 'text']\n",
    "    \n",
    "    # Standard Pair\n",
    "    train_examples.append(InputExample(texts=[t1, t2], label=1.0))\n",
    "    \n",
    "    # Augmentation (only for subset)\n",
    "    if random.random() < CONFIG['augmentation_ratio']:\n",
    "        train_examples.append(InputExample(texts=[simple_augment(t1), t2], label=1.0))\n",
    "\n",
    "# --- Negatives to InputExamples ---\n",
    "for i1, i2, score in neg_pairs_idxs:\n",
    "    t1 = df_incidents.at[i1, 'text']\n",
    "    t2 = df_incidents.at[i2, 'text']\n",
    "    train_examples.append(InputExample(texts=[t1, t2], label=0.0))\n",
    "\n",
    "# Shuffle\n",
    "random.shuffle(train_examples)\n",
    "\n",
    "# Split Train/Eval\n",
    "train_size = int(len(train_examples) * (1 - CONFIG['eval_split']))\n",
    "train_data = train_examples[:train_size]\n",
    "eval_data = train_examples[train_size:]\n",
    "\n",
    "print(f\"Training Samples: {len(train_data)}\")\n",
    "print(f\"Evaluation Samples: {len(eval_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(CONFIG['base_model'])\n",
    "model.max_seq_length = CONFIG['max_seq_length']\n",
    "\n",
    "# --- Loss Function ---\n",
    "# MultipleNegativesRankingLoss is standard for semantic search.\n",
    "# It treats other samples in the batch as negatives.\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# --- DataLoader ---\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=CONFIG['batch_size'])\n",
    "\n",
    "print(f\"Model loaded: {CONFIG['base_model']}\")\n",
    "print(f\"Max Seq Length: {model.max_seq_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Setup & Custom Callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveEvaluator(SentenceEvaluator):\n",
    "    \n",
    "    Custom evaluator to track multiple metrics: Spearman, Pearson, ROC AUC, F1.\n",
    "    \n",
    "    def __init__(self, examples, batch_size=32, name='', show_progress_bar=False):\n",
    "        self.examples = examples\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "        self.show_progress_bar = show_progress_bar\n",
    "        \n",
    "        self.texts1 = [ex.texts[0] for ex in examples]\n",
    "        self.texts2 = [ex.texts[1] for ex in examples]\n",
    "        self.labels = [ex.label for ex in examples]\n",
    "\n",
    "    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n",
    "        model.eval()\n",
    "        \n",
    "        # Encode\n",
    "        emb1 = model.encode(self.texts1, batch_size=self.batch_size, show_progress_bar=self.show_progress_bar, convert_to_numpy=True)\n",
    "        emb2 = model.encode(self.texts2, batch_size=self.batch_size, show_progress_bar=self.show_progress_bar, convert_to_numpy=True)\n",
    "        \n",
    "        # Cosine Similarity\n",
    "        cosine_scores = np.sum(emb1 * emb2, axis=1) / (np.linalg.norm(emb1, axis=1) * np.linalg.norm(emb2, axis=1))\n",
    "        \n",
    "        # Metrics\n",
    "        eval_pearson, _ = pearsonr(self.labels, cosine_scores)\n",
    "        eval_spearman, _ = spearmanr(self.labels, cosine_scores)\n",
    "        \n",
    "        # Classification Metrics (Threshold optimization)\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(self.labels, cosine_scores)\n",
    "            pr_auc = average_precision_score(self.labels, cosine_scores)\n",
    "        except ValueError:\n",
    "            roc_auc = 0.0\n",
    "            pr_auc = 0.0\n",
    "\n",
    "        logger.info(f'Epoch {epoch}: Spearman={eval_spearman:.4f}, ROC_AUC={roc_auc:.4f}')\n",
    "        \n",
    "        # Save detailed metrics to CSV\n",
    "        if output_path:\n",
    "            csv_path = os.path.join(output_path, 'eval_metrics.csv')\n",
    "            file_exists = os.path.isfile(csv_path)\n",
    "            with open(csv_path, mode='a', newline='') as f:\n",
    "                header = 'epoch,steps,spearman,pearson,roc_auc,pr_auc\n",
    "'\n",
    "                if not file_exists: f.write(header)\n",
    "                f.write(f'{epoch},{steps},{eval_spearman:.4f},{eval_pearson:.4f},{roc_auc:.4f},{pr_auc:.4f}\n",
    "')\n",
    "\n",
    "        return eval_spearman\n",
    "\n",
    "# Initialize Evaluator\n",
    "evaluator = ComprehensiveEvaluator(eval_data, batch_size=CONFIG['batch_size'], name='dev')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Output Path\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "save_path = f\"{CONFIG['output_dir']}_{timestamp}\"\n",
    "\n",
    "# Train\n",
    "print(f'üöÄ Starting training... Saving to {save_path}')\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    warmup_steps=int(len(train_dataloader) * CONFIG['epochs'] * CONFIG['warmup_ratio']),\n",
    "    optimizer_params={'lr': CONFIG['learning_rate'], 'weight_decay': CONFIG['weight_decay']},\n",
    "    output_path=save_path,\n",
    "    evaluation_steps=CONFIG['eval_steps'],\n",
    "    save_best_model=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print('‚úÖ Training complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation & Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload Best Model\n",
    "best_model = SentenceTransformer(save_path)\n",
    "\n",
    "# Encode Eval Data\n",
    "eval_texts1 = [ex.texts[0] for ex in eval_data]\n",
    "eval_texts2 = [ex.texts[1] for ex in eval_data]\n",
    "eval_labels = [ex.label for ex in eval_data]\n",
    "\n",
    "embeddings1 = best_model.encode(eval_texts1)\n",
    "embeddings2 = best_model.encode(eval_texts2)\n",
    "cosine_scores = np.sum(embeddings1 * embeddings2, axis=1)\n",
    "\n",
    "# --- Plots ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Distribution\n",
    "sns.histplot(cosine_scores[np.array(eval_labels)==1], color='green', label='Positive', kde=True, ax=axes[0])\n",
    "sns.histplot(cosine_scores[np.array(eval_labels)==0], color='red', label='Negative', kde=True, ax=axes[0])\n",
    "axes[0].set_title('Cosine Similarity Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(eval_labels, cosine_scores)\n",
    "roc_auc = roc_auc_score(eval_labels, cosine_scores)\n",
    "axes[1].plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. Precision-Recall\n",
    "precision, recall, _ = precision_recall_curve(eval_labels, cosine_scores)\n",
    "pr_auc = average_precision_score(eval_labels, cosine_scores)\n",
    "axes[2].plot(recall, precision, label=f'PR AUC = {pr_auc:.3f}')\n",
    "axes[2].set_title('Precision-Recall Curve')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Relationship Classifier (Optional)\n",
    "Trains a secondary classifier to predict relationship types (duplicate, causal, related).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMBLEARN_AVAILABLE and os.path.exists(CONFIG['relationship_data']):\n",
    "    print('üß† Training Relationship Classifier...')\n",
    "    with open(CONFIG['relationship_data'], 'r') as f:\n",
    "        rel_data = json.load(f)\n",
    "    \n",
    "    rel_df = pd.DataFrame(rel_data)\n",
    "    # Filter valid labels\n",
    "    valid_labels = ['duplicate', 'causal', 'related', 'none']\n",
    "    rel_df = rel_df[rel_df['label'].isin(valid_labels)]\n",
    "    \n",
    "    # Encode features using fine-tuned model\n",
    "    text_a = rel_df['text_a'].tolist()\n",
    "    text_b = rel_df['text_b'].tolist()\n",
    "    \n",
    "    emb_a = best_model.encode(text_a)\n",
    "    emb_b = best_model.encode(text_b)\n",
    "    \n",
    "    # Feature Engineering: (u, v, |u-v|, u*v)\n",
    "    X = np.hstack([emb_a, emb_b, np.abs(emb_a - emb_b), emb_a * emb_b])\n",
    "    y = rel_df['label']\n",
    "    \n",
    "    # SMOTE Balancing\n",
    "    smote = SMOTE(k_neighbors=2, random_state=42)\n",
    "    X_res, y_res = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Train Classifier\n",
    "    clf = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluation\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Save\n",
    "    joblib.dump(clf, os.path.join(save_path, 'relationship_classifier.joblib'))\n",
    "    print('‚úÖ Relationship classifier saved.')\n",
    "else:\n",
    "    print('‚ö†Ô∏è Skipping relationship classifier (missing data or imbalanced-learn).')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
